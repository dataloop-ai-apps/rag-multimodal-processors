diff --git a/.bumpversion.cfg b/.bumpversion.cfg
index f59e579..3f3b790 100644
--- a/.bumpversion.cfg
+++ b/.bumpversion.cfg
@@ -1,14 +1,9 @@
 [bumpversion]
-current_version = 0.1.9
+current_version = 0.0.1
 commit = True
 tag = True
 tag_name = {new_version}
 
-[bumpversion:file:apps/pdf_processor/dataloop.json]
+[bumpversion:file:apps/pdf-processor/dataloop.json]
 search = "{current_version}"
 replace = "{new_version}"
-
-[bumpversion:file:apps/doc_processor/dataloop.json]
-search = "{current_version}"
-replace = "{new_version}"
-
diff --git a/.dataloop.cfg b/.dataloop.cfg
index 3884954..5e5ec67 100644
--- a/.dataloop.cfg
+++ b/.dataloop.cfg
@@ -1,7 +1,6 @@
 {
   "manifests": [
-    "apps/pdf_processor/dataloop.json",
-    "apps/doc_processor/dataloop.json"
+    "apps/pdf-processor/dataloop.json"
   ],
   "public_app" : false
 }
\ No newline at end of file
diff --git a/.gitignore b/.gitignore
index 9c2830e..ed32dac 100644
--- a/.gitignore
+++ b/.gitignore
@@ -360,7 +360,4 @@ pyrightconfig.json
 
 # End of https://www.toptal.com/developers/gitignore/api/python,pycharm,pycharm+all,data
 /idea
-*.whl
-scripts/
-.claude/*
-.cursor/*
+*.whl
\ No newline at end of file
diff --git a/ARCHITECTURE.md b/ARCHITECTURE.md
deleted file mode 100644
index 246f82c..0000000
--- a/ARCHITECTURE.md
+++ /dev/null
@@ -1,564 +0,0 @@
-# Architecture
-
-## Overview
-
-**Type-safe, stateless architecture** using `ExtractedData` dataclass as the central data structure:
-
-```
-Item -> App (Extract -> Clean -> Chunk -> Upload) -> Chunks
-```
-
-Each file type is a self-contained processor that follows a consistent pipeline pattern with typed data flow.
-
-## Supported File Types
-
-- **PDF** (.pdf) - `PDFProcessor` with `PDFExtractor`
-- **DOC** (.docx) - `DOCProcessor` with `DOCExtractor`
-
-## Core Data Structure: ExtractedData
-
-All pipeline operations use `ExtractedData` dataclass for type-safe data flow:
-
-```python
-@dataclass
-class ExtractedData:
-    # Input
-    item: Optional[dl.Item] = None
-    target_dataset: Optional[dl.Dataset] = None
-    config: Config = field(default_factory=Config)
-
-    # Extraction outputs
-    content_text: str = ""
-    images: List[ImageContent] = field(default_factory=list)
-    tables: List[TableContent] = field(default_factory=list)
-    metadata: Dict[str, Any] = field(default_factory=dict)
-
-    # Processing outputs
-    cleaned_text: str = ""
-    chunks: List[str] = field(default_factory=list)
-    chunk_metadata: List[Dict[str, Any]] = field(default_factory=list)
-    uploaded_items: List[Any] = field(default_factory=list)
-
-    # Error tracking
-    errors: ErrorTracker = field(default_factory=ErrorTracker)
-    current_stage: str = "init"
-```
-
-## Components
-
-### 1. Apps (`apps/`)
-
-File-type specific processor classes. Each app:
-- Uses `ExtractedData` throughout the pipeline
-- Implements static methods for composable processing
-- Calls shared transforms for reusable operations
-- Has dedicated extractor module for file-specific logic
-
-**Structure:**
-```
-apps/
-â”œâ”€â”€ __init__.py
-â”œâ”€â”€ pdf_processor/
-â”‚   â”œâ”€â”€ __init__.py
-â”‚   â”œâ”€â”€ app.py              # PDFProcessor class
-â”‚   â”œâ”€â”€ pdf_extractor.py    # PDFExtractor - extraction logic
-â”‚   â”œâ”€â”€ dataloop.json
-â”‚   â””â”€â”€ Dockerfile
-â””â”€â”€ doc_processor/
-    â”œâ”€â”€ __init__.py
-    â”œâ”€â”€ app.py              # DOCProcessor class
-    â”œâ”€â”€ doc_extractor.py    # DOCExtractor - extraction logic
-    â”œâ”€â”€ dataloop.json
-    â””â”€â”€ Dockerfile
-```
-
-**Example Processor:**
-```python
-class PDFProcessor(dl.BaseServiceRunner):
-    @staticmethod
-    def extract(data: ExtractedData) -> ExtractedData:
-        return PDFExtractor.extract(data)
-
-    @staticmethod
-    def clean(data: ExtractedData) -> ExtractedData:
-        return transforms.clean(data)
-
-    @staticmethod
-    def chunk(data: ExtractedData) -> ExtractedData:
-        return transforms.chunk(data)
-
-    @staticmethod
-    def upload(data: ExtractedData) -> ExtractedData:
-        return transforms.upload_to_dataloop(data)
-
-    @staticmethod
-    def run(item: dl.Item, target_dataset: dl.Dataset, config: Optional[Dict] = None) -> List[dl.Item]:
-        cfg = Config.from_dict(config or {})
-        data = ExtractedData(item=item, target_dataset=target_dataset, config=cfg)
-
-        data = PDFProcessor.extract(data)
-        data = PDFProcessor.clean(data)
-        data = PDFProcessor.chunk(data)
-        data = PDFProcessor.upload(data)
-
-        return data.uploaded_items
-```
-
-### 2. Extractors (`apps/*/extractor.py`)
-
-Dedicated extraction modules with file-specific logic:
-
-```python
-class PDFExtractor:
-    @staticmethod
-    def extract(data: ExtractedData) -> ExtractedData:
-        data.current_stage = "extraction"
-        # File-specific extraction logic
-        data.content_text = extracted_text
-        data.images = extracted_images
-        data.metadata['page_count'] = page_count
-        return data
-```
-
-### 3. Transforms (`transforms/`)
-
-**Pipeline operations with uniform signature:** `(data: ExtractedData) -> ExtractedData`
-
-**Available Transforms:**
-
-| Module | Functions |
-|--------|-----------|
-| `text_normalization.py` | `clean()`, `normalize_whitespace()`, `remove_empty_lines()`, `deep_clean()` |
-| `chunking.py` | `chunk()`, `chunk_with_images()`, `TextChunker` |
-| `ocr.py` | `ocr_enhance()`, `describe_images()` |
-| `llm.py` | `llm_chunk_semantic()`, `llm_summarize()`, `llm_extract_entities()`, `llm_translate()` |
-| `upload.py` | `upload_to_dataloop()`, `ChunkUploader` |
-
-**Example Transform:**
-```python
-def clean(data: ExtractedData) -> ExtractedData:
-    """Clean and normalize text content."""
-    data.current_stage = "cleaning"
-    content = data.content_text
-
-    if not content:
-        data.cleaned_text = ""
-        return data
-
-    # Apply cleaning
-    content = content.strip()
-    content = re.sub(r' +', ' ', content)
-    content = re.sub(r'\n{3,}', '\n\n', content)
-
-    data.cleaned_text = content
-    data.metadata['cleaning_applied'] = True
-    return data
-```
-
-### 4. Utils (`utils/`)
-
-**Core utilities and data models:**
-
-| Module | Purpose |
-|--------|---------|
-| `extracted_data.py` | `ExtractedData` dataclass - central pipeline structure |
-| `config.py` | `Config` dataclass with validation |
-| `errors.py` | `ErrorTracker` for error/warning tracking |
-| `data_types.py` | `ImageContent`, `TableContent` data models |
-| `chunk_metadata.py` | `ChunkMetadata` dataclass |
-| `dataloop_helpers.py` | Dataloop integration helpers |
-
-### 5. Main API (`main.py`)
-
-Routes requests to appropriate apps based on MIME type:
-
-```python
-from apps import PDFProcessor, DOCProcessor
-
-APP_REGISTRY: Dict[str, Type[dl.BaseServiceRunner]] = {
-    'application/pdf': PDFProcessor,
-    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': DOCProcessor,
-}
-
-def process_item(item, target_dataset, config=None):
-    app_class = APP_REGISTRY[item.mimetype]
-    return app_class.run(item, target_dataset, config or {})
-```
-
-## Configuration
-
-Configuration is handled through the `Config` dataclass with validation:
-
-```python
-@dataclass
-class Config:
-    # Error handling
-    error_mode: Literal['stop', 'continue'] = 'continue'
-    max_errors: int = 10
-
-    # Extraction
-    extraction_method: Literal['markdown', 'basic'] = 'markdown'
-    extract_images: bool = True
-    extract_tables: bool = True
-
-    # OCR
-    use_ocr: bool = False
-    ocr_method: Literal['local', 'batch', 'auto'] = 'local'
-    ocr_model_id: Optional[str] = None
-
-    # Chunking
-    chunking_strategy: Literal['recursive', 'fixed', 'sentence', 'none'] = 'recursive'
-    max_chunk_size: int = 300
-    chunk_overlap: int = 20
-
-    # Cleaning
-    normalize_whitespace: bool = True
-    remove_empty_lines: bool = True
-    use_deep_clean: bool = False
-
-    # LLM
-    llm_model_id: Optional[str] = None
-    generate_summary: bool = False
-    extract_entities: bool = False
-    translate: bool = False
-    target_language: str = 'English'
-
-    # Vision
-    vision_model_id: Optional[str] = None
-
-    # Upload
-    remote_path: str = '/chunks'
-
-    def validate(self) -> None:
-        """Validate configuration consistency."""
-        # Validates chunk settings, OCR requirements, LLM requirements, etc.
-```
-
-## Error Handling
-
-Error tracking through `ErrorTracker`:
-
-```python
-@dataclass
-class ErrorTracker:
-    errors: List[str] = field(default_factory=list)
-    warnings: List[str] = field(default_factory=list)
-    max_errors: int = 10
-    error_mode: str = 'continue'
-
-    def add_error(self, message: str, stage: str = "") -> bool:
-        """Add error and return whether to continue processing."""
-        self.errors.append(f"[{stage}] {message}" if stage else message)
-
-        if self.error_mode == 'stop':
-            return False
-        return len(self.errors) < self.max_errors
-```
-
-**Error Modes:**
-- `'stop'`: Halt on first error
-- `'continue'`: Allow up to `max_errors` before stopping
-
-## Extension Guide
-
-### Adding a New File Type
-
-1. **Create Extractor** (`apps/xls_processor/xls_extractor.py`):
-```python
-from utils.extracted_data import ExtractedData
-
-class XLSExtractor:
-    @staticmethod
-    def extract(data: ExtractedData) -> ExtractedData:
-        data.current_stage = "extraction"
-        # File-specific extraction
-        data.content_text = extracted_text
-        return data
-```
-
-2. **Create Processor** (`apps/xls_processor/app.py`):
-```python
-import transforms
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-
-class XLSProcessor(dl.BaseServiceRunner):
-    @staticmethod
-    def run(item, target_dataset, config=None):
-        cfg = Config.from_dict(config or {})
-        data = ExtractedData(item=item, target_dataset=target_dataset, config=cfg)
-
-        data = XLSExtractor.extract(data)
-        data = transforms.clean(data)
-        data = transforms.chunk(data)
-        data = transforms.upload_to_dataloop(data)
-
-        return data.uploaded_items
-```
-
-3. **Register** in `main.py`:
-```python
-APP_REGISTRY['application/vnd.ms-excel'] = XLSProcessor
-```
-
-### Adding a New Transform
-
-```python
-# transforms/custom.py
-from utils.extracted_data import ExtractedData
-
-def my_transform(data: ExtractedData) -> ExtractedData:
-    """Custom transform with standard signature."""
-    data.current_stage = "custom"
-    content = data.get_text()
-    # Transform logic
-    data.cleaned_text = transformed_content
-    return data
-```
-
-Export from `transforms/__init__.py`:
-```python
-from .custom import my_transform
-```
-
-## Testing
-
-```bash
-# Unit tests
-pytest tests/test_core.py tests/test_extractors.py tests/test_transforms.py -v
-
-# Integration tests (requires Dataloop auth)
-pytest tests/test_processors.py -v
-```
-
-| Test File | Coverage |
-|-----------|----------|
-| `test_core.py` | Config, ErrorTracker, ExtractedData, ChunkMetadata |
-| `test_extractors.py` | PDFExtractor, DOCExtractor |
-| `test_transforms.py` | All transform functions |
-| `test_processors.py` | Integration tests (PDF, DOC) |
-
-## Design Principles
-
-### Stateless Architecture
-- All callable functions are static methods
-- No instance variables - state passed through `ExtractedData`
-- Thread-safe by design for concurrent processing
-
-### Type Safety
-- `ExtractedData` dataclass with typed fields
-- `Config` dataclass with validation
-- Transform signatures: `(ExtractedData) -> ExtractedData`
-
-### Separation of Concerns
-- **Extractors**: File-specific extraction logic
-- **Transforms**: Reusable pipeline operations
-- **Utils**: Core utilities and data models
-- **Apps**: Compose extractors and transforms
-
-### Concurrency Support
-- Stateless functions enable parallel document processing
-- No shared state or race conditions
-- Each `ExtractedData` instance is independent
-
----
-
-## Improvement Suggestions & Questions for Design/Product Team
-
-This section documents design questions and technical decisions that need product/design team input.
-
-### Configuration Architecture
-
-#### Public API vs Internal Configuration
-**Current State:** Config class has 23 fields, but only 7 are exposed in the public API (dataloop.json).
-
-**Public API Fields (7):**
-- `ocr_from_images` â€“ enable OCR on images (default: False)
-- `ocr_integration_method` â€“ how OCR text is merged (append_to_page, separate_chunks, combine_all)
-- `use_markdown_extraction` â€“ extract Markdown formatting (default: False)
-- `chunking_strategy` â€“ text chunking method (recursive, fixed-size, nltk-sentence, nltk-paragraph, 1-chunk)
-- `max_chunk_size` â€“ max size per chunk (default: 300)
-- `chunk_overlap` â€“ overlap between chunks (default: 40)
-- `to_correct_spelling` â€“ enable spell correction (default: False)
-
-**Internal-Only Fields (16):**
-- Error handling: `error_mode`, `max_errors`
-- Extraction: `extract_images`, `extract_tables`
-- OCR internals: `ocr_method`, `ocr_model_id`
-- Cleaning: `normalize_whitespace`, `remove_empty_lines`
-- Upload: `remote_path`
-- **Unused LLM:** `llm_model_id`, `generate_summary`, `extract_entities`, `translate`, `target_language`
-- **Unused Vision:** `vision_model_id`
-
----
-
-### Questions for Product/Design Team
-
-#### 1. Unused Feature Fields (LLM & Vision)
-**Status:** Fields exist but are not implemented
-
-**LLM Fields:**
-- `llm_model_id`, `generate_summary`, `extract_entities`, `translate`, `target_language`
-- Functions exist but log "not yet implemented"
-- Validation logic exists but can never be used
-
-**Vision Fields:**
-- `vision_model_id`
-- `describe_images()` returns immediately without processing
-
-**Questions:**
-1. Should we remove these fields until there's active development?
-2. Or keep them as "reserved for future use" with TODO comments?
-3. If keeping, should we remove the validation logic that can never trigger?
-
-**Impact:** Keeping them adds dead code and suggests features that don't exist
-
-**Recommendation:** Remove entirely until implementation is planned
-
----
-
-#### 2. Image & Table Extraction Control
-**Status:** `extract_images=True` and `extract_tables=True` in Config but NOT in public API
-
-**Current Behavior:**
-- Images and tables are always extracted
-- Users cannot disable this through the UI
-- Config fields exist but are hardcoded to `True`
-
-**Questions:**
-1. Should these be:
-   - **Option A:** Always enabled (remove from config, hardcode in extractors)
-   - **Option B:** Added to public API as user-configurable checkboxes
-   - **Option C:** Kept internal-only for future optimization use cases
-
-2. Is there a use case for disabling image/table extraction in RAG pipelines?
-
-**Recommendation:** Option A (always extract) - simplifies code and matches expected RAG behavior
-
----
-
-#### 3. Config Class Structure
-**Current Confusion:** Developers cannot easily tell which fields are user-facing vs internal
-
-**Questions:**
-1. Should we split into `PublicConfig` (7 fields) and `InternalConfig` (internal fields)?
-2. Or document clearly which fields are public in code comments?
-
-**Impact:**
-- Splitting improves code clarity significantly
-- Requires refactoring but no API changes
-
-**Recommendation:** Split for clarity, or at minimum add clear documentation
-
----
-
-### Field Naming & UX Questions
-
-#### 4. Field Name: `to_correct_spelling`
-**Current State:** Grammatically awkward field name
-
-**Issues:**
-- Grammar: "to correct" vs "correct"
-- Misleading: Actually triggers "deep clean" (not just spell correction)
-- UI Label: "Apply Text Cleaning" (clearer than field name)
-
-**Questions:**
-1. Can we rename to match behavior better?
-   - Options: `apply_text_cleaning`, `deep_clean`, `enable_deep_clean`
-2. Or keep field name as-is and rely on UI label?
-
-**Impact:** Renaming is a breaking API change
-
-**Recommendation:** Keep field name, ensure UI tooltip explains it's more than spelling
-
----
-
-#### 5. OCR Integration Method Naming
-**Current Options:**
-- `append_to_page` - Appends OCR text after each page marker
-- `separate_chunks` - Creates separate section at end with page markers
-- `combine_all` - Combines all OCR at end without page markers
-
-**Potential Confusion:**
-- "separate_chunks" doesn't create separate chunks - it's a separate section
-- Distinction between `separate_chunks` and `combine_all` is subtle
-
-**Questions:**
-1. Have users reported confusion about these names?
-2. Should we rename for clarity (breaking change)?
-3. Or improve tooltips/documentation?
-
-**Recommendation:** Keep names unless user feedback indicates confusion
-
----
-
-#### 6. Chunking Strategy & Field Dependencies
-**Current Behavior:**
-- `chunking_strategy='1-chunk'` ignores `max_chunk_size` and `chunk_overlap`
-- `chunking_strategy='nltk-sentence'` ignores `chunk_overlap`
-- All fields are always visible in UI
-
-**Questions:**
-1. Should we hide incompatible fields using `dependsOn` in dataloop.json?
-2. Should we show warning messages when incompatible options are selected?
-3. Or document the behavior and let users configure freely?
-
-**Recommendation:** Use `dependsOn` to hide incompatible fields - reduces confusion
-
----
-
-### Feature Roadmap Questions
-
-#### 7. Batch OCR Implementation
-**Current State:**
-- `ocr_method` config field exists with options: `local`, `batch`, `auto`
-- Only `local` (EasyOCR) works
-- `batch` and `auto` fall back to `local` with warning log
-- Field is internal-only (not exposed in UI)
-
-**Questions:**
-1. Is batch OCR planned for near-term development?
-2. If not, should we remove this field from config?
-3. Should users ever see this option?
-
-**Recommendation:** Keep internal until batch OCR is implemented
-
----
-
-### Documentation Gaps
-
-#### 8. Pipeline Flow & Field Interactions
-**Missing Documentation:**
-1. Which config fields are public vs internal
-2. Standard processing pipeline flow
-3. How config fields affect pipeline behavior
-4. Field interactions (e.g., `to_correct_spelling` â†’ `deep_clean` instead of `clean`)
-
-**Questions:**
-1. Where should this be documented?
-   - README.md for developers?
-   - Inline tooltips for users?
-   - Separate user guide?
-2. Who is the audience - end users or developers?
-
-**Recommendation:** Add to README.md and expand tooltips in dataloop.json
-
----
-
-### Summary of Decisions Needed
-
-| Priority | Question | Options | Recommendation |
-|----------|----------|---------|----------------|
-| **HIGH** | Remove unused LLM/Vision fields? | Remove / Keep as TODO | **Remove** |
-| **HIGH** | Image/table extraction control? | Always-on / Configurable / Internal | **Always-on** |
-| **MEDIUM** | Split Public/Internal Config? | Split / Document only | **Split** |
-| **MEDIUM** | Rename `to_correct_spelling`? | Rename / Keep | **Keep** |
-| **MEDIUM** | OCR method names confusing? | Rename / Keep / Improve tooltips | **Keep + tooltips** |
-| **MEDIUM** | Hide incompatible chunking fields? | Yes / No | **Yes** |
-| **LOW** | Batch OCR roadmap? | Plan / Remove | **Clarify** |
-| **LOW** | Documentation location? | README / Tooltips / Guide | **Both** |
-
----
-
-**Last Updated:** 2025-11-30
-**Status:** Alpha - awaiting product team input on configuration architecture
diff --git a/README.md b/README.md
index 5d69b43..6cc696c 100644
--- a/README.md
+++ b/README.md
@@ -1,339 +1,99 @@
-# RAG Document Processors
+# RAG Multimodal Processors
 
-Modular, extensible processors for converting **PDF and DOC files** into RAG-ready chunks. Built on Dataloop with a type-safe pipeline architecture using `ExtractedData` dataclass.
+A collection of independent Dataloop applications for processing various file types and creating chunks for Retrieval-Augmented Generation (RAG) workflows.
 
-## Supported File Types
+## ðŸŽ¯ Overview
 
-- **PDF** (.pdf) - ML-enhanced text extraction with PyMuPDF Layout, optional OCR
-- **Microsoft Word** (.docx) - Document processing with tables and images
+This repository contains **separate apps per data type**, each independently deployable as a Dataloop service. All apps share common modules (chunkers, utils, extractors) to avoid code duplication while maintaining independence.
 
-## Key Features
+## ðŸ“ Repository Structure
 
-- **Type-Safe Pipeline** - `ExtractedData` dataclass replaces dict-based data flow
-- **Modular Architecture** - Clean separation between apps, transforms, and utilities
-- **Easy File Type Addition** - Add new processors with consistent patterns
-- **Pipeline Design** - Simple extract -> clean -> chunk -> upload flow
-- **Static Methods** - Composable processing steps with no instance dependencies
-- **Flexible OCR** - Local EasyOCR or Dataloop batch models
-- **Multiple Chunking Strategies** - Recursive, semantic, sentence, fixed
-- **Error Handling** - Configurable error modes ('stop' or 'continue')
-- **Comprehensive Tests** - Unit and integration test coverage
-
-## Quick Start
-
-```python
-import dtlpy as dl
-from apps.pdf_processor.app import PDFProcessor
-from apps.doc_processor.app import DOCProcessor
-
-# Get items
-item = dl.items.get(item_id='your-item-id')
-dataset = dl.datasets.get(dataset_id='your-dataset-id')
-
-# Process PDF
-chunks = PDFProcessor.run(item, dataset, {})
-print(f"Created {len(chunks)} chunks")
-
-# Process PDF with OCR
-chunks = PDFProcessor.run(item, dataset, {'use_ocr': True, 'max_chunk_size': 500})
-
-# Process DOCX
-chunks = DOCProcessor.run(item, dataset, {'max_chunk_size': 1000})
-```
-
-## Processing Options
-
-### Basic Processing
-
-```python
-chunks = PDFProcessor.run(item, dataset, {})
-```
-
-Pipeline: Extract -> Clean -> Chunk -> Upload
-
-### OCR for Scanned Documents
-
-```python
-chunks = PDFProcessor.run(item, dataset, {'use_ocr': True})
-```
-
-Pipeline: Extract -> OCR -> Clean -> Chunk -> Upload
-
-### Custom Chunk Size
-
-```python
-chunks = PDFProcessor.run(item, dataset, {
-    'max_chunk_size': 500,
-    'chunk_overlap': 50
-})
-```
-
-### Chunking Strategies
-
-```python
-# Recursive (default) - Smart splitting on paragraphs, sentences, then characters
-chunks = PDFProcessor.run(item, dataset, {'chunking_strategy': 'recursive'})
-
-# Sentence-based - Split on sentence boundaries
-chunks = PDFProcessor.run(item, dataset, {'chunking_strategy': 'sentence'})
-
-# Fixed-size chunks
-chunks = PDFProcessor.run(item, dataset, {'chunking_strategy': 'fixed'})
-```
-
-### OCR Methods
-
-```python
-# Local OCR with EasyOCR (default, no model needed)
-chunks = PDFProcessor.run(item, dataset, {
-    'use_ocr': True,
-    'ocr_method': 'local'
-})
-
-# Batch OCR via Dataloop model
-chunks = PDFProcessor.run(item, dataset, {
-    'use_ocr': True,
-    'ocr_method': 'batch',
-    'ocr_model_id': 'your-ocr-model-id'
-})
-```
-
-## Configuration
-
-All configuration options are passed as a dictionary:
-
-```python
-chunks = PDFProcessor.run(item, dataset, {
-    # Chunking options
-    'max_chunk_size': 300,              # Maximum chunk size
-    'chunk_overlap': 20,                # Overlap between chunks
-    'chunking_strategy': 'recursive',   # 'recursive', 'fixed', 'sentence', 'none'
-
-    # OCR options
-    'use_ocr': True,                    # Enable OCR
-    'ocr_method': 'local',              # 'local', 'batch', or 'auto'
-    'ocr_model_id': 'model-id',         # Required for batch/auto OCR
-
-    # Cleaning options
-    'normalize_whitespace': True,       # Normalize whitespace
-    'remove_empty_lines': True,         # Remove empty lines
-    'use_deep_clean': False,            # Aggressive text cleaning
-
-    # Error handling
-    'error_mode': 'continue',           # 'stop' or 'continue' on errors
-    'max_errors': 10,                   # Maximum errors before stopping
-
-    # LLM options
-    'llm_model_id': 'model-id',         # Required for LLM features
-    'generate_summary': False,          # Generate document summary
-    'extract_entities': False,          # Extract named entities
-    'translate': False,                 # Translate content
-    'target_language': 'English',       # Target language for translation
-
-    # Vision options
-    'vision_model_id': 'model-id',      # Model for image descriptions
-
-    # Upload options
-    'remote_path': '/chunks',           # Remote directory for chunks
-})
-```
-
-### Configuration Reference
-
-| Option | Type | Default | Description |
-|--------|------|---------|-------------|
-| `max_chunk_size` | int | 300 | Maximum characters per chunk |
-| `chunk_overlap` | int | 20 | Characters to overlap between chunks |
-| `chunking_strategy` | str | 'recursive' | Strategy: 'recursive', 'fixed', 'sentence', 'none' |
-| `use_ocr` | bool | False | Enable OCR text extraction from images |
-| `ocr_method` | str | 'local' | OCR method: 'local', 'batch', 'auto' |
-| `ocr_model_id` | str | None | Dataloop model ID (required for batch/auto) |
-| `normalize_whitespace` | bool | True | Normalize whitespace in text |
-| `remove_empty_lines` | bool | True | Remove empty lines from text |
-| `use_deep_clean` | bool | False | Apply aggressive text cleaning |
-| `error_mode` | str | 'continue' | Error handling: 'stop' or 'continue' |
-| `max_errors` | int | 10 | Maximum errors before stopping |
-| `llm_model_id` | str | None | Dataloop model ID for LLM features |
-| `generate_summary` | bool | False | Generate document summary |
-| `extract_entities` | bool | False | Extract named entities |
-| `translate` | bool | False | Translate content |
-| `target_language` | str | 'English' | Target language for translation |
-| `vision_model_id` | str | None | Dataloop model ID for image descriptions |
-| `remote_path` | str | '/chunks' | Remote directory for uploaded chunks |
-
-## Architecture
-
-The system uses a **type-safe, stateless architecture** with `ExtractedData` as the central data structure:
-
-```
-Item -> App (Extract -> Clean -> Chunk -> Upload) -> Chunks
-```
-
-### Core Components
-
-```
-apps/                       # File-type processors
-â”œâ”€â”€ pdf_processor/
-â”‚   â”œâ”€â”€ app.py             # PDFProcessor class
-â”‚   â””â”€â”€ pdf_extractor.py   # PDF extraction logic
-â””â”€â”€ doc_processor/
-    â”œâ”€â”€ app.py             # DOCProcessor class
-    â””â”€â”€ doc_extractor.py   # DOCX extraction logic
-
-transforms/                 # Pipeline transforms: (ExtractedData) -> ExtractedData
-â”œâ”€â”€ text_normalization.py  # clean(), normalize_whitespace(), deep_clean()
-â”œâ”€â”€ chunking.py            # chunk(), chunk_with_images(), TextChunker
-â”œâ”€â”€ ocr.py                 # ocr_enhance(), describe_images()
-â”œâ”€â”€ llm.py                 # llm_chunk_semantic(), llm_summarize(), llm_translate()
-â””â”€â”€ upload.py              # upload_to_dataloop()
-
-utils/                      # Core utilities and data models
-â”œâ”€â”€ extracted_data.py      # ExtractedData dataclass
-â”œâ”€â”€ config.py              # Config dataclass with validation
-â”œâ”€â”€ errors.py              # ErrorTracker for error handling
-â”œâ”€â”€ data_types.py          # ImageContent, TableContent
-â””â”€â”€ chunk_metadata.py      # ChunkMetadata dataclass
-```
-
-### Key Design: ExtractedData
-
-All transforms use `ExtractedData` dataclass for type-safe data flow:
-
-```python
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-
-# Create typed data structure
-data = ExtractedData(item=item, target_dataset=dataset, config=Config())
-
-# Pipeline with type safety
-data = PDFExtractor.extract(data)    # Populates content_text, images, tables
-data = transforms.clean(data)         # Populates cleaned_text
-data = transforms.chunk(data)         # Populates chunks, chunk_metadata
-data = transforms.upload_to_dataloop(data)  # Populates uploaded_items
-
-return data.uploaded_items
 ```
-
-### Transform Signature
-
-All transforms follow the signature: `(data: ExtractedData) -> ExtractedData`
-
-```python
-import transforms
-
-# Text transforms
-data = transforms.clean(data)
-data = transforms.normalize_whitespace(data)
-
-# Chunking transforms
-data = transforms.chunk(data)
-data = transforms.chunk_with_images(data)
-
-# OCR transforms
-data = transforms.ocr_enhance(data)
-
-# Upload
-data = transforms.upload_to_dataloop(data)
+rag-multimodal-processors/
+â”œâ”€â”€ apps/                    # Independent applications
+â”‚   â”œâ”€â”€ pdf-processor/      # PDF processing app
+â”‚   â”œâ”€â”€ image-processor/    # Coming soon
+â”‚   â””â”€â”€ audio-processor/    # Coming soon
+â”œâ”€â”€ chunkers/               # Shared chunking strategies
+â”œâ”€â”€ utils/                  # Shared utilities
+â”œâ”€â”€ extractors/             # Shared extractors (OCR, transcription, etc.)
+â””â”€â”€ tests/                  # Centralized tests
 ```
 
-## Testing
+### Key Design Principles
 
-```bash
-# Run unit tests
-pytest tests/test_core.py tests/test_extractors.py tests/test_transforms.py -v
+1. **App Independence**: Each app is a separate service with its own deployment
+2. **Shared Modules**: Common functionality (chunkers, utils, extractors) shared across apps
+3. **Dataloop Model Integration**: Support for Dataloop models for OCR, transcription, etc.
+4. **External Library Fallbacks**: EasyOCR, Whisper, and other libraries when no model provided
+5. **Flexible Configuration**: Each app has its own config schema in `dataloop.json`
+6. **Easy Deployment**: Independent Docker images and versioning per app
 
-# Run integration tests (requires Dataloop auth)
-pytest tests/test_processors.py -v
+## ðŸ“¦ Available Apps
 
-# Run specific processor
-pytest tests/test_processors.py -k pdf -v
-pytest tests/test_processors.py -k doc -v
-```
+### ðŸ“„ [PDF Processor](apps/pdf-processor/)
+Processes PDF documents to extract text, apply OCR, and create chunks for RAG.
 
-Test files:
-- `test_core.py` - Config, ErrorTracker, ExtractedData, ChunkMetadata
-- `test_extractors.py` - PDFExtractor, DOCExtractor
-- `test_transforms.py` - Transform functions
-- `test_processors.py` - Integration tests (PDF, DOC)
+**Features:**
+- Text extraction (plain and markdown-aware)
+- Image extraction and OCR (EasyOCR or custom models)
+- Multiple chunking strategies
+- Configurable text cleaning
 
-## Adding New File Types
+**[â†’ Full Documentation](apps/pdf-processor/README.md)**
 
-### 1. Create Extractor
+### ðŸ“ Document Processor *(Coming Soon)*
+Process Word documents, text files, and other document formats for RAG workflows.
 
-```python
-# apps/xls_processor/xls_extractor.py
-from utils.extracted_data import ExtractedData
+### ðŸ–¼ï¸ Image Processor *(Coming Soon)*
+Process images for RAG workflows with captioning and OCR.
 
-class XLSExtractor:
-    @staticmethod
-    def extract(data: ExtractedData) -> ExtractedData:
-        data.current_stage = "extraction"
-        # Extract content from Excel file
-        data.content_text = extracted_text
-        data.metadata['processor'] = 'xls'
-        return data
-```
+### ðŸŽµ Audio Processor *(Coming Soon)*
+Transcribe audio files and create searchable chunks.
 
-### 2. Create Processor
+### ðŸŽ¬ Video Processor *(Coming Soon)*
+Extract audio, captions, and keyframes from videos.
 
-```python
-# apps/xls_processor/app.py
-import transforms
-from utils.extracted_data import ExtractedData
-from utils.config import Config
+## ðŸ§© Shared Modules
 
-class XLSProcessor(dl.BaseServiceRunner):
-    @staticmethod
-    def run(item, target_dataset, config=None):
-        cfg = Config.from_dict(config or {})
-        data = ExtractedData(item=item, target_dataset=target_dataset, config=cfg)
+### Chunkers (`chunkers/`)
 
-        data = XLSExtractor.extract(data)
-        data = transforms.clean(data)
-        data = transforms.chunk(data)
-        data = transforms.upload_to_dataloop(data)
+Reusable text chunking strategies for breaking content into embedding-friendly pieces:
 
-        return data.uploaded_items
-```
+- **TextChunker**: General text chunking with multiple strategies
+  - `recursive`: Intelligent splitting respecting semantic boundaries
+  - `fixed-size`: Uniform chunks with configurable overlap
+  - `nltk-sentence`: Sentence-based chunking
+  - `nltk-paragraphs`: Paragraph-based chunking
+  - `markdown-aware`: Respects markdown structure
 
-### 3. Register in main.py
+**Future Chunkers:**
+- `CodeChunker`: Function/class-based chunking for code
+- `TableChunker`: Row-wise chunking with header preservation
+- `StructuredChunker`: Hierarchy-preserving for JSON/XML
 
-```python
-APP_REGISTRY['application/vnd.ms-excel'] = XLSProcessor
-```
+### Extractors (`extractors/`)
 
-## Adding New Transforms
+Unified interfaces for content extraction using Dataloop models or external libraries:
 
-```python
-# transforms/custom.py
-from utils.extracted_data import ExtractedData
+- **OCRExtractor**: Text extraction from images
+  - EasyOCR (default): Local processing
+  - Custom Dataloop models: Batch processing with automatic cleanup
 
-def my_transform(data: ExtractedData) -> ExtractedData:
-    """Custom transform following the standard signature."""
-    data.current_stage = "custom"
-    content = data.get_text()
-    # Transform content
-    data.cleaned_text = transformed_content
-    return data
-```
-
-Export from `transforms/__init__.py`:
-```python
-from .custom import my_transform
-```
+**Future Extractors:**
+- `TranscriptionExtractor`: Audio â†’ Text
+- `CaptioningExtractor`: Image â†’ Description
+- `EmbeddingExtractor`: Content â†’ Vectors
 
-Use in any processor:
-```python
-data = transforms.my_transform(data)
-```
+### Utils (`utils/`)
 
-## Documentation
+Shared utility functions:
 
-- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Technical architecture details
-- **[tests/README.md](tests/README.md)** - Testing guide
+- **`text_cleaning.py`**: Text normalization and cleaning
+- **`dataloop_helpers.py`**: Dataset operations, chunk uploading, metadata handling
+- **`dataloop_model_executor.py`**: Base class for Dataloop model execution
 
-## Links
+## ðŸ”— Links
 
 - [Dataloop Platform](https://dataloop.ai)
-- [Dataloop SDK](https://sdk-docs.dataloop.ai)
+- [Dataloop SDK Documentation](https://sdk-docs.dataloop.ai)
diff --git a/apps/__init__.py b/apps/__init__.py
deleted file mode 100644
index 281aa33..0000000
--- a/apps/__init__.py
+++ /dev/null
@@ -1,18 +0,0 @@
-"""
-Apps package for file-type specific processors.
-
-Each app processes a specific file type using shared extractors and stages.
-Each app is a separate Dataloop application with its own Dockerfile and configuration.
-"""
-
-from .pdf_processor.app import PDFProcessor
-from .pdf_processor.pdf_extractor import PDFExtractor
-from .doc_processor.app import DOCProcessor
-from .doc_processor.doc_extractor import DOCExtractor
-
-__all__ = [
-    'PDFProcessor',
-    'PDFExtractor',
-    'DOCProcessor',
-    'DOCExtractor',
-]
diff --git a/apps/doc_processor/Dockerfile b/apps/doc_processor/Dockerfile
deleted file mode 100644
index 05b6161..0000000
--- a/apps/doc_processor/Dockerfile
+++ /dev/null
@@ -1,13 +0,0 @@
-FROM hub.dataloop.ai/dtlpy-runner-images/cpu:python3.10_opencv
-# Install Python dependencies
-RUN ${DL_PYTHON_EXECUTABLE} -m pip install -U \
-    pandas>=2.0.0 \
-    python-docx>=1.1.0 \
-    Pillow>=10.0.0 \
-    unstructured>=0.10.0 \
-    nltk>=3.8.0 \
-    langchain-text-splitters>=0.0.1
-    
-# docker build --no-cache -t gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/doc-processor:0.0.1 -f apps/doc_processor/Dockerfile .
-# docker push gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/doc-processor:0.0.1
-
diff --git a/apps/doc_processor/README.md b/apps/doc_processor/README.md
deleted file mode 100644
index 2b15f47..0000000
--- a/apps/doc_processor/README.md
+++ /dev/null
@@ -1,145 +0,0 @@
-# DOC Processor
-
-A modular Dataloop application for processing DOCX files into RAG-ready chunks with support for text, tables, and images.
-
-## ðŸŽ¯ Features
-
-### Text Extraction
-- **Paragraph Extraction**: Extracts all text paragraphs from DOCX files using `python-docx`
-- **Table Extraction**: Optional extraction of tables as markdown format
-- **Image Extraction**: Optional extraction of embedded images
-
-### Chunk Upload
-- **Bulk Upload**: Upload all chunks in a single operation using pandas DataFrame
-- **Per-Chunk Metadata**: Track images and extraction details per chunk
-- **Resilient Fallback**: Automatic retry with individual uploads if needed
-
-### Text Chunking
-Multiple strategies for optimal embedding:
-- `recursive`: Intelligent splitting respecting semantic boundaries
-- `fixed-size`: Uniform chunks with configurable overlap
-- `nltk-sentence`: Sentence-based chunking
-- `nltk-paragraphs`: Paragraph-based chunking
-- `1-chunk`: Single chunk (for short documents)
-
-### Text Processing
-- **Text Cleaning**: Optional normalization using `unstructured.io`
-  - Unicode quote replacement
-  - Non-ASCII character handling
-  - Punctuation normalization
-  - Whitespace normalization
-
-## âš™ï¸ Configuration
-
-All parameters are configured via the pipeline node in Dataloop.
-
-### Core Parameters
-
-| Parameter | Type | Default | Valid Values/Range | Description |
-|-----------|------|---------|-------------------|-------------|
-| `extract_tables` | boolean | `false` | `true` or `false` | When enabled, extracts all tables from the DOCX document and includes them as markdown-formatted text in the chunks. Tables are converted to markdown format with headers and rows. |
-| `extract_images` | boolean | `true` | `true` or `false` | When enabled, extracts embedded images from the DOCX document. Images are included in the extraction metadata but not directly in chunk text. |
-| `chunking_strategy` | string | `"recursive"` | `"recursive"`, `"fixed-size"`, `"nltk-sentence"`, `"nltk-paragraphs"`, or `"1-chunk"` | Determines how the extracted text is split into chunks. `recursive`: Intelligently splits text respecting semantic boundaries (recommended for most use cases). `fixed-size`: Creates uniform chunks of equal size with overlap. `nltk-sentence`: Splits by sentences using NLTK. `nltk-paragraphs`: Splits by paragraphs using NLTK. `1-chunk`: No splitting, entire document as one chunk (useful for short documents). |
-| `max_chunk_size` | integer | `300` | `1` to `2000` characters | Maximum size of each text chunk in characters. Smaller chunks provide more granular retrieval but may lose context. Larger chunks maintain more context but may be less precise. Recommended range: 300-500 for most RAG applications. This parameter works with `chunk_overlap` to control chunk boundaries. |
-| `chunk_overlap` | integer | `20` | `0` to `400` characters | Number of characters that overlap between consecutive chunks. Overlap helps maintain context across chunk boundaries and prevents information loss at split points. Should be 10-20% of `max_chunk_size`. Set to 0 for no overlap. Higher values improve context preservation but increase storage requirements. |
-| `correct_spelling` | boolean | `false` | `true` or `false` | When enabled, applies text cleaning and normalization using the `unstructured.io` library. This includes: unicode quote replacement, non-ASCII character handling, punctuation normalization, and whitespace normalization. Enable for documents with inconsistent formatting. May slow down processing. |
-
-## ðŸš€ Deployment
-
-1. Go to Dataloop Marketplace
-2. Search for "DOC to Chunks"
-3. Click "Install"
-4. Configure in your pipeline
-
-## ðŸ“Š Usage
-
-### In a Pipeline
-
-1. **Add the app node** to your pipeline
-2. **Configure parameters** in the node settings
-3. **Connect input** (DOCX items from dataset)
-4. **Connect output** (chunks will be created in target dataset)
-5. **Run the pipeline**
-
-### Example Pipeline Flow
-
-```
-[DOCX Dataset] â†’ [DOC to Chunks] â†’ [Embedding Model] â†’ [Vector Database]
-```
-
-### Example Configuration for Production
-
-```json
-{
-  "extract_tables": true,
-  "extract_images": true,
-  "chunking_strategy": "recursive",
-  "max_chunk_size": 500,
-  "chunk_overlap": 50,
-  "correct_spelling": false
-}
-```
-
-## ðŸ“¤ Output Format
-
-### Chunk Items
-
-Each chunk is uploaded as a text file (`.txt`):
-```
-{original_filename}_chunk_{index:04d}.txt
-```
-
-Example:
-```
-document.docx â†’ document_chunk_0001.txt
-              â†’ document_chunk_0002.txt
-              â†’ document_chunk_0003.txt
-```
-
-### Metadata Structure
-
-Each chunk includes comprehensive metadata for provenance tracking:
-
-```json
-{
-  "user": {
-    "source_item_id": "65f2a3b4c1e2d3f4a5b6c7d8",
-    "source_file": "example.docx",
-    "source_dataset_id": "65f2a3b4c1e2d3f4a5b6c7d9",
-    "chunk_index": 0,
-    "total_chunks": 45,
-    "extracted_chunk": true,
-    "processing_timestamp": 1698765432.123,
-    "processor": "doc",
-    "extraction_method": "python-docx",
-    "image_ids": ["img_id_1", "img_id_2", "img_id_3", "img_id_4", "img_id_5"]
-  }
-}
-```
-
-## ðŸ—ï¸ Architecture
-
-The DOC processor uses a type-safe, stateless architecture:
-
-```
-DOCProcessor (app.py)
-    â”œâ”€â”€ DOCExtractor (doc_extractor.py) - DOCX-specific extraction
-    â””â”€â”€ Transforms - Shared pipeline operations
-        â”œâ”€â”€ transforms.clean() - Text normalization
-        â”œâ”€â”€ transforms.chunk() - Text chunking
-        â””â”€â”€ transforms.upload_to_dataloop() - Chunk upload
-```
-
-**Key Components:**
-- `ExtractedData` dataclass flows through the entire pipeline
-- `Config` dataclass handles validated configuration
-- All methods are static for concurrent processing support
-
-**Pipeline Flow:**
-```python
-data = DOCExtractor.extract(data)    # Extract text, images, tables
-data = transforms.clean(data)         # Normalize text
-data = transforms.chunk(data)         # Split into chunks
-data = transforms.upload_to_dataloop(data)  # Upload chunks
-```
-
diff --git a/apps/doc_processor/__init__.py b/apps/doc_processor/__init__.py
deleted file mode 100644
index 08b2e76..0000000
--- a/apps/doc_processor/__init__.py
+++ /dev/null
@@ -1,6 +0,0 @@
-"""DOC processor app for Dataloop platform."""
-
-from .app import DOCProcessor
-from .doc_extractor import DOCExtractor
-
-__all__ = ['DOCProcessor', 'DOCExtractor']
diff --git a/apps/doc_processor/app.py b/apps/doc_processor/app.py
deleted file mode 100644
index 64366b9..0000000
--- a/apps/doc_processor/app.py
+++ /dev/null
@@ -1,58 +0,0 @@
-"""
-DOC/DOCX processor app.
-
-DOCX processor that uses DOCExtractor and ExtractedData throughout.
-"""
-
-import logging
-from typing import List
-
-import dtlpy as dl
-import nltk
-
-import transforms
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-from apps.doc_processor.doc_extractor import DOCExtractor
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class DOCProcessor(dl.BaseServiceRunner):
-    """DOCX processing application."""
-
-    def __init__(self):
-        """Initialize DOC processor."""
-        dl.client_api._upload_session_timeout = 60
-        dl.client_api._upload_chuck_timeout = 30
-
-        for resource in ['tokenizers/punkt', 'taggers/averaged_perceptron_tagger']:
-            try:
-                nltk.data.find(resource)
-            except LookupError:
-                nltk.download(resource.split('/')[-1], quiet=True)
-
-    @staticmethod
-    def run(item: dl.Item, target_dataset: dl.Dataset, context: dl.Context) -> List[dl.Item]:
-        """Process a DOCX document into chunks."""
-        config = context.node.metadata.get('customNodeConfig', {})
-        cfg = Config.from_dict(config)
-        data = ExtractedData(item=item, target_dataset=target_dataset, config=cfg)
-
-        try:
-            data = DOCExtractor.extract(data)
-            if cfg.ocr_from_images:
-                data = transforms.ocr_enhance(data)
-            if cfg.to_correct_spelling:
-                data = transforms.deep_clean(data)
-            else:
-                data = transforms.clean(data)
-            data = transforms.chunk(data)
-            data = transforms.upload_to_dataloop(data)
-
-            logger.info(f"Processed {item.name}: {len(data.uploaded_items)} chunks, {data.errors.get_summary()}")
-            return data.uploaded_items
-
-        except Exception as e:
-            logger.error(f"Processing failed: {e}", exc_info=True)
-            raise
diff --git a/apps/doc_processor/dataloop.json b/apps/doc_processor/dataloop.json
deleted file mode 100644
index 4291b32..0000000
--- a/apps/doc_processor/dataloop.json
+++ /dev/null
@@ -1,291 +0,0 @@
-{
-  "displayName": "RAG DOC Processor",
-  "name": "rag-doc-processor",
-  "scope": "public",
-  "version": "0.1.9",
-  "description": "DOC/DOCX processing service that extracts text, tables, images, applies OCR, and creates chunks for RAG pipelines. Supports multiple chunking strategies.",
-  "attributes": {
-    "Category": "Application",
-    "Provider": "Dataloop",
-    "Deployed By": "Dataloop",
-    "License": "Apache 2.0",
-    "Application Type": ["Pipeline Node"],
-    "Media Type": ["Document"]
-  },
-  "components": {
-    "computeConfigs": [
-      {
-        "name": "doc-processor-service",
-        "runtime": {
-          "podType": "regular-m",
-          "runnerImage": "gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/doc-processor:0.0.1",
-          "concurrency": 25,
-          "autoscaler": {
-            "minReplicas": 0,
-            "maxReplicas": 10,
-            "queueLength": 100
-          }
-        }
-      }
-    ],
-    "pipelineNodes": [
-      {
-        "invoke": {
-          "type": "function",
-          "namespace": "doc-processor-service.doc-processor-module.run"
-        },
-        "name": "doc-processor-node",
-        "categories": ["document-processing"],
-        "displayName": "DOC to Chunks",
-        "description": "Process DOC/DOCX documents to extract text, tables, images, apply OCR, and create chunks",
-        "scope": "project",
-        "configuration": {
-          "fields": [
-            {
-              "name": "name",
-              "title": "Node Name",
-              "props": {
-                "title": true,
-                "type": "string",
-                "default": "DOC-to-Chunks",
-                "required": true,
-                "placeholder": "Insert node name",
-                "redAsterisk": true,
-                "tooltip": "Unique name for this pipeline node"
-              },
-              "rules": [
-                {
-                  "type": "required",
-                  "effect": "error"
-                }
-              ],
-              "widget": "dl-input"
-            },
-            {
-              "name": "to_correct_spelling",
-              "title": "Apply Text Cleaning",
-              "props": {
-                "type": "boolean",
-                "title": true,
-                "default": false,
-                "tooltip": "Clean and normalize text to fix formatting inconsistencies"
-              },
-              "widget": "dl-checkbox"
-            },
-            {
-              "name": "ocr_from_images",
-              "title": "Extract Text from Images (OCR)",
-              "props": {
-                "type": "boolean",
-                "title": true,
-                "default": false,
-                "tooltip": "Extract images from DOCX and apply OCR to extract text from them"
-              },
-              "widget": "dl-checkbox"
-            },
-            {
-              "name": "ocr_integration_method",
-              "title": "OCR Integration Method",
-              "props": {
-                "type": "string",
-                "required": true,
-                "default": "append_to_page",
-                "redAsterisk": true,
-                "tooltip": "How to integrate OCR text with document text",
-                "options": [
-                  {
-                    "value": "append_to_page",
-                    "label": "Append to Page Text"
-                  },
-                  {
-                    "value": "separate_chunks",
-                    "label": "Create Separate Chunks"
-                  },
-                  {
-                    "value": "combine_all",
-                    "label": "Combine All Text"
-                  }
-                ]
-              },
-              "rules": [
-                {
-                  "type": "required",
-                  "effect": "error"
-                }
-              ],
-              "widget": "dl-select",
-              "dependsOn": [
-                {
-                  "field": "ocr_from_images",
-                  "value": true,
-                  "operator": "equals"
-                }
-              ]
-            },
-            {
-              "name": "use_markdown_extraction",
-              "title": "Use Markdown Extraction",
-              "props": {
-                "type": "boolean",
-                "title": true,
-                "default": false,
-                "tooltip": "Extract DOCX as Markdown for better structure preservation"
-              },
-              "widget": "dl-checkbox"
-            },
-            {
-              "name": "chunking_strategy",
-              "title": "Chunking Strategy",
-              "props": {
-                "type": "string",
-                "required": true,
-                "default": "recursive",
-                "redAsterisk": true,
-                "tooltip": "Method for splitting text into chunks",
-                "options": [
-                  {
-                    "value": "fixed-size",
-                    "label": "Fixed Size"
-                  },
-                  {
-                    "value": "recursive",
-                    "label": "Recursive"
-                  },
-                  {
-                    "value": "nltk-sentence",
-                    "label": "NLTK Sentence"
-                  },
-                  {
-                    "value": "nltk-paragraphs",
-                    "label": "NLTK Paragraphs"
-                  },
-                  {
-                    "value": "1-chunk",
-                    "label": "Single Chunk"
-                  }
-                ]
-              },
-              "rules": [
-                {
-                  "type": "required",
-                  "effect": "error"
-                }
-              ],
-              "widget": "dl-select"
-            },
-            {
-              "name": "max_chunk_size",
-              "title": "Max Chunk Size",
-              "props": {
-                "type": "number",
-                "default": 300,
-                "min": 1,
-                "max": 2000,
-                "step": 50,
-                "title": true,
-                "required": true,
-                "redAsterisk": true,
-                "tooltip": "Maximum number of characters per chunk"
-              },
-              "rules": [
-                {
-                  "type": "required",
-                  "effect": "error"
-                }
-              ],
-              "widget": "dl-slider",
-              "dependsOn": [
-                {
-                  "field": "chunking_strategy",
-                  "value": "1-chunk",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-sentence",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-paragraphs",
-                  "operator": "notEquals"
-                }
-              ]
-            },
-            {
-              "name": "chunk_overlap",
-              "title": "Chunk Overlap",
-              "props": {
-                "type": "number",
-                "default": 40,
-                "min": 0,
-                "max": 400,
-                "step": 10,
-                "title": true,
-                "required": true,
-                "redAsterisk": true,
-                "tooltip": "Number of overlapping characters between consecutive chunks"
-              },
-              "rules": [
-                {
-                  "type": "required",
-                  "effect": "error"
-                }
-              ],
-              "widget": "dl-slider",
-              "dependsOn": [
-                {
-                  "field": "chunking_strategy",
-                  "value": "1-chunk",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-sentence",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-paragraphs",
-                  "operator": "notEquals"
-                }
-              ]
-            }
-          ]
-        }
-      }
-    ],
-    "modules": [
-      {
-        "name": "doc-processor-module",
-        "computeConfig": "doc-processor-service",
-        "entryPoint": "apps/doc_processor/app.py",
-        "className": "DOCProcessor",
-        "initInputs": [],
-        "functions": [
-          {
-            "name": "run",
-            "input": [
-              {
-                "type": "Item",
-                "name": "item"
-              },
-              {
-                "type": "Dataset",
-                "name": "target_dataset"
-              }
-            ],
-            "output": [
-              {
-                "type": "Item[]",
-                "name": "items"
-              }
-            ],
-            "displayIcon": "icon-dl-task",
-            "displayName": "DOC to Chunks"
-          }
-        ]
-      }
-    ]
-  }
-}
diff --git a/apps/doc_processor/doc_extractor.py b/apps/doc_processor/doc_extractor.py
deleted file mode 100644
index ad8d125..0000000
--- a/apps/doc_processor/doc_extractor.py
+++ /dev/null
@@ -1,279 +0,0 @@
-"""
-DOCX extraction logic.
-
-Handles DOCX-specific extraction operations:
-- Text extraction from paragraphs
-- Image extraction from embedded resources
-- Table extraction with markdown conversion
-"""
-
-import io
-import logging
-import os
-import tempfile
-from typing import List, Dict, Tuple, Optional
-
-from docx import Document
-from PIL import Image
-
-from utils.extracted_data import ExtractedData
-from utils.data_types import ImageContent, TableContent
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class DOCExtractor:
-    """DOCX extraction operations."""
-
-    @staticmethod
-    def extract(data: ExtractedData) -> ExtractedData:
-        """Extract content from DOCX item."""
-        data.current_stage = "extraction"
-
-        if not data.item:
-            data.log_error("No item provided for extraction")
-            return data
-
-        try:
-            with tempfile.TemporaryDirectory() as temp_dir:
-                file_path = data.item.download(local_path=temp_dir)
-                doc = Document(file_path)
-
-                # Extract images if configured
-                if data.config.extract_images:
-                    data.images = DOCExtractor._extract_images(doc, temp_dir)
-
-                # Extract tables if configured
-                if data.config.extract_tables:
-                    data.tables = DOCExtractor._extract_tables(doc)
-
-                # Extract content based on use_markdown_extraction setting
-                use_markdown = data.config.use_markdown_extraction
-
-                if use_markdown:
-                    data.content_text = DOCExtractor._extract_markdown(doc, data.tables)
-                else:
-                    data.content_text = DOCExtractor._extract_plain_text(doc)
-
-                # Set metadata
-                data.metadata = {
-                    'source_file': data.item_name,
-                    'extraction_method': 'python-docx',
-                    'format': 'markdown' if use_markdown else 'plain',
-                    'image_count': len(data.images),
-                    'table_count': len(data.tables),
-                    'processor': 'doc',
-                }
-
-        except Exception as e:
-            data.log_error("Document extraction failed. Check logs for details.")
-            logger.exception(f"DOCX extraction error: {e}")
-
-        return data
-
-    @staticmethod
-    def _extract_plain_text(doc: Document) -> str:
-        """Extract plain text from DOCX without formatting."""
-        return '\n\n'.join(para.text for para in doc.paragraphs if para.text.strip())
-
-    @staticmethod
-    def _extract_markdown(doc: Document, tables: List[TableContent]) -> str:
-        """Convert DOCX content to markdown format."""
-        md_parts = []
-        table_iter = iter(tables)
-        current_table = next(table_iter, None)
-
-        for element in doc.element.body:
-            tag = element.tag.split('}')[-1]
-
-            if tag == 'p':
-                para = doc.paragraphs[DOCExtractor._get_para_index(doc, element)]
-                md_line = DOCExtractor._paragraph_to_markdown(para)
-                if md_line:
-                    md_parts.append(md_line)
-
-            elif tag == 'tbl' and current_table:
-                # Insert table markdown inline
-                md_parts.append(current_table.markdown)
-                current_table = next(table_iter, None)
-
-        return '\n\n'.join(md_parts)
-
-    @staticmethod
-    def _get_para_index(doc: Document, element) -> int:
-        """Get paragraph index for an element."""
-        for i, para in enumerate(doc.paragraphs):
-            if para._element is element:
-                return i
-        return 0
-
-    @staticmethod
-    def _paragraph_to_markdown(para) -> str:
-        """Convert a paragraph to markdown."""
-        text = para.text.strip()
-        if not text:
-            return ""
-
-        style_name = para.style.name.lower() if para.style else ""
-
-        # Handle headings
-        if 'heading' in style_name:
-            level = DOCExtractor._get_heading_level(style_name)
-            return f"{'#' * level} {text}"
-
-        # Handle list items
-        if DOCExtractor._is_list_item(para):
-            if DOCExtractor._is_numbered_list(para):
-                return f"1. {text}"
-            return f"- {text}"
-
-        # Handle regular paragraph with inline formatting
-        return DOCExtractor._apply_inline_formatting(para)
-
-    @staticmethod
-    def _get_heading_level(style_name: str) -> int:
-        """Extract heading level from style name."""
-        for i in range(1, 10):
-            if str(i) in style_name:
-                return i
-        return 1
-
-    @staticmethod
-    def _is_list_item(para) -> bool:
-        """Check if paragraph is a list item."""
-        pPr = para._element.find('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}pPr')
-        if pPr is not None:
-            numPr = pPr.find('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}numPr')
-            return numPr is not None
-        return False
-
-    @staticmethod
-    def _is_numbered_list(para) -> bool:
-        """Check if list item is numbered."""
-        pPr = para._element.find('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}pPr')
-        if pPr is not None:
-            numPr = pPr.find('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}numPr')
-            if numPr is not None:
-                numId = numPr.find('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}numId')
-                if numId is not None:
-                    val = numId.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')
-                    # numId > 1 typically indicates numbered list
-                    return val and int(val) > 1
-        return False
-
-    @staticmethod
-    def _apply_inline_formatting(para) -> str:
-        """Apply bold/italic markdown formatting to runs."""
-        result = []
-        for run in para.runs:
-            text = run.text
-            if not text:
-                continue
-            if run.bold and run.italic:
-                text = f"***{text}***"
-            elif run.bold:
-                text = f"**{text}**"
-            elif run.italic:
-                text = f"*{text}*"
-            result.append(text)
-        return ''.join(result) or para.text
-
-    @staticmethod
-    def _extract_images(doc: Document, temp_dir: str) -> List[ImageContent]:
-        """Extract embedded images from DOCX with size metadata."""
-        images = []
-
-        for img_index, rel in enumerate(doc.part.rels.values()):
-            if "image" not in rel.target_ref:
-                continue
-
-            try:
-                blob = rel.target_part.blob
-                original_filename = rel.target_ref.split('/')[-1]
-                ext = original_filename.split('.')[-1] if '.' in original_filename else 'png'
-
-                # Use index prefix to avoid duplicate filenames
-                filename = f"img{img_index}_{original_filename}"
-                image_path = os.path.join(temp_dir, filename)
-
-                with open(image_path, 'wb') as f:
-                    f.write(blob)
-
-                # Extract image dimensions using PIL
-                size = DOCExtractor._get_image_size(blob)
-
-                images.append(ImageContent(path=image_path, format=ext, size=size))
-            except (IOError, OSError, ValueError, KeyError) as e:
-                logger.warning(f"Failed to extract image {img_index} from DOCX: {e}")
-
-        return images
-
-    @staticmethod
-    def _get_image_size(blob: bytes) -> Optional[Tuple[int, int]]:
-        """Extract image dimensions from blob data."""
-        try:
-            with Image.open(io.BytesIO(blob)) as img:
-                return img.size
-        except (IOError, OSError, ValueError):
-            return None
-
-    @staticmethod
-    def _extract_tables(doc: Document) -> List[TableContent]:
-        """Extract tables from DOCX with markdown conversion."""
-        tables = []
-
-        for table_index, table in enumerate(doc.tables):
-            try:
-                if not table.rows:
-                    continue
-
-                # Extract headers, handling merged cells by deduplicating
-                headers = DOCExtractor._get_unique_row_cells(table.rows[0])
-                if not headers:
-                    continue
-
-                # Extract data rows
-                rows = []
-                for row in table.rows[1:]:
-                    cell_values = DOCExtractor._get_unique_row_cells(row)
-                    row_data = {}
-                    for i, value in enumerate(cell_values):
-                        if i < len(headers):
-                            row_data[headers[i]] = value
-                    rows.append(row_data)
-
-                markdown = DOCExtractor._table_to_markdown(headers, rows)
-                tables.append(TableContent(data=rows, markdown=markdown))
-            except (ValueError, AttributeError, IndexError) as e:
-                logger.warning(f"Failed to extract table {table_index}: {e}")
-
-        return tables
-
-    @staticmethod
-    def _get_unique_row_cells(row) -> List[str]:
-        """Extract cell values from a row, handling merged cells."""
-        seen_cells = set()
-        values = []
-        for cell in row.cells:
-            # Skip duplicate cell references (merged cells)
-            cell_id = id(cell._tc)
-            if cell_id in seen_cells:
-                continue
-            seen_cells.add(cell_id)
-            values.append(cell.text.strip())
-        return values
-
-    @staticmethod
-    def _table_to_markdown(headers: List[str], rows: List[Dict[str, str]]) -> str:
-        """Convert table data to markdown format."""
-        if not headers:
-            return ""
-
-        md = "| " + " | ".join(headers) + " |\n"
-        md += "| " + " | ".join(["---"] * len(headers)) + " |\n"
-
-        for row in rows:
-            values = [str(row.get(h, '')) for h in headers]
-            md += "| " + " | ".join(values) + " |\n"
-
-        return md
diff --git a/apps/doc_processor/requirements.txt b/apps/doc_processor/requirements.txt
deleted file mode 100644
index 079de54..0000000
--- a/apps/doc_processor/requirements.txt
+++ /dev/null
@@ -1,16 +0,0 @@
-# Core Dataloop SDK
-dtlpy>=1.0.0
-
-# Data Processing
-pandas>=2.0.0
-
-# Document Processing
-python-docx>=1.1.0
-
-# Image Processing
-Pillow>=10.0.0
-
-# Text Processing
-unstructured>=0.10.0
-nltk>=3.8.0
-langchain-text-splitters>=0.0.1
diff --git a/apps/pdf-processor/Dockerfile b/apps/pdf-processor/Dockerfile
new file mode 100644
index 0000000..87ac360
--- /dev/null
+++ b/apps/pdf-processor/Dockerfile
@@ -0,0 +1,20 @@
+FROM hub.dataloop.ai/dtlpy-runner-images/cpu:python3.10_opencv
+USER 1000
+
+# Install Python dependencies
+RUN pip install -U \
+    pypdfium2==4.28.0 \
+    pypdf==4.2.0 \
+    nltk==3.8.1 \
+    PyMuPDF==1.24.0 \
+    autocorrect==2.6.1 \
+    langchain==0.1.14 \
+    requests-toolbelt==1.0.0 \
+    numpy==1.26.4 \
+    "unstructured[all-docs]==0.12.0" \
+    pymupdf4llm \
+    easyocr
+    
+# docker build --no-cache -t gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/pdf-processor:0.0.1 -f Dockerfile .
+# docker push gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/pdf-processor:0.0.1
+
diff --git a/apps/pdf_processor/README.md b/apps/pdf-processor/README.md
similarity index 62%
rename from apps/pdf_processor/README.md
rename to apps/pdf-processor/README.md
index ed9c974..ddab213 100644
--- a/apps/pdf_processor/README.md
+++ b/apps/pdf-processor/README.md
@@ -1,28 +1,20 @@
 # PDF Processor
 
-A modular Dataloop application for processing PDF files into RAG-ready chunks with ML-enhanced extraction and flexible OCR.
+A Dataloop application that extracts text from PDF documents, applies OCR to images, and creates chunks for Retrieval-Augmented Generation (RAG) workflows.
 
 ## ðŸŽ¯ Features
 
 ### Text Extraction
-- **Plain Text**: Standard extraction using PyMuPDF (fitz)
-- **ML-Enhanced Markdown**: PyMuPDF Layout for improved structure preservation
-  - ML-based layout analysis
-  - Intelligent OCR evaluation (uses Tesseract when beneficial)
-  - Better header/footer detection
-  - Improved multi-column and table handling
-
-### Chunk Upload
-- **Bulk Upload**: Upload all chunks in a single operation using pandas DataFrame
-- **Per-Chunk Metadata**: Track page numbers, images, extraction method per chunk
-- **Resilient Fallback**: Automatic retry with individual uploads if needed
+- **Plain Text**: Fast extraction using PyMuPDF (fitz)
+- **Markdown-Aware**: Preserves document structure (headers, lists, tables) using `pymupdf4llm`
 
 ### OCR from Images
 Extract embedded images and apply OCR to extract text:
 
-- **Flexible OCR Backends**: Dataloop models, EasyOCR fallback, or Tesseract via PyMuPDF Layout
-- **Local Processing**: Images processed as temporary files without creating Dataloop items
-- **Multiple Integration Methods**: Choose how OCR text integrates with document text
+- âœ… Local processing, no upload required
+- âœ… Processes images as temporary files
+- âœ… No Dataloop items created
+- âœ… Fast and efficient text extraction
 
 **OCR Integration Methods**
 - `append_to_page`: Attaches OCR text to corresponding page (maintains structure)
@@ -58,7 +50,7 @@ All parameters are configured via the pipeline node in Dataloop.
 | `chunking_strategy` | string | `"recursive"` | `"recursive"`, `"fixed-size"`, `"nltk-sentence"`, `"nltk-paragraphs"`, or `"1-chunk"` | Determines how the extracted text is split into chunks. `recursive`: Intelligently splits text respecting semantic boundaries (recommended for most use cases). `fixed-size`: Creates uniform chunks of equal size with overlap. `nltk-sentence`: Splits by sentences using NLTK. `nltk-paragraphs`: Splits by paragraphs using NLTK. `1-chunk`: No splitting, entire document as one chunk (useful for short documents). |
 | `max_chunk_size` | integer | `300` | `1` to `2000` characters | Maximum size of each text chunk in characters. Smaller chunks provide more granular retrieval but may lose context. Larger chunks maintain more context but may be less precise. Recommended range: 300-500 for most RAG applications. This parameter works with `chunk_overlap` to control chunk boundaries. |
 | `chunk_overlap` | integer | `20` | `0` to `400` characters | Number of characters that overlap between consecutive chunks. Overlap helps maintain context across chunk boundaries and prevents information loss at split points. Should be 10-20% of `max_chunk_size`. Set to 0 for no overlap. Higher values improve context preservation but increase storage requirements. |
-| `correct_spelling` | boolean | `false` | `true` or `false` | When enabled, applies text cleaning and normalization using the `unstructured.io` library. This includes: unicode quote replacement, non-ASCII character handling, punctuation normalization, and whitespace normalization. Enable for documents with OCR errors or inconsistent formatting. May slow down processing. |
+| `to_correct_spelling` | boolean | `false` | `true` or `false` | When enabled, applies text cleaning and normalization using the `unstructured.io` library. This includes: unicode quote replacement, non-ASCII character handling, punctuation normalization, and whitespace normalization. Enable for documents with OCR errors or inconsistent formatting. May slow down processing. |
 
 ### OCR Configuration
 
@@ -107,7 +99,7 @@ OCR processing uses **EasyOCR** for local, efficient text extraction from images
   "chunking_strategy": "recursive",
   "max_chunk_size": 500,
   "chunk_overlap": 50,
-  "correct_spelling": false
+  "to_correct_spelling": false
 }
 ```
 
@@ -129,56 +121,66 @@ document.pdf â†’ document_chunk_0001.txt
 
 ### Metadata Structure
 
-Each chunk includes comprehensive metadata for provenance tracking:
+The processor creates two types of metadata:
+
+#### 1. Original PDF Item Metadata
+
+The original PDF document is updated with processing information:
 
 ```json
 {
   "user": {
-    "source_item_id": "65f2a3b4c1e2d3f4a5b6c7d8",
-    "source_file": "example.pdf",
-    "source_dataset_id": "65f2a3b4c1e2d3f4a5b6c7d9",
-    "chunk_index": 0,
+    "total_pages": 25,
     "total_chunks": 120,
-    "extracted_chunk": true,
-    "processing_timestamp": 1698765432.123,
-    "processor": "pdf",
-    "extraction_method": "pymupdf4llm_layout",
-    "layout_enhancement": true,
-    "page_numbers": [1, 2],
-    "image_ids": ["img_id_1", "img_id_2"]
+    "chunks_dataset_id": "65f2a3b4c1e2d3f4a5b6c7d9",
+    "extraction_method": "pymupdf4llm",
+    "extraction_format": "markdown",
+    "chunking_strategy": "recursive",
+    "chunk_size": 500,
+    "chunk_overlap": 50,
+    "ocr_enabled": true,
+    "ocr_integration_method": "append_to_page",
+    "text_cleaning_enabled": false,
+    "processing_timestamp": "2024-10-26T10:30:00Z"
   }
 }
 ```
 
-**Key Metadata Fields:**
-- `extraction_method`: `"pymupdf4llm_layout"` (ML-enhanced) or `"pymupdf4llm"` (standard) or `"pymupdf"` (plain text)
-- `layout_enhancement`: `true` if PyMuPDF Layout was active during extraction
-- `page_numbers`: List of source pages for this chunk
-- `image_ids`: IDs of associated images (if any)
-- `chunk_index` / `total_chunks`: Position in document for reconstruction
+**Fields:**
+- `total_pages`: Number of pages in the PDF
+- `total_chunks`: Total number of chunks created
+- `chunks_dataset_id`: Dataset ID where chunks were stored
+- `extraction_method`: Library used (`pymupdf4llm` or `fitz`)
+- `extraction_format`: Output format (`markdown` or `plain`)
+- `chunking_strategy`: Strategy used (`recursive`, `fixed-size`, etc.)
+- `chunk_size`: Maximum chunk size in characters
+- `chunk_overlap`: Overlap between chunks in characters
+- `ocr_enabled`: Whether OCR processing was applied
+- `ocr_integration_method`: How OCR text was integrated (only if OCR enabled)
+- `text_cleaning_enabled`: Whether text cleaning was applied
+- `processing_timestamp`: ISO 8601 timestamp (UTC) of processing
 
-## ðŸ—ï¸ Architecture
+#### 2. Chunk Item Metadata
 
-The PDF processor uses a type-safe, stateless architecture:
+Each chunk item contains minimal reference information:
 
-```
-PDFProcessor (app.py)
-    â”œâ”€â”€ PDFExtractor (pdf_extractor.py) - PDF-specific extraction
-    â””â”€â”€ Transforms - Shared pipeline operations
-        â”œâ”€â”€ transforms.clean() - Text normalization
-        â”œâ”€â”€ transforms.chunk() - Text chunking
-        â””â”€â”€ transforms.upload_to_dataloop() - Chunk upload
+```json
+{
+  "user": {
+    "document": "example.pdf",
+    "document_type": "application/pdf",
+    "chunk_index": 1,
+    "total_chunks": 120,
+    "original_item_id": "65f2a3b4c1e2d3f4a5b6c7d8",
+    "original_dataset_id": "65f2a3b4c1e2d3f4a5b6c7d0"
+  }
+}
 ```
 
-**Key Components:**
-- `ExtractedData` dataclass flows through the entire pipeline
-- `Config` dataclass handles validated configuration
-- All methods are static for concurrent processing support
-
-**Pipeline Flow:**
-```python
-data = PDFExtractor.extract(data)    # Extract text, images, tables
-data = transforms.clean(data)         # Normalize text
-data = transforms.chunk(data)         # Split into chunks
-data = transforms.upload_to_dataloop(data)  # Upload chunks
-```
+**Fields:**
+- `document`: Original filename
+- `document_type`: MIME type of the original document
+- `chunk_index`: Index of this chunk (1-based, starts at 1)
+- `total_chunks`: Total number of chunks from the document
+- `original_item_id`: Dataloop item ID of the original PDF
+- `original_dataset_id`: Dataloop dataset ID of the original PDF
diff --git a/apps/pdf_processor/dataloop.json b/apps/pdf-processor/dataloop.json
similarity index 70%
rename from apps/pdf_processor/dataloop.json
rename to apps/pdf-processor/dataloop.json
index d10108b..8914abd 100644
--- a/apps/pdf_processor/dataloop.json
+++ b/apps/pdf-processor/dataloop.json
@@ -1,8 +1,8 @@
 {
-  "displayName": "RAG PDF Processor",
-  "name": "rag-pdf-processor",
+  "displayName": "PDF to Chunks",
+  "name": "pdf_to_chunks",
   "scope": "public",
-  "version": "0.1.9",
+  "version": "0.0.1",
   "description": "PDF processing service that extracts text, applies OCR, and creates chunks for RAG pipelines. Supports markdown extraction and multiple chunking strategies.",
   "attributes": {
     "Category": "Application",
@@ -16,6 +16,11 @@
       "Document"
     ]
   },
+  "codebase": {
+    "type": "git",
+    "gitUrl": "https://github.com/dataloop-ai-apps/rag-multimodal-processors.git",
+    "gitTag": "0.1.0"
+  },
   "components": {
     "computeConfigs": [
       {
@@ -36,10 +41,12 @@
       {
         "invoke": {
           "type": "function",
-          "namespace": "pdf-processor-service.pdf-processor-module.run"
+          "namespace": "pdf-processor-service.pdf-processor-module.process_document"
         },
-        "name": "pdf-processor-node",
-        "categories": ["document-processing"],
+        "name": "process_document",
+        "categories": [
+          "document-processing"
+        ],
         "displayName": "PDF to Chunks",
         "description": "Process PDF documents to extract text, apply OCR, and create chunks",
         "scope": "project",
@@ -53,9 +60,7 @@
                 "type": "string",
                 "default": "PDF-to-Chunks",
                 "required": true,
-                "placeholder": "Insert node name",
-                "redAsterisk": true,
-                "tooltip": "Unique name for this pipeline node"
+                "placeholder": "Insert node name"
               },
               "rules": [
                 {
@@ -65,17 +70,6 @@
               ],
               "widget": "dl-input"
             },
-            {
-              "name": "to_correct_spelling",
-              "title": "Apply Text Cleaning",
-              "props": {
-                "type": "boolean",
-                "title": true,
-                "default": false,
-                "tooltip": "Clean and normalize text to fix OCR errors and inconsistent formatting"
-              },
-              "widget": "dl-checkbox"
-            },
             {
               "name": "ocr_from_images",
               "title": "Extract Text from Images (OCR)",
@@ -83,7 +77,7 @@
                 "type": "boolean",
                 "title": true,
                 "default": false,
-                "tooltip": "Extract images from PDF and apply OCR to extract text from them"
+                "description": "Extract images from PDF and apply OCR to extract text from them"
               },
               "widget": "dl-checkbox"
             },
@@ -94,8 +88,6 @@
                 "type": "string",
                 "required": true,
                 "default": "append_to_page",
-                "redAsterisk": true,
-                "tooltip": "How to integrate OCR text: append to page, create separate chunks, or combine all",
                 "options": [
                   {
                     "value": "append_to_page",
@@ -117,14 +109,7 @@
                   "effect": "error"
                 }
               ],
-              "widget": "dl-select",
-              "dependsOn": [
-                {
-                  "field": "ocr_from_images",
-                  "value": true,
-                  "operator": "equals"
-                }
-              ]
+              "widget": "dl-select"
             },
             {
               "name": "use_markdown_extraction",
@@ -133,7 +118,7 @@
                 "type": "boolean",
                 "title": true,
                 "default": false,
-                "tooltip": "Extract PDF as Markdown to preserve document structure and formatting"
+                "description": "Extract PDF as Markdown for better structure preservation"
               },
               "widget": "dl-checkbox"
             },
@@ -144,8 +129,6 @@
                 "type": "string",
                 "required": true,
                 "default": "recursive",
-                "redAsterisk": true,
-                "tooltip": "Method for splitting text into chunks",
                 "options": [
                   {
                     "value": "fixed-size",
@@ -187,9 +170,7 @@
                 "max": 2000,
                 "step": 50,
                 "title": true,
-                "required": true,
-                "redAsterisk": true,
-                "tooltip": "Maximum number of characters per chunk"
+                "required": true
               },
               "rules": [
                 {
@@ -197,38 +178,19 @@
                   "effect": "error"
                 }
               ],
-              "widget": "dl-slider",
-              "dependsOn": [
-                {
-                  "field": "chunking_strategy",
-                  "value": "1-chunk",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-sentence",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-paragraphs",
-                  "operator": "notEquals"
-                }
-              ]
+              "widget": "dl-slider"
             },
             {
               "name": "chunk_overlap",
               "title": "Chunk Overlap",
               "props": {
                 "type": "number",
-                "default": 40,
+                "default": 50,
                 "min": 0,
                 "max": 400,
                 "step": 10,
                 "title": true,
-                "required": true,
-                "redAsterisk": true,
-                "tooltip": "Number of overlapping characters between consecutive chunks"
+                "required": true
               },
               "rules": [
                 {
@@ -236,24 +198,18 @@
                   "effect": "error"
                 }
               ],
-              "widget": "dl-slider",
-              "dependsOn": [
-                {
-                  "field": "chunking_strategy",
-                  "value": "1-chunk",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-sentence",
-                  "operator": "notEquals"
-                },
-                {
-                  "field": "chunking_strategy",
-                  "value": "nltk-paragraphs",
-                  "operator": "notEquals"
-                }
-              ]
+              "widget": "dl-slider"
+            },
+            {
+              "name": "to_correct_spelling",
+              "title": "Apply Text Cleaning",
+              "props": {
+                "type": "boolean",
+                "title": true,
+                "default": false,
+                "description": "Apply text cleaning and normalization to fix OCR errors and inconsistent formatting"
+              },
+              "widget": "dl-checkbox"
             }
           ]
         }
@@ -263,12 +219,12 @@
       {
         "name": "pdf-processor-module",
         "computeConfig": "pdf-processor-service",
-        "entryPoint": "apps/pdf_processor/app.py",
+        "entryPoint": "apps/pdf-processor/pdf_processor.py",
         "className": "PDFProcessor",
         "initInputs": [],
         "functions": [
           {
-            "name": "run",
+            "name": "process_document",
             "input": [
               {
                 "type": "Item",
diff --git a/apps/pdf-processor/pdf_processor.py b/apps/pdf-processor/pdf_processor.py
new file mode 100644
index 0000000..0813f1c
--- /dev/null
+++ b/apps/pdf-processor/pdf_processor.py
@@ -0,0 +1,631 @@
+"""
+PDF Processor Application for Dataloop.
+Extracts text from PDFs, applies OCR, and creates chunks for RAG.
+"""
+
+from typing import List, Dict, Any
+import dtlpy as dl
+import logging
+import tempfile
+import fitz
+import pymupdf4llm
+import os
+import sys
+
+# Add parent directories to path for shared imports
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
+
+from chunkers.text_chunker import TextChunker
+from extractors.ocr_extractor import OCRExtractor
+from utils.text_cleaning import clean_text
+from utils.dataloop_helpers import upload_chunks, cleanup_temp_items_and_folder
+from utils.chunk_metadata import ChunkMetadata
+
+logger = logging.getLogger('pdf-processor')
+
+
+class PDFProcessor(dl.BaseServiceRunner):
+    """
+    PDF Processor for extracting text, applying OCR, and creating chunks.
+    
+    Supports:
+    - Text extraction (plain and markdown-aware)
+    - Image extraction and OCR
+    - Multiple chunking strategies
+    - Text cleaning and normalization
+    """
+
+    def __init__(self):
+        """Initialize the PDF processor."""
+        # Configure Dataloop client timeouts
+        dl.client_api._upload_session_timeout = 60
+        dl.client_api._upload_chuck_timeout = 30
+        
+        # Download required NLTK data (only if not already present)
+        import nltk
+        try:
+            nltk.data.find('tokenizers/punkt')
+        except LookupError:
+            logger.info("Downloading NLTK punkt tokenizer")
+            nltk.download('punkt', quiet=True)
+        
+        try:
+            nltk.data.find('taggers/averaged_perceptron_tagger')
+        except LookupError:
+            logger.info("Downloading NLTK averaged_perceptron_tagger")
+            nltk.download('averaged_perceptron_tagger', quiet=True)
+        
+        logger.info("PDFProcessor initialized")
+
+    def process_document(self, item: dl.Item, target_dataset: dl.Dataset, context: dl.Context) -> List[dl.Item]:
+        """
+        Main entry point for PDF processing.
+        
+        Args:
+            item (dl.Item): PDF item to process
+            target_dataset (dl.Dataset): Target dataset for storing chunks
+            context (dl.Context): Processing context with configuration
+            
+        Returns:
+            List[dl.Item]: List of chunk items
+        """
+        logger.info(
+            f"Processing PDF | item_id={item.id} name={item.name} mimetype={item.mimetype} "
+            f"target_dataset={target_dataset.name}"
+        )
+        
+        # Get configuration from node
+        node = context.node
+        config = node.metadata['customNodeConfig']
+        
+        # Extract configuration parameters
+        ocr_from_images = config.get('ocr_from_images', False)
+        ocr_integration_method = config.get('ocr_integration_method', 'append_to_page')
+        use_markdown_extraction = config.get('use_markdown_extraction', False)
+        chunking_strategy = config.get('chunking_strategy', 'recursive')
+        max_chunk_size = config.get('max_chunk_size', 300)
+        chunk_overlap = config.get('chunk_overlap', 20)
+        to_correct_spelling = config.get('to_correct_spelling', False)
+        
+        # Hardcoded values (not configurable by user)
+        remote_path_for_chunks = '/chunks'
+
+        logger.info(
+            f"Config | markdown={use_markdown_extraction} ocr_from_images={ocr_from_images} "
+            f"ocr_method=EasyOCR strategy={chunking_strategy} chunk_size={max_chunk_size}"
+        )
+
+        # Extract content
+        combined_text, total_pages = self._extract_content(
+            item, ocr_from_images, ocr_integration_method, use_markdown_extraction
+        )
+        
+        # Create chunker
+        chunker = TextChunker(
+            chunk_size=max_chunk_size,
+            chunk_overlap=chunk_overlap,
+            strategy=chunking_strategy,
+            use_markdown_splitting=use_markdown_extraction
+        )
+        
+        # Create chunks
+        chunks = chunker.chunk(combined_text)
+        logger.info(f"Created {len(chunks)} chunks")
+        
+        # Apply cleaning if requested
+        if to_correct_spelling:
+            logger.info("Applying text cleaning")
+            chunks = [clean_text(chunk) for chunk in chunks]
+        
+        # Update original item metadata with processing info
+        self._update_original_item_metadata(
+            item=item,
+            total_pages=total_pages,
+            total_chunks=len(chunks),
+            target_dataset_id=target_dataset.id,
+            use_markdown_extraction=use_markdown_extraction,
+            chunking_strategy=chunking_strategy,
+            max_chunk_size=max_chunk_size,
+            chunk_overlap=chunk_overlap,
+            ocr_from_images=ocr_from_images,
+            ocr_integration_method=ocr_integration_method,
+            to_correct_spelling=to_correct_spelling
+        )
+        
+        # Upload chunks
+        chunked_items = upload_chunks(
+            chunks=chunks,
+            original_item=item,
+            target_dataset=target_dataset,
+            remote_path=remote_path_for_chunks
+        )
+        
+        logger.info(
+            f"Processing completed | chunks={len(chunked_items)} dataset={target_dataset.name}"
+        )
+        return chunked_items
+
+    def _extract_content(self, item: dl.Item, ocr_from_images: bool,
+                        ocr_integration_method: str, use_markdown_extraction: bool) -> tuple[str, int]:
+        """
+        Extract content from PDF item.
+        
+        Returns:
+            tuple: (combined_text, total_pages)
+        """
+        with tempfile.TemporaryDirectory() as temp_dir:
+            item_local_path = item.download(local_path=temp_dir)
+            logger.info(f"Downloaded item | path={item_local_path}")
+            
+            if ocr_from_images and not use_markdown_extraction:
+                page_texts, total_pages, images = self._extract_text_and_images(item_local_path, item.id)
+                
+                logger.info(f"OCR enabled | using EasyOCR")
+                ocr_extractor = OCRExtractor(model_id=None)
+                ocr_texts = self._process_images_with_easyocr(images, item.id, ocr_extractor)
+            else:
+                # Extract text and images separately (markdown or no OCR not used)
+                if use_markdown_extraction:
+                    page_texts, total_pages = self._extract_text_as_markdown(item_local_path, item.id)
+                else:
+                    page_texts, total_pages = self._extract_text(item_local_path, item.id)
+                
+                ocr_texts = []
+                if ocr_from_images:
+                    logger.info(f"OCR enabled | using EasyOCR")
+                    ocr_extractor = OCRExtractor(model_id=None)
+                    ocr_texts = self._extract_and_ocr_with_easyocr(
+                        item_local_path, item.id, ocr_extractor
+                    )
+            
+            combined_text = self._combine_texts(page_texts, ocr_texts, ocr_integration_method)
+            logger.info(f"Combined text | length={len(combined_text)} pages={total_pages}")
+            
+            return combined_text, total_pages
+
+    def _extract_text(self, pdf_path: str, item_id: str) -> tuple[List[str], int]:
+        """
+        Extract text from PDF using PyMuPDF.
+        
+        Returns:
+            tuple: (page_texts, total_pages)
+        """
+        logger.info(f"Extracting text | item_id={item_id}")
+        
+        with fitz.open(pdf_path) as doc:
+            total_pages = len(doc)
+            page_texts = []
+            for page in doc:
+                page_texts.append(page.get_text())
+            
+            logger.info(f"Extracted {len(page_texts)} pages")
+            return page_texts, total_pages
+    
+    def _extract_text_and_images(self, pdf_path: str, item_id: str) -> tuple[List[str], int, List[Dict[str, Any]]]:
+        """
+        Extract both text and images from PDF in a single pass.
+        
+        Returns:
+            tuple: (page_texts, total_pages, images)
+        """
+        logger.info(f"Extracting text and images in single pass | item_id={item_id}")
+        
+        page_texts = []
+        images = []
+        
+        with fitz.open(pdf_path) as pdf_file:
+            total_pages = len(pdf_file)
+            
+            for page_index in range(total_pages):
+                page = pdf_file.load_page(page_index)
+                
+                # Extract text from this page
+                page_texts.append(page.get_text())
+                
+                # Extract images from this page
+                image_list = page.get_images(full=True)
+                for image_index, img in enumerate(image_list):
+                    try:
+                        xref = img[0]
+                        base_image = pdf_file.extract_image(xref)
+                        images.append({
+                            'bytes': base_image["image"],
+                            'page_index': page_index,
+                            'image_index': image_index,
+                            'extension': base_image.get("ext", "png")
+                        })
+                    except Exception as e:
+                        logger.warning(f"Failed to extract image page={page_index} img={image_index}: {e}")
+                        continue
+        
+        logger.info(f"Extracted {total_pages} pages and {len(images)} images in single pass")
+        return page_texts, total_pages, images
+
+    def _extract_text_as_markdown(self, pdf_path: str, item_id: str) -> tuple[List[str], int]:
+        """
+        Extract text as markdown using pymupdf4llm.
+        
+        Returns:
+            tuple: (page_texts, total_pages)
+        """
+        logger.info(f"Extracting markdown | item_id={item_id}")
+        
+        try:
+            md_text = pymupdf4llm.to_markdown(
+                pdf_path,
+                page_chunks=True,
+                write_images=False,
+                show_progress=True
+            )
+            
+            if isinstance(md_text, list):
+                page_texts = []
+                for page_data in md_text:
+                    if isinstance(page_data, dict):
+                        page_texts.append(page_data.get('text', ''))
+                    else:
+                        page_texts.append(str(page_data))
+            elif isinstance(md_text, str):
+                page_texts = md_text.split('\n-----\n')
+            else:
+                logger.warning("Unexpected markdown format, falling back")
+                return self._extract_text(pdf_path, item_id)
+            
+            total_pages = len(page_texts)
+            logger.info(f"Extracted {total_pages} pages as markdown")
+            return page_texts, total_pages
+            
+        except Exception as e:
+            logger.exception(f"Markdown extraction failed: {e}")
+            return self._extract_text(pdf_path, item_id)
+
+    def _extract_images_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
+        """
+        Extract all images from PDF.
+        
+        Returns:
+            List of dicts with: bytes, page_index, image_index, extension
+        """
+        images = []
+        
+        with fitz.open(pdf_path) as pdf_file:
+            for page_index in range(len(pdf_file)):
+                page = pdf_file.load_page(page_index)
+                image_list = page.get_images(full=True)
+                
+                for image_index, img in enumerate(image_list):
+                    try:
+                        xref = img[0]
+                        base_image = pdf_file.extract_image(xref)
+                        images.append({
+                            'bytes': base_image["image"],
+                            'page_index': page_index,
+                            'image_index': image_index,
+                            'extension': base_image.get("ext", "png")
+                        })
+                    except Exception as e:
+                        logger.warning(f"Failed to extract image page={page_index} img={image_index}: {e}")
+                        continue
+        
+        logger.info(f"Extracted {len(images)} images from PDF")
+        return images
+    
+    def _process_images_with_easyocr(self, images: List[Dict[str, Any]], item_id: str,
+                                     ocr_extractor: OCRExtractor) -> List[Dict[str, Any]]:
+        """
+        Process already-extracted images with EasyOCR.
+        Uses temporary files, no upload to Dataloop.
+        
+        Args:
+            images: List of image dictionaries with 'bytes', 'page_index', 'image_index', 'extension'
+            item_id: Item ID for logging
+            ocr_extractor: OCR extractor instance
+            
+        Returns:
+            List of OCR results with page/image indices
+        """
+        if not images:
+            logger.info("No images to process")
+            return []
+        
+        logger.info(f"Processing {len(images)} images with EasyOCR | item_id={item_id}")
+        
+        ocr_results = []
+        for image_data in images:
+            try:
+                # Save to temp file for EasyOCR processing
+                temp_image_path = os.path.join(
+                    tempfile.gettempdir(),
+                    f"ocr_img_{item_id}_{image_data['page_index']}_{image_data['image_index']}.{image_data['extension']}"
+                )
+                
+                with open(temp_image_path, 'wb') as f:
+                    f.write(image_data['bytes'])
+                
+                # Run EasyOCR on local file
+                ocr_text = ocr_extractor.extract_text(temp_image_path)
+                
+                # Clean up temp file
+                try:
+                    os.remove(temp_image_path)
+                except:
+                    pass
+                
+                ocr_results.append({
+                    'page_index': image_data['page_index'],
+                    'image_index': image_data['image_index'],
+                    'text': ocr_text,
+                    'extension': image_data['extension']
+                })
+            except Exception as e:
+                logger.warning(f"EasyOCR failed on image page={image_data['page_index']} img={image_data['image_index']}: {e}")
+                continue
+        
+        logger.info(f"EasyOCR completed | images_processed={len(ocr_results)}")
+        return ocr_results
+    
+    def _extract_and_ocr_with_easyocr(self, pdf_path: str, item_id: str,
+                                      ocr_extractor: OCRExtractor) -> List[Dict[str, Any]]:
+        """
+        Extract images from PDF and apply OCR using EasyOCR (local processing).
+        Uses temporary files, no upload to Dataloop.
+        """
+        logger.info(f"Extracting images for EasyOCR | item_id={item_id}")
+        
+        images = self._extract_images_from_pdf(pdf_path)
+        return self._process_images_with_easyocr(images, item_id, ocr_extractor)
+    
+    def _extract_and_ocr_with_dataloop_model_from_images(self, images: List[Dict[str, Any]], 
+                                                         original_item: dl.Item,
+                                                         ocr_extractor: OCRExtractor) -> List[Dict[str, Any]]:
+        """
+        Process already-extracted images with custom Dataloop OCR model.
+        Flow: Upload â†’ Predict â†’ Get text â†’ Delete items + folder
+        
+        Args:
+            images: List of image dictionaries with 'bytes', 'page_index', 'image_index', 'extension'
+            original_item: Original PDF item
+            ocr_extractor: OCR extractor instance
+            
+        Returns:
+            List of OCR results with page/image indices
+        """
+        if not images:
+            logger.info("No images to process")
+            return []
+        
+        logger.info(f"Processing {len(images)} images with Dataloop OCR model | item_id={original_item.id}")
+        
+        # Setup temp directories
+        temp_folder_name = f"./dataloop/temp_images_ocr_{original_item.name}"
+        temp_local_dir = tempfile.mkdtemp(prefix=f"ocr_images_{original_item.id}_")
+        
+        try:
+            # Save images locally and upload to Dataloop
+            uploaded_items = self._upload_images_to_dataloop(
+                images, temp_local_dir, temp_folder_name, original_item.dataset
+            )
+            
+            # Run batch OCR
+            ocr_results = self._run_batch_ocr(uploaded_items, ocr_extractor)
+            
+            return ocr_results
+            
+        finally:
+            # Cleanup all temporary resources
+            uploaded_item_objects = [item for item, _ in uploaded_items] if uploaded_items else []
+            cleanup_temp_items_and_folder(
+                uploaded_item_objects,
+                temp_folder_name,
+                original_item.dataset,
+                temp_local_dir
+            )
+    
+    def _extract_and_ocr_with_dataloop_model(self, pdf_path: str, original_item: dl.Item,
+                                             ocr_extractor: OCRExtractor) -> List[Dict[str, Any]]:
+        """
+        Extract images from PDF, upload to Dataloop, run custom OCR model, then cleanup.
+        Flow: Extract â†’ Upload â†’ Predict â†’ Get text â†’ Delete items + folder
+        """
+        logger.info(f"Extracting images for Dataloop OCR model | item_id={original_item.id}")
+        
+        # Extract images from PDF
+        images = self._extract_images_from_pdf(pdf_path)
+        return self._extract_and_ocr_with_dataloop_model_from_images(images, original_item, ocr_extractor)
+    
+    def _upload_images_to_dataloop(
+        self,
+        images: List[Dict[str, Any]],
+        local_dir: str,
+        remote_folder: str,
+        dataset: dl.Dataset
+     ) -> List[tuple]:
+        """
+        Save images locally and upload to Dataloop in batch.
+        
+        Returns:
+            List of tuples: (uploaded_item, metadata)
+        """
+        if not images:
+            return []
+        
+        # Step 1: Save all images locally
+        image_paths = []
+        image_metadata_list = []
+        
+        for image_data in images:
+            try:
+                # Save to local temp directory
+                image_filename = f"page_{image_data['page_index']}_img_{image_data['image_index']}.{image_data['extension']}"
+                image_path = os.path.join(local_dir, image_filename)
+                
+                with open(image_path, 'wb') as f:
+                    f.write(image_data['bytes'])
+                
+                image_paths.append(image_path)
+                image_metadata_list.append({
+                    'page_index': image_data['page_index'],
+                    'image_index': image_data['image_index'],
+                    'extension': image_data['extension']
+                })
+                
+            except Exception as e:
+                logger.warning(f"Failed to save image page={image_data['page_index']}: {e}")
+                continue
+        
+        if not image_paths:
+            logger.warning("No images could be saved locally")
+            return []
+        
+        # Step 2: Batch upload to Dataloop
+        logger.info(f"Batch uploading {len(image_paths)} images to Dataloop | folder={remote_folder}")
+        try:
+            uploaded = dataset.items.upload(
+                local_path=image_paths,
+                remote_path=remote_folder,
+                overwrite=True
+            )
+            
+            # Handle single item vs list response
+            if isinstance(uploaded, dl.Item):
+                uploaded_items_list = [uploaded]
+            else:
+                uploaded_items_list = list(uploaded)
+            
+            # Pair uploaded items with their metadata
+            uploaded_items = []
+            for idx, uploaded_item in enumerate(uploaded_items_list):
+                if idx < len(image_metadata_list):
+                    uploaded_items.append((uploaded_item, image_metadata_list[idx]))
+                    logger.debug(f"Uploaded image | item_id={uploaded_item.id} page={image_metadata_list[idx]['page_index']}")
+            
+            logger.info(f"Batch upload completed | uploaded={len(uploaded_items)} images")
+            return uploaded_items
+            
+        except Exception as e:
+            logger.error(f"Batch upload failed: {e}, falling back to individual uploads")
+            uploaded_items = []
+            for image_path, metadata in zip(image_paths, image_metadata_list):
+                try:
+                    uploaded_item = dataset.items.upload(
+                        local_path=image_path,
+                        remote_path=remote_folder,
+                        overwrite=True
+                    )
+                    uploaded_items.append((uploaded_item, metadata))
+                except Exception as upload_err:
+                    logger.warning(f"Failed to upload {image_path}: {upload_err}")
+                    continue
+            
+            logger.info(f"Individual uploads completed | uploaded={len(uploaded_items)} images")
+            return uploaded_items
+    
+    def _run_batch_ocr(
+        self,
+        uploaded_items: List[tuple],
+        ocr_extractor: OCRExtractor
+     ) -> List[Dict[str, Any]]:
+        """
+        Run batch OCR on uploaded items.
+        
+        Args:
+            uploaded_items: List of (item, metadata) tuples
+            ocr_extractor: OCR extractor instance
+            
+        Returns:
+            List of OCR results with page/image indices
+        """
+        uploaded_item_objects = [item for item, _ in uploaded_items]
+        ocr_text_results = ocr_extractor.extract_text_batch(uploaded_item_objects)
+        
+        # Map results back to original metadata
+        ocr_results = []
+        for uploaded_item, metadata in uploaded_items:
+            ocr_text = ocr_text_results.get(uploaded_item.id, "")
+            ocr_results.append({
+                'page_index': metadata['page_index'],
+                'image_index': metadata['image_index'],
+                'text': ocr_text,
+                'extension': metadata['extension']
+            })
+        
+            logger.info(f"OCR completed | successful={sum(1 for r in ocr_results if r['text'])}/{len(uploaded_items)}")
+        return ocr_results
+
+    def _combine_texts(self, page_texts: List[str], ocr_texts: List[Dict[str, Any]],
+                      integration_method: str) -> str:
+        """Combine PDF text and OCR text."""
+        if integration_method == 'append_to_page':
+            combined_pages = page_texts.copy()
+            for ocr_result in ocr_texts:
+                page_idx = ocr_result['page_index']
+                if page_idx < len(combined_pages):
+                    combined_pages[page_idx] += f"\n\n[OCR_IMAGE_{ocr_result['image_index']}]\n{ocr_result['text']}" if ocr_result['text'] else ''
+            return '\n\n'.join(combined_pages)
+            
+        elif integration_method == 'separate_chunks':
+            pdf_text = '\n\n'.join(page_texts)
+            ocr_text = '\n\n'.join([f"[OCR_PAGE_{r['page_index']}_IMAGE_{r['image_index']}]\n{r['text']}" if r['text'] else ''
+                                   for r in ocr_texts])
+            return f"{pdf_text}\n\n[OCR_SECTION]\n{ocr_text}"
+            
+        else:  # combine_all
+            all_text = '\n\n'.join(page_texts)
+            for ocr_result in ocr_texts:
+                all_text += f"\n\n[OCR_PAGE_{ocr_result['page_index']}_IMAGE_{ocr_result['image_index']}]\n{ocr_result['text']}" if ocr_result['text'] else ''
+            return all_text
+
+    def _update_original_item_metadata(
+        self,
+        item: dl.Item,
+        total_pages: int,
+        total_chunks: int,
+        target_dataset_id: str,
+        use_markdown_extraction: bool,
+        chunking_strategy: str,
+        max_chunk_size: int,
+        chunk_overlap: int,
+        ocr_from_images: bool,
+        ocr_integration_method: str,
+        to_correct_spelling: bool
+    ) -> None:
+        """
+        Update the original PDF item's metadata with processing information.
+        
+        Args:
+            item: Original PDF item to update
+            total_pages: Total number of pages in the PDF
+            total_chunks: Number of chunks created
+            target_dataset_id: Dataset ID where chunks were stored
+            use_markdown_extraction: Whether markdown extraction was used
+            chunking_strategy: Chunking strategy used
+            max_chunk_size: Maximum chunk size in characters
+            chunk_overlap: Overlap between chunks in characters
+            ocr_from_images: Whether OCR was enabled
+            ocr_integration_method: How OCR text was integrated
+            to_correct_spelling: Whether text cleaning was applied
+        """
+        from datetime import datetime
+        
+        processing_metadata = {
+            'total_pages': total_pages,
+            'total_chunks': total_chunks,
+            'chunks_dataset_id': target_dataset_id,
+            'extraction_method': 'pymupdf4llm' if use_markdown_extraction else 'fitz',
+            'extraction_format': 'markdown' if use_markdown_extraction else 'plain',
+            'chunking_strategy': chunking_strategy,
+            'chunk_size': max_chunk_size,
+            'chunk_overlap': chunk_overlap,
+            'ocr_enabled': ocr_from_images,
+            'ocr_integration_method': ocr_integration_method if ocr_from_images else None,
+            'text_cleaning_enabled': to_correct_spelling,
+            'processing_timestamp': datetime.utcnow().isoformat() + 'Z'
+        }
+        
+        # Update item metadata
+        item.metadata['user'] = item.metadata.get('user', {})
+        item.metadata['user'].update(processing_metadata)
+        item.update(system_metadata=True)
+        
+        logger.info(f"Updated original item metadata | item_id={item.id} total_chunks={total_chunks}")
+
diff --git a/apps/pdf_processor/Dockerfile b/apps/pdf_processor/Dockerfile
deleted file mode 100644
index 5fd6e7f..0000000
--- a/apps/pdf_processor/Dockerfile
+++ /dev/null
@@ -1,15 +0,0 @@
-FROM hub.dataloop.ai/dtlpy-runner-images/cpu:python3.10_opencv
-# Install Python dependencies
-RUN ${DL_PYTHON_EXECUTABLE} -m pip install -U \
-    pandas>=2.0.0 \
-    PyMuPDF>=1.23.0 \
-    pymupdf-layout>=0.1.0 \
-    pymupdf4llm>=0.0.8 \
-    unstructured>=0.10.0 \
-    nltk>=3.8.0 \
-    langchain-text-splitters>=0.0.1 \
-    easyocr>=1.7.0
-
-# docker build --no-cache -t gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/pdf-processor:0.0.1 -f apps/pdf_processor/Dockerfile .
-# docker push gcr.io/viewo-g/piper/agent/runner/apps/rag-multimodal-processors/pdf-processor:0.0.1
-
diff --git a/apps/pdf_processor/__init__.py b/apps/pdf_processor/__init__.py
deleted file mode 100644
index 9e170a8..0000000
--- a/apps/pdf_processor/__init__.py
+++ /dev/null
@@ -1,6 +0,0 @@
-"""PDF processor app for Dataloop platform."""
-
-from .app import PDFProcessor
-from .pdf_extractor import PDFExtractor
-
-__all__ = ['PDFProcessor', 'PDFExtractor']
diff --git a/apps/pdf_processor/app.py b/apps/pdf_processor/app.py
deleted file mode 100644
index ad96641..0000000
--- a/apps/pdf_processor/app.py
+++ /dev/null
@@ -1,58 +0,0 @@
-"""
-PDF processor app.
-
-PDF processor that uses PDFExtractor and ExtractedData throughout.
-"""
-
-import logging
-from typing import List
-
-import dtlpy as dl
-import nltk
-
-import transforms
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-from apps.pdf_processor.pdf_extractor import PDFExtractor
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class PDFProcessor(dl.BaseServiceRunner):
-    """PDF Processor for extracting text, applying OCR, and creating chunks."""
-
-    def __init__(self):
-        """Initialize PDF processor."""
-        dl.client_api._upload_session_timeout = 60
-        dl.client_api._upload_chuck_timeout = 30
-
-        for resource in ['tokenizers/punkt', 'taggers/averaged_perceptron_tagger']:
-            try:
-                nltk.data.find(resource)
-            except LookupError:
-                nltk.download(resource.split('/')[-1], quiet=True)
-
-    @staticmethod
-    def run(item: dl.Item, target_dataset: dl.Dataset, context: dl.Context) -> List[dl.Item]:
-        """Process a PDF document into chunks."""
-        config = context.node.metadata.get('customNodeConfig', {})
-        cfg = Config.from_dict(config)
-        data = ExtractedData(item=item, target_dataset=target_dataset, config=cfg)
-
-        try:
-            data = PDFExtractor.extract(data)
-            if cfg.ocr_from_images:
-                data = transforms.ocr_enhance(data)
-            if cfg.to_correct_spelling:
-                data = transforms.deep_clean(data)
-            else:
-                data = transforms.clean(data)
-            data = transforms.chunk(data)
-            data = transforms.upload_to_dataloop(data)
-
-            logger.info(f"Processed {item.name}: {len(data.uploaded_items)} chunks, {data.errors.get_summary()}")
-            return data.uploaded_items
-
-        except Exception as e:
-            logger.error(f"Processing failed: {e}", exc_info=True)
-            raise
diff --git a/apps/pdf_processor/pdf_extractor.py b/apps/pdf_processor/pdf_extractor.py
deleted file mode 100644
index 70ba470..0000000
--- a/apps/pdf_processor/pdf_extractor.py
+++ /dev/null
@@ -1,149 +0,0 @@
-"""
-PDF extraction logic.
-
-Handles PDF-specific extraction operations:
-- Text extraction (basic PyMuPDF or markdown with ML layout)
-- Image extraction with positional metadata
-- Metadata collection
-"""
-
-import logging
-import os
-import tempfile
-from typing import List, Tuple, Dict, Any
-
-import fitz
-import pymupdf.layout  # Must be imported before pymupdf4llm for ML layout features
-import pymupdf4llm
-
-from utils.extracted_data import ExtractedData
-from utils.data_types import ImageContent
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class PDFExtractor:
-    """PDF extraction operations."""
-
-    @staticmethod
-    def extract(data: ExtractedData) -> ExtractedData:
-        """Extract content from PDF item."""
-        data.current_stage = "extraction"
-
-        if not data.item:
-            data.log_error("No item provided for extraction")
-            return data
-
-        try:
-            with tempfile.TemporaryDirectory() as temp_dir:
-                file_path = data.item.download(local_path=temp_dir)
-                use_markdown = data.config.use_markdown_extraction
-                extract_images = data.config.extract_images
-
-                if use_markdown:
-                    content, images, metadata = PDFExtractor._extract_markdown(file_path, temp_dir, extract_images)
-                else:
-                    content, images, metadata = PDFExtractor._extract_pymupdf(file_path, temp_dir, extract_images)
-
-                data.content_text = content
-                data.images = images
-                metadata['source_file'] = data.item_name
-                data.metadata = metadata
-
-        except Exception as e:
-            data.log_error("PDF extraction failed. Check logs for details.")
-            logger.exception(f"PDF extraction error: {e}")
-
-        return data
-
-    @staticmethod
-    def _extract_pymupdf(
-        file_path: str, temp_dir: str, extract_images: bool
-    ) -> Tuple[str, List[ImageContent], Dict[str, Any]]:
-        """Extract text and images using basic PyMuPDF."""
-        doc = fitz.open(file_path)
-        text_parts = []
-        images = []
-
-        try:
-            for page_num, page in enumerate(doc):
-                page_text = page.get_text()
-                text_parts.append(f"\n\n--- Page {page_num + 1} ---\n\n{page_text}")
-
-                if extract_images:
-                    page_images = PDFExtractor._extract_images_from_page(page, page_num, temp_dir)
-                    images.extend(page_images)
-
-            metadata = {
-                'page_count': len(doc),
-                'extraction_method': 'pymupdf',
-                'image_count': len(images),
-                'processor': 'pdf',
-            }
-        finally:
-            doc.close()
-
-        return ''.join(text_parts), images, metadata
-
-    @staticmethod
-    def _extract_markdown(
-        file_path: str, temp_dir: str, extract_images: bool
-    ) -> Tuple[str, List[ImageContent], Dict[str, Any]]:
-        """Extract text and images using pymupdf4llm with ML-based layout."""
-        content = pymupdf4llm.to_markdown(file_path)
-        images = []
-
-        if extract_images:
-            doc = fitz.open(file_path)
-            try:
-                for page_num, page in enumerate(doc):
-                    page_images = PDFExtractor._extract_images_from_page(page, page_num, temp_dir)
-                    images.extend(page_images)
-            finally:
-                doc.close()
-
-        metadata = {
-            'extraction_method': 'pymupdf4llm',
-            'format': 'markdown',
-            'layout_enhancement': True,
-            'image_count': len(images),
-            'processor': 'pdf',
-        }
-
-        return content, images, metadata
-
-    @staticmethod
-    def _extract_images_from_page(page: fitz.Page, page_num: int, temp_dir: str) -> List[ImageContent]:
-        """Extract images from a PDF page with positional metadata."""
-        images = []
-        image_list = page.get_images(full=True)
-
-        for img_index, img in enumerate(image_list):
-            try:
-                xref = img[0]
-                base_image = page.parent.extract_image(xref)
-
-                ext = base_image.get('ext', 'png')
-                image_path = os.path.join(temp_dir, f"page{page_num}_img{img_index}.{ext}")
-                with open(image_path, 'wb') as f:
-                    f.write(base_image['image'])
-
-                bbox = None
-                image_rects = page.get_image_rects(xref)
-                if image_rects:
-                    rect = image_rects[0] if isinstance(image_rects, list) else image_rects
-                    bbox = (rect.x0, rect.y0, rect.width, rect.height)
-
-                images.append(
-                    ImageContent(
-                        path=image_path,
-                        page_number=page_num + 1,
-                        format=ext,
-                        size=(base_image.get('width'), base_image.get('height')),
-                        bbox=bbox,
-                    )
-                )
-            except (IOError, OSError, ValueError, KeyError) as e:
-                logger.warning(f"Failed to extract image {img_index} from page {page_num}: {e}")
-
-        return images
diff --git a/apps/pdf_processor/requirements.txt b/apps/pdf_processor/requirements.txt
deleted file mode 100644
index 9ab9e5f..0000000
--- a/apps/pdf_processor/requirements.txt
+++ /dev/null
@@ -1,18 +0,0 @@
-# Core Dataloop SDK
-dtlpy>=1.0.0
-
-# Data Processing
-pandas>=2.0.0
-
-# PDF Processing
-PyMuPDF>=1.23.0
-pymupdf-layout>=0.1.0
-pymupdf4llm>=0.0.8
-
-# Text Processing
-unstructured>=0.10.0
-nltk>=3.8.0
-langchain-text-splitters>=0.0.1
-
-# OCR (optional - for use_ocr=True)
-easyocr>=1.7.0
diff --git a/chunkers/__init__.py b/chunkers/__init__.py
new file mode 100644
index 0000000..ab5d9f8
--- /dev/null
+++ b/chunkers/__init__.py
@@ -0,0 +1,9 @@
+"""
+Chunkers package for splitting text into manageable pieces.
+Different chunking strategies for different use cases.
+"""
+
+from .text_chunker import TextChunker
+
+__all__ = ['TextChunker']
+
diff --git a/chunkers/text_chunker.py b/chunkers/text_chunker.py
new file mode 100644
index 0000000..2c639e8
--- /dev/null
+++ b/chunkers/text_chunker.py
@@ -0,0 +1,121 @@
+"""
+Text chunker implementation with multiple chunking strategies.
+Supports fixed-size, recursive, NLTK-based, and markdown-aware chunking.
+"""
+
+from typing import List
+import logging
+import nltk
+from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
+
+logger = logging.getLogger('item-processor-logger')
+
+
+class TextChunker:
+    """
+    Text chunker with support for multiple chunking strategies.
+    """
+    
+    def __init__(self, 
+                 chunk_size: int = 300, 
+                 chunk_overlap: int = 20,
+                 strategy: str = 'recursive',
+                 use_markdown_splitting: bool = False):
+        """
+        Initialize text chunker with strategy.
+        
+        Args:
+            chunk_size (int): Maximum size of each chunk
+            chunk_overlap (int): Overlap between consecutive chunks
+            strategy (str): Chunking strategy ('fixed-size', 'recursive', 'nltk-sentence', 'nltk-paragraphs', '1-chunk')
+            use_markdown_splitting (bool): Use markdown-aware separators for recursive splitting
+        """
+        self.chunk_size = chunk_size
+        self.chunk_overlap = chunk_overlap
+        self.strategy = strategy
+        self.use_markdown_splitting = use_markdown_splitting
+    
+    def chunk(self, text: str) -> List[str]:
+        """
+        Split text into chunks based on the configured strategy.
+        
+        Args:
+            text (str): Input text to chunk
+            
+        Returns:
+            List[str]: List of text chunks
+        """
+        logger.info(
+            f"Chunking text | strategy={self.strategy} chunk_size={self.chunk_size} "
+            f"chunk_overlap={self.chunk_overlap} use_markdown={self.use_markdown_splitting} "
+            f"text_length={len(text)}"
+        )
+        
+        if self.strategy == 'fixed-size':
+            chunks = self._chunk_fixed_size(text)
+        elif self.strategy == 'recursive':
+            chunks = self._chunk_recursive(text)
+        elif self.strategy == 'nltk-sentence':
+            chunks = self._chunk_nltk_sentence(text)
+        elif self.strategy == 'nltk-paragraphs':
+            chunks = self._chunk_nltk_paragraphs(text)
+        elif self.strategy == '1-chunk':
+            chunks = [text]
+        else:
+            logger.warning(f"Unknown chunking strategy: {self.strategy}, using recursive")
+            chunks = self._chunk_recursive(text)
+        
+        logger.info(f"Chunking complete | chunks_created={len(chunks)}")
+        return chunks
+    
+    def _chunk_fixed_size(self, text: str) -> List[str]:
+        """Fixed-size chunking with character-based splitting."""
+        text_splitter = CharacterTextSplitter(
+            separator="",
+            chunk_size=self.chunk_size,
+            chunk_overlap=self.chunk_overlap
+        )
+        chunks = text_splitter.create_documents([text])
+        return [chunk.page_content for chunk in chunks]
+    
+    def _chunk_recursive(self, text: str) -> List[str]:
+        """Recursive chunking that respects semantic boundaries."""
+        if self.use_markdown_splitting:
+            # Markdown-aware separators (in order of priority)
+            separators = [
+                "\n## ",      # H2 headers
+                "\n### ",     # H3 headers
+                "\n#### ",    # H4 headers
+                "\n---\n",    # Horizontal rules
+                "\n\n",       # Paragraphs
+                "\n",         # Lines
+                " ",          # Words
+                ""            # Characters
+            ]
+            text_splitter = RecursiveCharacterTextSplitter(
+                chunk_size=self.chunk_size,
+                chunk_overlap=self.chunk_overlap,
+                length_function=len,
+                is_separator_regex=False,
+                separators=separators
+            )
+        else:
+            # Standard recursive splitting
+            text_splitter = RecursiveCharacterTextSplitter(
+                chunk_size=self.chunk_size,
+                chunk_overlap=self.chunk_overlap,
+                length_function=len,
+                is_separator_regex=False,
+            )
+        
+        chunks = text_splitter.create_documents([text])
+        return [chunk.page_content for chunk in chunks]
+    
+    def _chunk_nltk_sentence(self, text: str) -> List[str]:
+        """Chunk by sentence boundaries using NLTK."""
+        return nltk.sent_tokenize(text)
+    
+    def _chunk_nltk_paragraphs(self, text: str) -> List[str]:
+        """Chunk by paragraph boundaries using NLTK."""
+        return nltk.tokenize.blankline_tokenize(text)
+
diff --git a/extractors/__init__.py b/extractors/__init__.py
new file mode 100644
index 0000000..04d330d
--- /dev/null
+++ b/extractors/__init__.py
@@ -0,0 +1,13 @@
+"""
+Extractors package for extracting content from items.
+Handles OCR, transcription, captioning, and other extraction methods.
+"""
+
+from .ocr_extractor import OCRExtractor
+
+# Future extractors (uncomment as implemented):
+# from .audio_extractor import AudioExtractor
+# from .caption_extractor import CaptionExtractor
+
+__all__ = ['OCRExtractor']
+
diff --git a/extractors/ocr_extractor.py b/extractors/ocr_extractor.py
new file mode 100644
index 0000000..c2d234b
--- /dev/null
+++ b/extractors/ocr_extractor.py
@@ -0,0 +1,182 @@
+"""
+OCR extractor for extracting text from images.
+Supports Dataloop models/services and external OCR libraries.
+"""
+
+import dtlpy as dl
+import logging
+from typing import Optional, List, Dict
+from utils.dataloop_model_executor import DataloopModelExecutor
+
+logger = logging.getLogger('item-processor-logger')
+
+
+class OCRExtractor(DataloopModelExecutor):
+    """
+    Extract text from images using OCR.
+    Defaults to EasyOCR, supports custom Dataloop models via custom_ocr_model_id.
+    """
+    
+    _easyocr_reader = None
+    _easyocr_languages = ['en', 'es', 'fr', 'de', 'it', 'pt']
+    
+    def extract_text(self, image_path: str) -> str:
+        """
+        Extract text from image file using EasyOCR.
+        This method is for local file processing only.
+        
+        For Dataloop model processing, use extract_text_batch() instead.
+        
+        Args:
+            image_path (str): Path to image file
+            
+        Returns:
+            str: Extracted text from the image
+            
+        Raises:
+            ValueError: If custom model is configured (use extract_text_batch instead)
+        """
+        if self.has_dataloop_backend():
+            raise ValueError(
+                f"extract_text() is for EasyOCR only. "
+                f"For custom Dataloop models, use extract_text_batch() instead."
+            )
+        
+        logger.debug("Using EasyOCR for local image file")
+        return self._external_ocr(image_path)
+    
+    def _parse_ocr_result(self, item: dl.Item) -> str:
+        """
+        Parse OCR result from updated Dataloop item.
+        Priority: item.description first, then text annotations.
+        
+        Args:
+            item (dl.Item): Updated item after model execution
+            
+        Returns:
+            str: Extracted text
+        """
+        extracted_text = ""
+        
+        # Priority 1: Check item.description
+        if item.description and item.description.strip():
+            logger.info(f"Found OCR text in item.description | length={len(item.description)}")
+            extracted_text = item.description.strip()
+            return extracted_text
+        
+        # Priority 2: Check annotations with label='Text'
+        try:
+            annotations = item.annotations.list()
+            text_annotations = []
+            
+            for annotation in annotations:
+                if hasattr(annotation, 'label') and annotation.label == 'Text':
+                    if hasattr(annotation, 'description') and annotation.description:
+                        text_annotations.append(annotation.description)
+                        logger.debug(f"Found text annotation | text={annotation.description[:50]}...")
+            
+            if text_annotations:
+                extracted_text = ' '.join(text_annotations)
+                logger.info(f"Found {len(text_annotations)} text annotations | total_length={len(extracted_text)}")
+                return extracted_text
+        
+        except Exception as e:
+            logger.warning(f"Failed to parse annotations: {str(e)}")
+        
+        # No results found
+        logger.warning(f"No OCR results found in item description or annotations for item {item.id}")
+        return ""
+    
+    def _external_ocr(self, image_path: str) -> str:
+        """
+        External OCR implementation using EasyOCR with model caching.
+        
+        Args:
+            image_path (str): Path to image file
+            
+        Returns:
+            str: Extracted text
+        """
+        try:
+            import easyocr
+            
+            # Initialize EasyOCR reader only once (class-level caching)
+            if OCRExtractor._easyocr_reader is None:
+                logger.info(f"Initializing EasyOCR reader with languages: {OCRExtractor._easyocr_languages}")
+                OCRExtractor._easyocr_reader = easyocr.Reader(OCRExtractor._easyocr_languages, gpu=False)
+                logger.info("EasyOCR reader initialized and cached")
+            else:
+                logger.debug("Using cached EasyOCR reader")
+            
+            # Perform OCR directly on image path
+            logger.debug(f"Running EasyOCR on image: {image_path}")
+            results = OCRExtractor._easyocr_reader.readtext(image_path)
+            
+            # Extract text from results (bbox, text, confidence)
+            all_text = ' '.join([text for (bbox, text, confidence) in results])
+            
+            logger.info(f"EasyOCR extracted {len(results)} text blocks, total length: {len(all_text)}")
+            return all_text
+            
+        except Exception as e:
+            logger.error(f"EasyOCR failed: {str(e)}")
+            return f"[OCR_ERROR: {str(e)}]"
+    
+    def extract_text_batch(self, items: List[dl.Item]) -> Dict[str, str]:
+        """
+        Run batch OCR on multiple Dataloop items.
+        Only works with custom Dataloop models.
+        
+        Args:
+            items (List[dl.Item]): List of Dataloop items to process
+            
+        Returns:
+            Dict[str, str]: Dictionary mapping item_id to extracted OCR text
+            
+        Raises:
+            ValueError: If custom model is not configured or not deployed
+            Exception: If batch execution fails
+        """
+        if not self.has_dataloop_backend():
+            raise ValueError("Batch OCR requires a custom_ocr_model_id. Use extract_text() with EasyOCR for single images.")
+        
+        if not items:
+            return {}
+        
+        item_ids = [item.id for item in items]
+        logger.info(f"Running batch OCR prediction | model_id={self.model_id} item_count={len(item_ids)}")
+        
+        try:
+            # Check model deployment
+            model, model_status = self._check_model_deployed(auto_deploy=False)
+            
+            # Execute batch prediction
+            execution = model.predict(item_ids=item_ids)
+            logger.info(f"Waiting for batch execution to complete | execution_id={execution.id}")
+            execution.wait()
+            
+            # Check execution status
+            if execution.latest_status['status'] == dl.ExecutionStatus.FAILED:
+                raise Exception(f"Batch OCR execution failed: {execution.latest_status.get('message', 'Unknown error')}")
+            elif execution.latest_status['status'] in ['success', dl.ExecutionStatus.SUCCESS]:
+                logger.info(f"Batch OCR execution successful | execution_id={execution.id}")
+            
+            # Fetch results from each item
+            results = {}
+            for item in items:
+                try:
+                    # Refresh item to get OCR results
+                    updated_item = dl.items.get(item_id=item.id)
+                    ocr_text = self._parse_ocr_result(updated_item)
+                    results[item.id] = ocr_text
+                    logger.debug(f"OCR text retrieved | item_id={item.id} text_length={len(ocr_text)}")
+                except Exception as e:
+                    logger.warning(f"Failed to retrieve OCR results for item {item.id}: {e}")
+                    results[item.id] = ""
+            
+            logger.info(f"Batch OCR completed | successful={len([r for r in results.values() if r])}/{len(items)}")
+            return results
+            
+        except Exception as e:
+            logger.error(f"Batch OCR prediction failed: {str(e)}")
+            raise
\ No newline at end of file
diff --git a/pytest.ini b/pytest.ini
deleted file mode 100644
index 3ae16bc..0000000
--- a/pytest.ini
+++ /dev/null
@@ -1,14 +0,0 @@
-[pytest]
-# Pytest configuration for RAG Multimodal Processors tests
-
-# Show print statements and other output
-addopts = -s -v
-
-# Test discovery patterns
-python_files = test_*.py
-python_classes = Test*
-python_functions = test_*
-
-# Test paths
-testpaths = tests
-
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 12710d7..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,27 +0,0 @@
-# Core Dataloop SDK
-dtlpy>=1.0.0
-
-# Data Processing
-pandas>=2.0.0
-
-# PDF Processing
-PyMuPDF>=1.23.0  # Provides 'fitz' module
-pymupdf-layout>=0.1.0  # ML-based layout analysis for pymupdf4llm
-pymupdf4llm>=0.0.8
-
-# Document Processing
-python-docx>=1.1.0  # Provides 'docx' module
-
-# Text Processing
-unstructured>=0.10.0  # Text cleaning utilities
-nltk>=3.8.0  # Sentence tokenization
-langchain-text-splitters>=0.0.1  # Text chunking
-
-# Image Processing & OCR
-Pillow>=10.0.0  # Image processing
-easyocr>=1.7.0  # OCR text extraction
-
-# Testing
-pytest>=7.4.0
-pytest-mock>=3.10.0
-
diff --git a/tests/README.md b/tests/README.md
deleted file mode 100644
index 24e0afb..0000000
--- a/tests/README.md
+++ /dev/null
@@ -1,80 +0,0 @@
-# Tests
-
-## Quick Start
-
-```bash
-# Unit tests (no Dataloop connection needed)
-pytest tests/test_core.py tests/test_extractors.py tests/test_transforms.py -v
-
-# Integration tests (requires Dataloop auth)
-pytest tests/test_processors.py -v
-```
-
-## Test Files
-
-| File | Type | Description |
-|------|------|-------------|
-| `test_core.py` | Unit | Config, ErrorTracker, ExtractedData, ChunkMetadata |
-| `test_extractors.py` | Unit | PDFExtractor, DOCExtractor |
-| `test_transforms.py` | Unit | clean, chunk, deep_clean, llm transforms |
-| `test_processors.py` | Integration | PDF and DOC processor end-to-end |
-| `test_config.py` | Config | Test item IDs, dataset IDs, processor settings |
-
-## Integration Test Setup
-
-Edit `test_config.py` with your Dataloop IDs:
-
-```python
-TEST_ITEMS = {
-    'pdf': {'item_id': 'your-pdf-item-id'},
-    'doc': {'item_id': 'your-docx-item-id'},
-}
-
-TARGET_DATASET_ID = 'your-target-dataset-id'
-
-PDF_CONFIG = {
-    'max_chunk_size': 500,
-    'chunking_strategy': 'recursive',
-    'remote_path': '/chunks',  # Upload directory
-}
-
-DOC_CONFIG = {
-    'max_chunk_size': 500,
-    'chunking_strategy': 'recursive',
-    'remote_path': '/chunks',
-}
-```
-
-Run specific processor:
-```bash
-pytest tests/test_processors.py -k pdf -v
-pytest tests/test_processors.py -k doc -v
-```
-
-## Writing Tests
-
-Transform test:
-```python
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-import transforms
-
-def test_my_transform():
-    data = ExtractedData(config=Config())
-    data.content_text = "Test content"
-
-    result = transforms.my_transform(data)
-
-    assert isinstance(result, ExtractedData)
-```
-
-Extractor test:
-```python
-from utils.extracted_data import ExtractedData
-from apps.pdf_processor.pdf_extractor import PDFExtractor
-
-def test_extractor():
-    data = ExtractedData()
-    result = PDFExtractor.extract(data)
-    assert result.current_stage == "extraction"
-```
diff --git a/tests/__init__.py b/tests/__init__.py
deleted file mode 100644
index ef66718..0000000
--- a/tests/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-"""Tests package for RAG Multimodal Processors."""
diff --git a/tests/conftest.py b/tests/conftest.py
deleted file mode 100644
index 4f5ce04..0000000
--- a/tests/conftest.py
+++ /dev/null
@@ -1,11 +0,0 @@
-"""
-Pytest configuration for RAG Multimodal Processors tests.
-
-This file ensures test output is visible when running tests.
-"""
-import sys
-from pathlib import Path
-
-# Add repository root to Python path for imports
-repo_root = Path(__file__).parent.parent
-sys.path.insert(0, str(repo_root))
diff --git a/tests/test_config.py b/tests/test_config.py
deleted file mode 100644
index 695bf9e..0000000
--- a/tests/test_config.py
+++ /dev/null
@@ -1,60 +0,0 @@
-"""
-Test configuration file for RAG Multimodal Processors tests.
-
-Edit the values below to configure your test runs.
-"""
-
-# ============================================================
-# TEST ITEMS AND DATASETS
-# ============================================================
-# Test items (source dataset is obtained from item.dataset)
-TEST_ITEMS = {
-    'pdf': {'item_id': "6911a710d4c1299c6780c14f"},
-    'doc': {'item_id': "6910ba43732d419b5d98b41c"},
-}
-
-# Target dataset where chunks will be uploaded (REQUIRED)
-TARGET_DATASET_ID = "6910ba261a0566b56d15a55a"  # Model mgmt demo: RAG demo source
-
-# ============================================================
-# PDF TEST CONFIGURATION
-# ============================================================
-
-PDF_CONFIG = {
-    'name': 'Test-PDF-Processor',
-    # Extraction settings
-    'extraction_method': 'basic',  # Options: 'markdown', 'basic'
-    'extract_images': True,
-    # OCR Processing
-    'use_ocr': False,  # Enable OCR on extracted images (uses EasyOCR)
-    'ocr_method': 'local',  # Options: 'local', 'batch', 'auto'
-    # Chunking Strategy
-    'chunking_strategy': 'recursive',  # Options: 'recursive', 'fixed', 'sentence', 'none'
-    'max_chunk_size': 500,
-    'chunk_overlap': 20,
-    # Text Cleaning
-    'normalize_whitespace': True,
-    'remove_empty_lines': True,
-    # Upload settings
-    'remote_path': '/chunks',  # Remote directory for uploaded chunks
-}
-
-# ============================================================
-# DOC TEST CONFIGURATION
-# ============================================================
-
-DOC_CONFIG = {
-    'name': 'Test-DOC-Processor',
-    # Extraction settings
-    'extract_images': True,
-    'extract_tables': True,
-    # Chunking Strategy
-    'chunking_strategy': 'recursive',  # Options: 'recursive', 'fixed', 'sentence', 'none'
-    'max_chunk_size': 500,
-    'chunk_overlap': 20,
-    # Text Cleaning
-    'normalize_whitespace': True,
-    'remove_empty_lines': True,
-    # Upload settings
-    'remote_path': '/chunks',  # Remote directory for uploaded chunks
-}
diff --git a/tests/test_core.py b/tests/test_core.py
deleted file mode 100644
index bf0e5bb..0000000
--- a/tests/test_core.py
+++ /dev/null
@@ -1,221 +0,0 @@
-"""
-Core utility tests - Config, ErrorTracker, ExtractedData, ChunkMetadata.
-
-Tests only essential functionality:
-- Config creation, validation, and serialization
-- ErrorTracker error/warning handling and modes
-- ExtractedData pipeline flow
-- ChunkMetadata creation and serialization
-"""
-
-import pytest
-from unittest.mock import Mock
-import dtlpy as dl
-
-from utils.config import Config
-from utils.errors import ErrorTracker
-from utils.extracted_data import ExtractedData
-from utils.chunk_metadata import ChunkMetadata
-from utils.data_types import ImageContent
-
-
-class TestConfig:
-    """Essential Config tests."""
-
-    def test_default_values(self):
-        """Config has sensible defaults."""
-        config = Config()
-        assert config.error_mode == 'continue'
-        assert config.max_chunk_size == 300
-        assert config.chunking_strategy == 'recursive'
-
-    def test_from_dict(self):
-        """Config can be created from dict."""
-        config = Config.from_dict({'max_chunk_size': 500, 'ocr_from_images': True})
-        assert config.max_chunk_size == 500
-        assert config.ocr_from_images is True
-
-    def test_from_dict_ignores_unknown_keys(self):
-        """Unknown keys are ignored."""
-        config = Config.from_dict({'max_chunk_size': 500, 'unknown_key': 'ignored'})
-        assert config.max_chunk_size == 500
-
-    def test_to_dict_roundtrip(self):
-        """Config survives dict roundtrip."""
-        original = Config(max_chunk_size=500, ocr_from_images=True)
-        restored = Config.from_dict(original.to_dict())
-        assert restored.max_chunk_size == original.max_chunk_size
-        assert restored.ocr_from_images == original.ocr_from_images
-
-    def test_validate_rejects_invalid_chunk_size(self):
-        """Validation catches invalid chunk size."""
-        config = Config(max_chunk_size=-1)
-        with pytest.raises(ValueError, match="max_chunk_size"):
-            config.validate()
-
-    def test_validate_rejects_overlap_greater_than_size(self):
-        """Validation catches overlap >= chunk size."""
-        config = Config(max_chunk_size=100, chunk_overlap=100)
-        with pytest.raises(ValueError, match="chunk_overlap"):
-            config.validate()
-
-    def test_validate_rejects_batch_ocr_without_model(self):
-        """Validation catches batch OCR without model ID."""
-        config = Config(ocr_from_images=True, ocr_method='batch', ocr_model_id=None)
-        with pytest.raises(ValueError, match="ocr_model_id"):
-            config.validate()
-
-    def test_validate_rejects_llm_features_without_model(self):
-        """Validation catches LLM features without model ID."""
-        config = Config(generate_summary=True, llm_model_id=None)
-        with pytest.raises(ValueError, match="llm_model_id"):
-            config.validate()
-
-
-class TestErrorTracker:
-    """Essential ErrorTracker tests."""
-
-    def test_add_error_records_message(self):
-        """Errors are recorded with stage prefix."""
-        tracker = ErrorTracker()
-        tracker.add_error("Something failed", stage="extraction")
-        assert len(tracker.errors) == 1
-        assert "[extraction]" in tracker.errors[0]
-
-    def test_continue_mode_allows_errors_up_to_max(self):
-        """Continue mode allows errors until max_errors."""
-        tracker = ErrorTracker(error_mode='continue', max_errors=2)
-        assert tracker.add_error("Error 1") is True
-        assert tracker.add_error("Error 2") is False
-
-    def test_stop_mode_stops_on_first_error(self):
-        """Stop mode returns False on first error."""
-        tracker = ErrorTracker(error_mode='stop')
-        assert tracker.add_error("First error") is False
-
-    def test_warnings_dont_affect_max_errors(self):
-        """Warnings don't count toward max_errors."""
-        tracker = ErrorTracker(max_errors=2)
-        tracker.add_warning("Warning 1")
-        tracker.add_warning("Warning 2")
-        # First error should still succeed (warnings don't count)
-        assert tracker.add_error("Error 1") is True
-
-
-class TestExtractedData:
-    """Essential ExtractedData tests."""
-
-    def test_default_creation(self):
-        """ExtractedData has sensible defaults."""
-        data = ExtractedData()
-        assert data.content_text == ""
-        assert data.chunks == []
-        assert data.current_stage == "init"
-
-    def test_config_sync_with_error_tracker(self):
-        """Error tracker syncs with config settings."""
-        data = ExtractedData(config=Config(error_mode='stop', max_errors=5))
-        assert data.errors.error_mode == 'stop'
-        assert data.errors.max_errors == 5
-
-    def test_config_from_dict(self):
-        """Config can be passed as dict."""
-        data = ExtractedData(config={'max_chunk_size': 1000})
-        assert isinstance(data.config, Config)
-        assert data.config.max_chunk_size == 1000
-
-    def test_get_text_prefers_cleaned(self):
-        """get_text returns cleaned text when available."""
-        data = ExtractedData()
-        data.content_text = "raw"
-        data.cleaned_text = "cleaned"
-        assert data.get_text() == "cleaned"
-
-    def test_log_error_includes_stage(self):
-        """log_error includes current stage in message."""
-        data = ExtractedData()
-        data.current_stage = "extraction"
-        data.log_error("Failed")
-        assert "[extraction]" in data.errors.errors[0]
-
-    def test_has_content_checks_text_and_images(self):
-        """has_content checks text, images, and tables."""
-        data = ExtractedData()
-        assert data.has_content() is False
-
-        data.content_text = "text"
-        assert data.has_content() is True
-
-    def test_item_properties(self):
-        """item_name and item_id work with mock item."""
-        class MockItem:
-            name = "test.pdf"
-            id = "item-123"
-
-        data = ExtractedData(item=MockItem())
-        assert data.item_name == "test.pdf"
-        assert data.item_id == "item-123"
-
-
-class TestChunkMetadata:
-    """Essential ChunkMetadata tests."""
-
-    def test_creation_with_required_fields(self):
-        """ChunkMetadata requires essential fields."""
-        metadata = ChunkMetadata(
-            source_item_id='item123',
-            source_file='test.pdf',
-            source_dataset_id='dataset123',
-            chunk_index=0,
-            total_chunks=10,
-        )
-        assert metadata.source_item_id == 'item123'
-        assert metadata.chunk_index == 0
-
-    def test_validation_rejects_empty_item_id(self):
-        """Empty source_item_id is rejected."""
-        with pytest.raises(ValueError, match="source_item_id is required"):
-            ChunkMetadata(
-                source_item_id='',
-                source_file='test.pdf',
-                source_dataset_id='dataset123',
-                chunk_index=0,
-                total_chunks=10,
-            )
-
-    def test_validation_rejects_negative_chunk_index(self):
-        """Negative chunk_index is rejected."""
-        with pytest.raises(ValueError, match="chunk_index must be non-negative"):
-            ChunkMetadata(
-                source_item_id='item123',
-                source_file='test.pdf',
-                source_dataset_id='dataset123',
-                chunk_index=-1,
-                total_chunks=10,
-            )
-
-    def test_to_dict_structure(self):
-        """to_dict returns proper structure."""
-        metadata = ChunkMetadata(
-            source_item_id='item123',
-            source_file='test.pdf',
-            source_dataset_id='dataset123',
-            chunk_index=0,
-            total_chunks=10,
-            processor='pdf',
-        )
-        result = metadata.to_dict()
-        assert 'user' in result
-        assert result['user']['source_item_id'] == 'item123'
-        assert result['user']['processor'] == 'pdf'
-
-    def test_create_from_item(self):
-        """ChunkMetadata.create() works with Dataloop item."""
-        mock_item = Mock(spec=dl.Item)
-        mock_item.id = 'item123'
-        mock_item.name = 'test.pdf'
-        mock_item.dataset.id = 'dataset123'
-
-        metadata = ChunkMetadata.create(source_item=mock_item, total_chunks=10, chunk_index=0)
-        assert metadata.source_item_id == 'item123'
-        assert metadata.source_file == 'test.pdf'
diff --git a/tests/test_extractors.py b/tests/test_extractors.py
deleted file mode 100644
index 7de511d..0000000
--- a/tests/test_extractors.py
+++ /dev/null
@@ -1,255 +0,0 @@
-"""Tests for PDF and DOC extractors."""
-import pytest
-from unittest.mock import Mock, patch, MagicMock
-import os
-import tempfile
-
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-from apps.pdf_processor.pdf_extractor import PDFExtractor
-from apps.doc_processor.doc_extractor import DOCExtractor
-
-
-class TestPDFExtractorBasics:
-    """Test PDFExtractor basic behavior."""
-
-    def test_extract_without_item_logs_error(self):
-        """Should log error when no item provided."""
-        data = ExtractedData()
-
-        result = PDFExtractor.extract(data)
-
-        assert len(result.errors.errors) > 0
-        assert "No item provided" in result.errors.errors[0]
-
-    def test_extract_sets_current_stage(self):
-        """Should set current_stage to extraction."""
-        data = ExtractedData()
-
-        PDFExtractor.extract(data)
-
-        assert data.current_stage == "extraction"
-
-
-class TestPDFExtractorWithMocks:
-    """Test PDFExtractor with mocked PDF library."""
-
-    @pytest.fixture
-    def mock_fitz(self):
-        """Create mock fitz module."""
-        with patch('apps.pdf_processor.pdf_extractor.fitz') as mock:
-            # Setup mock document
-            mock_doc = MagicMock()
-            mock_page = MagicMock()
-            mock_page.get_text.return_value = "Page 1 content"
-            mock_page.get_images.return_value = []
-            mock_doc.__iter__ = lambda self: iter([mock_page])
-            mock_doc.__len__ = lambda self: 1
-            mock.open.return_value = mock_doc
-            yield mock
-
-    @pytest.fixture
-    def mock_item(self):
-        """Create mock Dataloop item."""
-        item = Mock()
-        item.name = "test.pdf"
-        item.id = "item-123"
-
-        # Mock download to create a temp file
-        def mock_download(local_path=None):
-            path = os.path.join(local_path or tempfile.gettempdir(), "test.pdf")
-            with open(path, 'wb') as f:
-                f.write(b'%PDF-1.4 fake pdf content')
-            return path
-
-        item.download = mock_download
-        return item
-
-    def test_extract_basic_pymupdf(self, mock_fitz, mock_item):
-        """Should extract text using basic PyMuPDF."""
-        config = Config(extraction_method='basic', extract_images=False)
-        data = ExtractedData(item=mock_item, config=config)
-
-        result = PDFExtractor.extract(data)
-
-        assert "Page 1 content" in result.content_text
-        assert result.metadata.get('extraction_method') == 'pymupdf'
-        assert len(result.errors.errors) == 0
-
-    def test_extract_populates_metadata(self, mock_fitz, mock_item):
-        """Should populate metadata correctly."""
-        config = Config(extraction_method='basic', extract_images=False)
-        data = ExtractedData(item=mock_item, config=config)
-
-        result = PDFExtractor.extract(data)
-
-        assert result.metadata.get('source_file') == 'test.pdf'
-        assert result.metadata.get('processor') == 'pdf'
-        assert 'page_count' in result.metadata
-
-
-class TestDOCExtractorBasics:
-    """Test DOCExtractor basic behavior."""
-
-    def test_extract_without_item_logs_error(self):
-        """Should log error when no item provided."""
-        data = ExtractedData()
-
-        result = DOCExtractor.extract(data)
-
-        assert len(result.errors.errors) > 0
-        assert "No item provided" in result.errors.errors[0]
-
-    def test_extract_sets_current_stage(self):
-        """Should set current_stage to extraction."""
-        data = ExtractedData()
-
-        DOCExtractor.extract(data)
-
-        assert data.current_stage == "extraction"
-
-
-class TestDOCExtractorWithMocks:
-    """Test DOCExtractor with mocked docx library."""
-
-    @pytest.fixture
-    def mock_document(self):
-        """Create mock Document class."""
-        with patch('apps.doc_processor.doc_extractor.Document') as MockDoc:
-            mock_doc = MagicMock()
-
-            # Mock paragraph elements
-            mock_element1 = MagicMock()
-            mock_element1.tag = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p'
-            mock_element2 = MagicMock()
-            mock_element2.tag = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p'
-
-            # Mock paragraphs
-            mock_para1 = MagicMock()
-            mock_para1.text = "First paragraph"
-            mock_para1.style.name = "Normal"
-            mock_para1._element = mock_element1
-            mock_para1.runs = []
-
-            mock_para2 = MagicMock()
-            mock_para2.text = "Second paragraph"
-            mock_para2.style.name = "Normal"
-            mock_para2._element = mock_element2
-            mock_para2.runs = []
-
-            mock_doc.paragraphs = [mock_para1, mock_para2]
-
-            # Mock document body iteration
-            mock_doc.element.body = [mock_element1, mock_element2]
-
-            # Mock tables (empty)
-            mock_doc.tables = []
-
-            # Mock rels for images (empty)
-            mock_doc.part.rels.values.return_value = []
-
-            MockDoc.return_value = mock_doc
-            yield MockDoc
-
-    @pytest.fixture
-    def mock_item(self):
-        """Create mock Dataloop item."""
-        item = Mock()
-        item.name = "test.docx"
-        item.id = "item-456"
-
-        def mock_download(local_path=None):
-            path = os.path.join(local_path or tempfile.gettempdir(), "test.docx")
-            with open(path, 'wb') as f:
-                f.write(b'fake docx content')
-            return path
-
-        item.download = mock_download
-        return item
-
-    def test_extract_paragraphs(self, mock_document, mock_item):
-        """Should extract text from paragraphs."""
-        config = Config(extract_images=False, extract_tables=False)
-        data = ExtractedData(item=mock_item, config=config)
-
-        result = DOCExtractor.extract(data)
-
-        assert "First paragraph" in result.content_text
-        assert "Second paragraph" in result.content_text
-        assert len(result.errors.errors) == 0
-
-    def test_extract_populates_metadata(self, mock_document, mock_item):
-        """Should populate metadata correctly."""
-        config = Config(extract_images=False, extract_tables=False)
-        data = ExtractedData(item=mock_item, config=config)
-
-        result = DOCExtractor.extract(data)
-
-        assert result.metadata.get('source_file') == 'test.docx'
-        assert result.metadata.get('processor') == 'doc'
-        assert result.metadata.get('extraction_method') == 'python-docx'
-        assert result.metadata.get('table_count') == 0
-
-
-class TestDOCExtractorTableConversion:
-    """Test table to markdown conversion."""
-
-    def test_table_to_markdown_basic(self):
-        """Should convert table to markdown format."""
-        headers = ["Name", "Age", "City"]
-        rows = [
-            {"Name": "Alice", "Age": "30", "City": "NYC"},
-            {"Name": "Bob", "Age": "25", "City": "LA"},
-        ]
-
-        result = DOCExtractor._table_to_markdown(headers, rows)
-
-        assert "| Name | Age | City |" in result
-        assert "| --- | --- | --- |" in result
-        assert "| Alice | 30 | NYC |" in result
-        assert "| Bob | 25 | LA |" in result
-
-    def test_table_to_markdown_empty_headers(self):
-        """Should handle empty headers."""
-        result = DOCExtractor._table_to_markdown([], [])
-        assert result == ""
-
-    def test_table_to_markdown_missing_values(self):
-        """Should handle missing values in rows."""
-        headers = ["A", "B"]
-        rows = [{"A": "1"}]  # Missing "B"
-
-        result = DOCExtractor._table_to_markdown(headers, rows)
-
-        assert "| 1 |  |" in result  # Empty value for B
-
-
-class TestExtractorIntegration:
-    """Test extractor integration with ExtractedData."""
-
-    def test_pdf_extractor_error_tracking(self):
-        """PDFExtractor should integrate with error tracking."""
-        data = ExtractedData(config=Config(error_mode='continue', max_errors=5))
-
-        PDFExtractor.extract(data)  # Will fail - no item
-
-        assert len(data.errors.errors) > 0
-        assert data.current_stage == "extraction"
-
-    def test_doc_extractor_error_tracking(self):
-        """DOCExtractor should integrate with error tracking."""
-        data = ExtractedData(config=Config(error_mode='continue', max_errors=5))
-
-        DOCExtractor.extract(data)  # Will fail - no item
-
-        assert len(data.errors.errors) > 0
-        assert data.current_stage == "extraction"
-
-    def test_extractor_respects_config(self):
-        """Extractors should respect configuration."""
-        config = Config(extract_images=False, extract_tables=False)
-        data = ExtractedData(config=config)
-
-        # Verify config is accessible
-        assert data.config.extract_images is False
-        assert data.config.extract_tables is False
diff --git a/tests/test_pdf_processor.py b/tests/test_pdf_processor.py
new file mode 100644
index 0000000..de2f917
--- /dev/null
+++ b/tests/test_pdf_processor.py
@@ -0,0 +1,178 @@
+"""
+Simple test for PDF Processor.
+
+Usage:
+    python tests/test_pdf_processor.py
+    
+    Edit the ITEM_ID, TARGET_DATASET_ID, and CONFIG variables below, then run the script.
+"""
+
+import sys
+import os
+
+# Add repository root to path for imports
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+
+import dtlpy as dl
+
+# Import from apps folder
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../apps/pdf-processor')))
+from pdf_processor import PDFProcessor
+
+
+# ============================================================
+# CONFIGURATION - Edit these values
+# ============================================================
+
+# Your Dataloop item ID to test
+ITEM_ID = "item_id"
+
+# Target dataset ID for storing chunks (required parameter)
+# Set to None to auto-create a {dataset_name}_chunks dataset
+TARGET_DATASET_ID = "dataset_id"
+
+# Configuration (matches dataloop.json schema)
+CONFIG = {
+    'name': 'Test-PDF-Processor',
+    
+    # OCR Processing
+    'ocr_from_images': True,  # Extract images and apply OCR (uses EasyOCR)
+    'ocr_integration_method': 'separate_chunks',  # Options: 'append_to_page', 'separate_chunks', 'combine_all'
+    
+    # Text Extraction
+    'use_markdown_extraction': False,
+    
+    # Chunking Strategy
+    'chunking_strategy': 'recursive',  # Options: 'recursive', 'fixed-size', 'nltk-sentence', 'nltk-paragraphs', '1-chunk'
+    'max_chunk_size': 500,
+    'chunk_overlap': 20,
+    
+    # Text Cleaning
+    'to_correct_spelling': True,
+}
+
+
+# ============================================================
+# Mock Context (replicates Dataloop pipeline context)
+# ============================================================
+
+class MockNode:
+    """Mock Dataloop node with configuration."""
+    def __init__(self, config):
+        self.metadata = {'customNodeConfig': config}
+
+
+class MockContext:
+    """Mock Dataloop context with node configuration."""
+    def __init__(self, config):
+        self.node = MockNode(config)
+
+
+# ============================================================
+# Test Function
+# ============================================================
+
+def test_pdf_processor(item_id: str, target_dataset_id: str, config: dict):
+    """
+    Test PDF processor with a Dataloop item.
+    
+    Args:
+        item_id: Dataloop item ID to process
+        target_dataset_id: Target dataset ID for storing chunks (None to auto-create)
+        config: Configuration dictionary
+        
+    Returns:
+        List of chunk items created
+    """
+    print(f"\n{'='*60}")
+    print(f"Testing PDF Processor")
+    print(f"{'='*60}\n")
+    
+    # Get the item
+    print(f"ðŸ“¥ Fetching item: {item_id}")
+    try:
+        item = dl.items.get(item_id=item_id)
+        print(f"âœ… Retrieved: {item.name} ({item.mimetype})")
+    except Exception as e:
+        print(f"âŒ Failed to retrieve item: {str(e)}")
+        raise
+    
+    # Get or create target dataset
+    print(f"\nðŸ“¦ Setting up target dataset...")
+    if target_dataset_id:
+        try:
+            target_dataset = dl.datasets.get(dataset_id=target_dataset_id)
+            print(f"âœ… Using specified dataset: {target_dataset.name} (ID: {target_dataset.id})")
+        except Exception as e:
+            print(f"âŒ Failed to get target dataset: {str(e)}")
+            raise
+    else:
+        # Auto-create chunks dataset
+        original_dataset_name = item.dataset.name
+        new_dataset_name = f"{original_dataset_name}_chunks"
+        print(f"ðŸ“ Auto-creating dataset: {new_dataset_name}")
+        
+        try:
+            # Try to get existing dataset
+            target_dataset = item.project.datasets.get(dataset_name=new_dataset_name)
+            print(f"âœ… Using existing chunks dataset: {target_dataset.name} (ID: {target_dataset.id})")
+        except dl.exceptions.NotFound:
+            # Create new dataset
+            target_dataset = item.project.datasets.create(dataset_name=new_dataset_name)
+            print(f"âœ… Created new dataset: {target_dataset.name} (ID: {target_dataset.id})")
+    
+    # Show configuration
+    print(f"\nðŸ“‹ Configuration:")
+    for key, value in config.items():
+        print(f"  â€¢ {key}: {value}")
+    
+    # Create mock context
+    context = MockContext(config)
+    
+    # Initialize PDF Processor
+    print(f"\nðŸ”§ Initializing PDF Processor...")
+    processor = PDFProcessor()
+    
+    # Process the item
+    print(f"\nâš™ï¸  Processing PDF...")
+    try:
+        chunk_items = processor.process_document(item, target_dataset, context)
+        
+        print(f"\n{'='*60}")
+        print(f"âœ… Success!")
+        print(f"{'='*60}")
+        print(f"\nðŸ“Š Results:")
+        print(f"  â€¢ Total chunks: {len(chunk_items)}")
+        print(f"  â€¢ Target dataset: {chunk_items[0].dataset.name if chunk_items else 'N/A'}")
+        print(f"  â€¢ Remote path: {chunk_items[0].dir if chunk_items else 'N/A'}")
+        
+        if chunk_items:
+            print(f"\nðŸ“ Sample chunks:")
+            for i, chunk_item in enumerate(chunk_items[:3]):
+                print(f"  {i+1}. {chunk_item.name}")
+            if len(chunk_items) > 3:
+                print(f"  ... and {len(chunk_items) - 3} more")
+        
+        return chunk_items
+        
+    except Exception as e:
+        print(f"\nâŒ Failed: {str(e)}")
+        import traceback
+        traceback.print_exc()
+        raise
+
+
+# ============================================================
+# Main
+# ============================================================
+
+if __name__ == '__main__':
+    """Run the test with the configured item ID, target dataset, and config."""
+    # Run test
+    try:
+        chunks = test_pdf_processor(ITEM_ID, TARGET_DATASET_ID, CONFIG)
+        print(f"\nâœ… Test completed successfully!")
+        sys.exit(0)
+    except Exception as e:
+        print(f"\nâŒ Test failed!")
+        sys.exit(1)
diff --git a/tests/test_processors.py b/tests/test_processors.py
deleted file mode 100644
index e8c0bf3..0000000
--- a/tests/test_processors.py
+++ /dev/null
@@ -1,129 +0,0 @@
-"""
-Integration tests for PDF and DOC processors.
-
-Usage:
-    pytest tests/test_processors.py -v
-    pytest tests/test_processors.py -k pdf
-    pytest tests/test_processors.py -k doc
-
-Edit TEST_ITEMS, PDF_CONFIG, and DOC_CONFIG in tests/test_config.py.
-"""
-
-import sys
-import traceback
-from unittest.mock import MagicMock
-
-import pytest
-import dtlpy as dl
-
-from apps.pdf_processor.app import PDFProcessor
-from apps.doc_processor.app import DOCProcessor
-from tests.test_config import TEST_ITEMS, TARGET_DATASET_ID, PDF_CONFIG, DOC_CONFIG
-
-
-# Processor registry for parameterized tests
-PROCESSORS = {
-    'pdf': {
-        'class': PDFProcessor,
-        'config': PDF_CONFIG,
-        'label': 'PDF',
-    },
-    'doc': {
-        'class': DOCProcessor,
-        'config': DOC_CONFIG,
-        'label': 'DOC/DOCX',
-    },
-}
-
-
-def _create_mock_context(config: dict) -> dl.Context:
-    """Create a mock dl.Context with the given config."""
-    context = MagicMock(spec=dl.Context)
-    context.node = MagicMock()
-    context.node.metadata = {'customNodeConfig': config}
-    return context
-
-
-@pytest.mark.parametrize("file_type", ['pdf', 'doc'])
-def test_processor(file_type):
-    """
-    Test processor with a Dataloop item.
-
-    Uses TEST_ITEMS, TARGET_DATASET_ID, and configs from test_config.py.
-    Asserts that chunks were successfully created.
-    """
-    # Validate configuration
-    if file_type not in TEST_ITEMS:
-        pytest.skip(f"'{file_type}' test configuration not found in TEST_ITEMS")
-
-    if not TARGET_DATASET_ID:
-        pytest.fail("TARGET_DATASET_ID is required. Set it in tests/test_config.py")
-
-    processor_info = PROCESSORS[file_type]
-    processor_cls = processor_info['class']
-    config = processor_info['config']
-    label = processor_info['label']
-
-    print(f"\n{'='*60}")
-    print(f"Testing {label} Processor")
-    print(f"{'='*60}\n")
-
-    # Get the item
-    item_id = TEST_ITEMS[file_type]['item_id']
-    print(f"Fetching item: {item_id}")
-    item = dl.items.get(item_id=item_id)
-    print(f"Retrieved: {item.name} ({item.mimetype})")
-    print(f"Source dataset: {item.dataset.name} (ID: {item.dataset.id})")
-
-    # Get target dataset
-    print(f"\nFetching target dataset: {TARGET_DATASET_ID}")
-    target_dataset = dl.datasets.get(dataset_id=TARGET_DATASET_ID)
-    print(f"Target dataset: {target_dataset.name} (ID: {target_dataset.id})")
-
-    # Show configuration
-    print(f"\nConfiguration:")
-    for key, value in config.items():
-        print(f"  {key}: {value}")
-
-    # Run processor
-    print(f"\nProcessing {label}...")
-    context = _create_mock_context(config)
-    chunk_items = processor_cls.run(item, target_dataset, context)
-
-    # Results
-    print(f"\n{'='*60}")
-    print(f"Success!")
-    print(f"{'='*60}")
-    print(f"\nResults:")
-    print(f"  Total chunks: {len(chunk_items)}")
-    print(f"  Target dataset: {chunk_items[0].dataset.name if chunk_items else 'N/A'}")
-    print(f"  Remote path: {chunk_items[0].dir if chunk_items else 'N/A'}")
-
-    if chunk_items:
-        print(f"\nSample chunks:")
-        for i, chunk_item in enumerate(chunk_items[:3]):
-            print(f"  {i+1}. {chunk_item.name}")
-        if len(chunk_items) > 3:
-            print(f"  ... and {len(chunk_items) - 3} more")
-
-    assert chunk_items, "No chunks were created"
-    assert len(chunk_items) > 0, "Expected at least one chunk"
-
-
-if __name__ == '__main__':
-    # Allow running specific processor from command line
-    file_type = sys.argv[1] if len(sys.argv) > 1 else 'pdf'
-
-    if file_type not in PROCESSORS:
-        print(f"Unknown file type: {file_type}")
-        print(f"Available: {', '.join(PROCESSORS.keys())}")
-        sys.exit(1)
-
-    try:
-        test_processor(file_type)
-        print(f"\nTest completed successfully!")
-        sys.exit(0)
-    except Exception as e:
-        print(f"\nTest failed: {e}")
-        traceback.print_exc()
-        sys.exit(1)
diff --git a/tests/test_transforms.py b/tests/test_transforms.py
deleted file mode 100644
index a540029..0000000
--- a/tests/test_transforms.py
+++ /dev/null
@@ -1,139 +0,0 @@
-"""
-Tests for transform functions.
-
-Tests core transform functionality:
-- clean() text normalization
-- chunk() text splitting
-- Transform chaining
-"""
-
-import pytest
-from utils.extracted_data import ExtractedData
-from utils.config import Config
-from utils.data_types import ImageContent
-import transforms
-
-
-class TestCleanTransform:
-    """Tests for text cleaning transform."""
-
-    def test_clean_normalizes_text(self):
-        """clean() strips whitespace, normalizes spaces and newlines."""
-        data = ExtractedData(config=Config())
-        data.content_text = "  Hello    World  \n\n\n\nMore text  "
-        result = transforms.clean(data)
-        assert result.cleaned_text == "Hello World\nMore text"
-        assert result.current_stage == "cleaning"
-        assert result.metadata.get('cleaning_applied') is True
-
-    def test_clean_respects_config_options(self):
-        """clean() respects normalize_whitespace and remove_empty_lines config."""
-        # With normalization disabled
-        data = ExtractedData(config=Config(normalize_whitespace=False, remove_empty_lines=False))
-        data.content_text = "Hello    World\n\n\nMore"
-        result = transforms.clean(data)
-        assert "    " in result.cleaned_text  # Multiple spaces preserved
-        assert "\n\n" in result.cleaned_text  # Empty lines preserved
-
-    def test_clean_empty_content(self):
-        """clean() handles empty content."""
-        data = ExtractedData(config=Config())
-        data.content_text = ""
-        result = transforms.clean(data)
-        assert result.cleaned_text == ""
-
-
-class TestChunkTransform:
-    """Tests for chunking transform."""
-
-    def test_chunk_creates_chunks(self):
-        """chunk() splits text into chunks."""
-        data = ExtractedData(config=Config(max_chunk_size=50, chunk_overlap=10))
-        data.content_text = "This is a long text that should be split into multiple chunks for testing purposes."
-        result = transforms.chunk(data)
-        assert len(result.chunks) > 1
-        assert result.current_stage == "chunking"
-        assert 'chunk_count' in result.metadata
-
-    def test_chunk_uses_cleaned_text(self):
-        """chunk() uses cleaned_text when available."""
-        data = ExtractedData(config=Config())
-        data.content_text = "raw content"
-        data.cleaned_text = "cleaned content"
-        result = transforms.chunk(data)
-        assert len(result.chunks) > 0
-
-    def test_chunk_empty_content(self):
-        """chunk() handles empty content."""
-        data = ExtractedData(config=Config())
-        data.content_text = ""
-        result = transforms.chunk(data)
-        assert result.chunks == []
-
-
-class TestChunkWithImages:
-    """Tests for chunking with image association."""
-
-    def test_chunk_with_images_creates_metadata(self):
-        """chunk_with_images() associates chunks with page images."""
-        data = ExtractedData(config=Config(max_chunk_size=100))
-        data.content_text = "--- Page 1 ---\nText on page 1.\n--- Page 2 ---\nText on page 2."
-        data.images = [
-            ImageContent(path="/tmp/img1.png", page_number=1),
-            ImageContent(path="/tmp/img2.png", page_number=2),
-        ]
-        result = transforms.chunk_with_images(data)
-        assert len(result.chunk_metadata) == len(result.chunks)
-        assert result.current_stage == "chunking"
-
-
-class TestTextChunker:
-    """Tests for TextChunker strategies."""
-
-    def test_chunker_strategies(self):
-        """TextChunker supports multiple strategies."""
-        text = "This is some text that needs to be chunked into pieces."
-
-        # Recursive (default)
-        chunks = transforms.TextChunker.chunk(text, chunk_size=30, strategy='recursive')
-        assert len(chunks) > 0
-
-        # Fixed
-        chunks = transforms.TextChunker.chunk(text, chunk_size=20, strategy='fixed')
-        assert len(chunks) > 0
-
-        # None (no splitting)
-        chunks = transforms.TextChunker.chunk(text, strategy='none')
-        assert len(chunks) == 1
-        assert chunks[0] == text
-
-
-class TestTransformChaining:
-    """Tests for chaining transforms together."""
-
-    def test_clean_then_chunk_pipeline(self):
-        """Transforms can be chained: clean -> chunk."""
-        data = ExtractedData(config=Config(max_chunk_size=50))
-        data.content_text = "  Hello    World   with   extra   spaces  "
-
-        data = transforms.clean(data)
-        data = transforms.chunk(data)
-
-        assert data.cleaned_text == "Hello World with extra spaces"
-        assert len(data.chunks) > 0
-        assert data.metadata.get('cleaning_applied') is True
-
-
-class TestTransformSignatures:
-    """Verify transforms follow correct signature."""
-
-    def test_all_transforms_return_extracted_data(self):
-        """All transforms return ExtractedData."""
-        data = ExtractedData(config=Config())
-        data.content_text = "test content"
-
-        assert isinstance(transforms.clean(data), ExtractedData)
-        assert isinstance(transforms.chunk(data), ExtractedData)
-        assert isinstance(transforms.deep_clean(data), ExtractedData)
-        assert isinstance(transforms.llm_chunk_semantic(data), ExtractedData)
-        assert isinstance(transforms.llm_summarize(data), ExtractedData)
diff --git a/transforms/__init__.py b/transforms/__init__.py
deleted file mode 100644
index ecc500f..0000000
--- a/transforms/__init__.py
+++ /dev/null
@@ -1,36 +0,0 @@
-"""
-Processing transforms for document processing.
-
-All transforms follow signature: (data: ExtractedData) -> ExtractedData
-"""
-
-from .text_normalization import clean, deep_clean, TextNormalizer
-from .chunking import chunk, chunk_with_images, TextChunker
-from .ocr import ocr_enhance, describe_images, OCREnhancer, ImageDescriber
-from .llm import llm_chunk_semantic, llm_summarize, llm_extract_entities, llm_translate, LLMProcessor
-from .upload import upload_to_dataloop, ChunkUploader
-
-__all__ = [
-    # Text Normalization
-    'clean',
-    'deep_clean',
-    'TextNormalizer',
-    # Chunking
-    'chunk',
-    'chunk_with_images',
-    'TextChunker',
-    # OCR
-    'ocr_enhance',
-    'describe_images',
-    'OCREnhancer',
-    'ImageDescriber',
-    # LLM
-    'llm_chunk_semantic',
-    'llm_summarize',
-    'llm_extract_entities',
-    'llm_translate',
-    'LLMProcessor',
-    # Upload
-    'upload_to_dataloop',
-    'ChunkUploader',
-]
diff --git a/transforms/chunking.py b/transforms/chunking.py
deleted file mode 100644
index 666f2a3..0000000
--- a/transforms/chunking.py
+++ /dev/null
@@ -1,190 +0,0 @@
-"""
-Chunking transforms for splitting content into chunks.
-
-All functions follow signature: (data: ExtractedData) -> ExtractedData
-"""
-
-import re
-from typing import List
-import logging
-import nltk
-from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
-
-from utils.extracted_data import ExtractedData
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class TextChunker:
-    """Text chunking with support for multiple strategies."""
-
-    @staticmethod
-    def chunk(
-        text: str,
-        chunk_size: int = 300,
-        chunk_overlap: int = 20,
-        strategy: str = 'recursive'
-    ) -> List[str]:
-        """
-        Split text into chunks based on strategy.
-
-        Args:
-            text: Input text to chunk
-            chunk_size: Maximum size of each chunk
-            chunk_overlap: Overlap between chunks
-            strategy: Chunking strategy ('fixed-size', 'recursive', 'nltk-sentence', 'nltk-paragraphs', '1-chunk')
-
-        Returns:
-            List of text chunks
-        """
-        logger.info(f"Chunking | strategy={strategy} size={chunk_size} overlap={chunk_overlap}")
-
-        if strategy == 'fixed-size':
-            chunks = TextChunker._chunk_fixed_size(text, chunk_size, chunk_overlap)
-        elif strategy == 'recursive':
-            chunks = TextChunker._chunk_recursive(text, chunk_size, chunk_overlap)
-        elif strategy == 'nltk-sentence':
-            chunks = TextChunker._chunk_sentence(text)
-        elif strategy == 'nltk-paragraphs':
-            chunks = TextChunker._chunk_paragraphs(text)
-        elif strategy == '1-chunk':
-            chunks = [text] if text else []
-        else:
-            logger.warning(f"Unknown strategy: {strategy}, using recursive")
-            chunks = TextChunker._chunk_recursive(text, chunk_size, chunk_overlap)
-
-        logger.info(f"Chunking complete | chunks={len(chunks)}")
-        return chunks
-
-    @staticmethod
-    def _chunk_fixed_size(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
-        """Fixed-size chunking."""
-        splitter = CharacterTextSplitter(
-            separator="",
-            chunk_size=chunk_size,
-            chunk_overlap=chunk_overlap
-        )
-        docs = splitter.create_documents([text])
-        return [doc.page_content for doc in docs]
-
-    @staticmethod
-    def _chunk_recursive(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
-        """Recursive chunking that respects semantic boundaries."""
-        splitter = RecursiveCharacterTextSplitter(
-            chunk_size=chunk_size,
-            chunk_overlap=chunk_overlap,
-            length_function=len,
-        )
-        docs = splitter.create_documents([text])
-        return [doc.page_content for doc in docs]
-
-    @staticmethod
-    def _chunk_sentence(text: str) -> List[str]:
-        """Chunk by sentence boundaries using NLTK."""
-        return nltk.sent_tokenize(text)
-
-    @staticmethod
-    def _chunk_paragraphs(text: str) -> List[str]:
-        """Chunk by paragraph boundaries using NLTK."""
-        return nltk.tokenize.blankline_tokenize(text)
-
-
-# Transform wrappers
-
-def chunk(data: ExtractedData) -> ExtractedData:
-    """
-    Chunk content using the strategy specified in config.
-
-    Handles strategy selection internally:
-    - 'semantic': Uses LLM-based semantic chunking
-    - 'recursive' with images: Uses chunk_with_images for page/image association
-    - Other strategies: Uses TextChunker (fixed, recursive, sentence, none)
-    """
-    data.current_stage = "chunking"
-    strategy = data.config.chunking_strategy
-
-    if strategy == 'semantic':
-        from .llm import llm_chunk_semantic
-        return llm_chunk_semantic(data)
-
-    if strategy == 'recursive' and data.has_images():
-        return chunk_with_images(data)
-
-    content = data.get_text()
-    if not content:
-        data.chunks = []
-        return data
-
-    data.chunks = TextChunker.chunk(
-        text=content,
-        chunk_size=data.config.max_chunk_size,
-        chunk_overlap=data.config.chunk_overlap,
-        strategy=strategy,
-    )
-    data.metadata['chunking_strategy'] = strategy
-    data.metadata['chunk_count'] = len(data.chunks)
-
-    return data
-
-
-def chunk_with_images(data: ExtractedData) -> ExtractedData:
-    """Chunk content and associate images based on page numbers."""
-    data.current_stage = "chunking"
-    content = data.get_text()
-
-    if not content:
-        data.chunks = []
-        data.chunk_metadata = []
-        return data
-
-    # Extract page positions from content
-    page_positions = []
-    for match in re.finditer(r'---\s*Page\s+(\d+)\s*---', content, re.IGNORECASE):
-        page_positions.append((match.start(), int(match.group(1))))
-
-    chunks = TextChunker.chunk(
-        text=content,
-        chunk_size=data.config.max_chunk_size,
-        chunk_overlap=data.config.chunk_overlap,
-        strategy='recursive',
-    )
-
-    # Build chunk metadata with page and image associations
-    chunk_metadata = []
-    for chunk_idx, chunk_text in enumerate(chunks):
-        chunk_start = content.find(chunk_text)
-        if chunk_start == -1:
-            chunk_start = sum(len(c) for c in chunks[:chunk_idx])
-
-        # Find pages for this chunk
-        chunk_pages = set()
-        for pos, page_num in page_positions:
-            if pos <= chunk_start:
-                chunk_pages.add(page_num)
-            elif pos > chunk_start + len(chunk_text):
-                break
-
-        if not chunk_pages and page_positions:
-            for pos, page_num in reversed(page_positions):
-                if pos < chunk_start:
-                    chunk_pages.add(page_num)
-                    break
-
-        # Find images for this chunk's pages
-        image_indices = []
-        for img_idx, img in enumerate(data.images):
-            if img.page_number and img.page_number in chunk_pages:
-                image_indices.append(img_idx)
-
-        chunk_metadata.append({
-            'chunk_index': chunk_idx,
-            'page_numbers': sorted(list(chunk_pages)) if chunk_pages else None,
-            'image_indices': image_indices,
-        })
-
-    data.chunks = chunks
-    data.chunk_metadata = chunk_metadata
-    data.metadata['chunking_strategy'] = 'recursive_with_images'
-    data.metadata['chunk_count'] = len(chunks)
-
-    return data
diff --git a/transforms/llm.py b/transforms/llm.py
deleted file mode 100644
index 82fa2bf..0000000
--- a/transforms/llm.py
+++ /dev/null
@@ -1,155 +0,0 @@
-"""
-LLM-based processing transforms.
-
-All functions follow signature: (data: ExtractedData) -> ExtractedData
-
-NOTE: Dataloop model integration is not yet implemented.
-"""
-
-import logging
-from typing import Optional, List, Dict, Any
-
-from utils.extracted_data import ExtractedData
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class LLMProcessor:
-    """LLM-based text processing operations. Dataloop model integration pending."""
-
-    @staticmethod
-    def call_model(model_id: str, prompt: str, dataset=None) -> Optional[str]:
-        """
-        Call LLM model with a text prompt.
-
-        TODO: Implement Dataloop model integration.
-        """
-        logger.warning("LLM model integration not yet implemented")
-        return None
-
-    @staticmethod
-    def chunk_semantic(text: str, model_id: str, dataset=None) -> List[str]:
-        """
-        Perform semantic chunking using an LLM.
-
-        TODO: Implement Dataloop model integration.
-        """
-        logger.warning("Semantic chunking not yet implemented")
-        return []
-
-    @staticmethod
-    def summarize(text: str, model_id: str, max_chars: int = 2000, dataset=None) -> Optional[str]:
-        """
-        Generate summary of text using an LLM.
-
-        TODO: Implement Dataloop model integration.
-        """
-        logger.warning("LLM summarization not yet implemented")
-        return None
-
-    @staticmethod
-    def extract_entities(text: str, model_id: str, max_chars: int = 1000, dataset=None) -> Optional[Dict[str, Any]]:
-        """
-        Extract named entities from text using an LLM.
-
-        TODO: Implement Dataloop model integration.
-        """
-        logger.warning("Entity extraction not yet implemented")
-        return None
-
-    @staticmethod
-    def translate(text: str, model_id: str, target_language: str, dataset=None) -> Optional[str]:
-        """
-        Translate text using an LLM.
-
-        TODO: Implement Dataloop model integration.
-        """
-        logger.warning("LLM translation not yet implemented")
-        return None
-
-
-# Transform wrappers
-
-def llm_chunk_semantic(data: ExtractedData) -> ExtractedData:
-    """Semantic chunking using an LLM. Not yet implemented."""
-    data.current_stage = "llm_chunking"
-    content = data.get_text()
-
-    if not content:
-        data.chunks = []
-        return data
-
-    if not data.config.llm_model_id:
-        data.log_warning("LLM model not configured. Skipping semantic chunking.")
-        data.chunks = []
-        return data
-
-    data.chunks = LLMProcessor.chunk_semantic(
-        text=content, model_id=data.config.llm_model_id, dataset=data.target_dataset
-    )
-    data.metadata['chunking_method'] = 'llm_semantic'
-
-    return data
-
-
-def llm_summarize(data: ExtractedData) -> ExtractedData:
-    """Generate summary of content. Not yet implemented."""
-    data.current_stage = "summarization"
-    content = data.get_text()
-
-    if not content or not data.config.generate_summary or not data.config.llm_model_id:
-        return data
-
-    response = LLMProcessor.summarize(
-        text=content, model_id=data.config.llm_model_id, max_chars=2000, dataset=data.target_dataset
-    )
-
-    if response:
-        data.metadata['summary'] = response.strip()
-
-    return data
-
-
-def llm_extract_entities(data: ExtractedData) -> ExtractedData:
-    """Extract named entities. Not yet implemented."""
-    data.current_stage = "entity_extraction"
-    content = data.get_text()
-
-    if not content or not data.config.extract_entities or not data.config.llm_model_id:
-        return data
-
-    entities = LLMProcessor.extract_entities(
-        text=content, model_id=data.config.llm_model_id, max_chars=1000, dataset=data.target_dataset
-    )
-
-    if entities:
-        if 'raw' in entities:
-            data.metadata['entities_raw'] = entities['raw']
-        else:
-            data.metadata['entities'] = entities
-
-    return data
-
-
-def llm_translate(data: ExtractedData) -> ExtractedData:
-    """Translate content. Not yet implemented."""
-    data.current_stage = "translation"
-    content = data.get_text()
-
-    if not content or not data.config.translate or not data.config.llm_model_id:
-        return data
-
-    response = LLMProcessor.translate(
-        text=content,
-        model_id=data.config.llm_model_id,
-        target_language=data.config.target_language,
-        dataset=data.target_dataset
-    )
-
-    if response:
-        data.metadata['original_content'] = content
-        data.metadata['original_language'] = 'auto-detected'
-        data.metadata['target_language'] = data.config.target_language
-        data.content_text = response.strip()
-
-    return data
diff --git a/transforms/ocr.py b/transforms/ocr.py
deleted file mode 100644
index d21040e..0000000
--- a/transforms/ocr.py
+++ /dev/null
@@ -1,285 +0,0 @@
-"""
-OCR enhancement transforms.
-
-All functions follow signature: (data: ExtractedData) -> ExtractedData
-
-NOTE: Dataloop model integration for batch OCR and image captioning is not yet implemented.
-Currently only local EasyOCR is supported.
-"""
-
-import logging
-import re
-import warnings
-from pathlib import Path
-from typing import Dict, List, Tuple, Optional
-
-from utils.extracted_data import ExtractedData
-from utils.data_types import ImageContent
-
-warnings.filterwarnings(
-    'ignore', category=DeprecationWarning, module='torch.ao.quantization', message='.*torch.ao.quantization.*'
-)
-
-_easyocr = None
-
-
-def _get_easyocr():
-    """Lazy import easyocr to avoid loading torch until needed."""
-    global _easyocr
-    if _easyocr is None:
-        import easyocr
-        _easyocr = easyocr
-    return _easyocr
-
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class OCREnhancer:
-    """OCR text extraction using local EasyOCR."""
-
-    _easyocr_reader = None
-    _easyocr_languages = ['en', 'es', 'fr', 'de', 'it', 'pt']
-
-    @staticmethod
-    def extract_local(images: List[ImageContent]) -> Tuple[Dict[int, List[str]], Dict[str, str]]:
-        """
-        Extract OCR text from images using local EasyOCR.
-
-        Returns:
-            Tuple of (ocr_by_page dict, errors dict mapping image_path -> error_message)
-        """
-        ocr_by_page: Dict[int, List[str]] = {}
-        ocr_errors: Dict[str, str] = {}
-
-        for img in images:
-            if img.path:
-                try:
-                    ocr_text, error_msg = OCREnhancer._extract_with_easyocr(img.path)
-                    if error_msg:
-                        ocr_errors[img.path] = error_msg
-                    elif ocr_text:
-                        page_num = img.page_number or 0
-                        if page_num not in ocr_by_page:
-                            ocr_by_page[page_num] = []
-                        ocr_by_page[page_num].append(ocr_text)
-                except Exception as e:
-                    error_message = f"OCR failed for {img.path}: {e}"
-                    logger.warning(error_message)
-                    ocr_errors[img.path] = str(e)
-
-        return ocr_by_page, ocr_errors
-
-    @staticmethod
-    def extract_batch(
-        images: List[ImageContent], model_id: str, dataset, item_name: str, item_id: str
-    ) -> Tuple[Dict[int, List[str]], Dict[str, str]]:
-        """
-        Extract OCR text from images using Dataloop batch processing.
-
-        TODO: Implement Dataloop model integration.
-        Currently falls back to local OCR.
-
-        Returns:
-            Tuple of (ocr_by_page dict, errors dict mapping image_path -> error_message)
-        """
-        logger.warning("Batch OCR with Dataloop models not yet implemented. Using local OCR.")
-        return OCREnhancer.extract_local(images)
-
-    @staticmethod
-    def integrate_ocr_append_to_page(content: str, ocr_by_page: Dict[int, List[str]]) -> str:
-        """Integrate OCR text by appending to each page."""
-        page_pattern = r'(--- Page (\d+) ---)'
-        parts = re.split(page_pattern, content)
-
-        if len(parts) <= 1:
-            all_ocr_texts = []
-            for page_num in sorted(ocr_by_page.keys()):
-                page_info = f" (Page {page_num})" if page_num else ""
-                for ocr_text in ocr_by_page[page_num]:
-                    all_ocr_texts.append(f"--- Image{page_info} ---\n{ocr_text}")
-            return content + '\n\n--- OCR Extracted Text ---\n\n' + '\n\n'.join(all_ocr_texts)
-
-        result_parts = []
-
-        if parts[0].strip():
-            result_parts.append(parts[0])
-
-        i = 1
-        while i < len(parts):
-            if i + 2 < len(parts):
-                page_marker = parts[i]
-                page_num = int(parts[i + 1])
-                page_content = parts[i + 2]
-
-                result_parts.append(page_marker)
-                result_parts.append(page_content)
-
-                if page_num in ocr_by_page and ocr_by_page[page_num]:
-                    ocr_section = [f"\n--- OCR from Page {page_num} Images ---"]
-                    for idx, ocr_text in enumerate(ocr_by_page[page_num], 1):
-                        ocr_section.append(f"\nImage {idx}:\n{ocr_text}")
-                    result_parts.append('\n'.join(ocr_section))
-
-                i += 3
-            else:
-                result_parts.extend(parts[i:])
-                break
-
-        return ''.join(result_parts)
-
-    @staticmethod
-    def integrate_ocr_separate_chunks(content: str, ocr_by_page: Dict[int, List[str]]) -> str:
-        """Integrate OCR text as a separate section at the end."""
-        all_ocr_texts = []
-        for page_num in sorted(ocr_by_page.keys()):
-            if ocr_by_page[page_num]:
-                for idx, ocr_text in enumerate(ocr_by_page[page_num], 1):
-                    page_info = f" (Page {page_num})" if page_num else ""
-                    all_ocr_texts.append(f"--- Image {idx}{page_info} ---\n{ocr_text}")
-
-        if all_ocr_texts:
-            return content + '\n\n--- OCR Extracted Text (Separate Section) ---\n\n' + '\n\n'.join(all_ocr_texts)
-        return content
-
-    @staticmethod
-    def integrate_ocr_combine_all(content: str, ocr_by_page: Dict[int, List[str]]) -> str:
-        """Integrate OCR text by combining all at the end without page markers."""
-        all_ocr_texts = []
-        for page_num in sorted(ocr_by_page.keys()):
-            all_ocr_texts.extend(ocr_by_page[page_num])
-
-        if all_ocr_texts:
-            combined_ocr = '\n\n'.join(all_ocr_texts)
-            return content + '\n\n--- All OCR Text Combined ---\n\n' + combined_ocr
-        return content
-
-    @staticmethod
-    def integrate_ocr_per_page(content: str, ocr_by_page: Dict[int, List[str]]) -> str:
-        """Legacy method - calls integrate_ocr_append_to_page for backward compatibility."""
-        return OCREnhancer.integrate_ocr_append_to_page(content, ocr_by_page)
-
-    @staticmethod
-    def _extract_with_easyocr(image_path: str) -> Tuple[str, Optional[str]]:
-        """
-        Extract text using EasyOCR.
-
-        Returns:
-            Tuple of (extracted_text, error_message)
-            - If successful: (text, None)
-            - If failed: ("", error_message_string)
-        """
-        try:
-            easyocr = _get_easyocr()
-            if OCREnhancer._easyocr_reader is None:
-                logger.info(f"Initializing EasyOCR reader with languages: {OCREnhancer._easyocr_languages}")
-                OCREnhancer._easyocr_reader = easyocr.Reader(OCREnhancer._easyocr_languages, gpu=False)
-                logger.info("EasyOCR reader initialized and cached")
-
-            resolved_path = str(Path(image_path).resolve())
-            results = OCREnhancer._easyocr_reader.readtext(resolved_path)
-            all_text = ' '.join([text for (bbox, text, confidence) in results])
-
-            logger.info(f"EasyOCR extracted {len(results)} text blocks, total length: {len(all_text)}")
-            return all_text, None
-
-        except Exception as e:
-            error_message = f"EasyOCR failed: {str(e)}"
-            logger.error(error_message)
-            return "", error_message
-
-
-class ImageDescriber:
-    """
-    Image description using vision models.
-    
-    TODO: Implement Dataloop model integration.
-    """
-
-    @staticmethod
-    def describe(images: List[ImageContent], model_id: str, dataset, item_name: str, item_id: str) -> List[str]:
-        """
-        Generate descriptions for images using a vision model.
-
-        TODO: Implement Dataloop model integration.
-        """
-        logger.warning("Image captioning not yet implemented")
-        return []
-
-
-# Transform wrappers
-
-
-def ocr_enhance(data: ExtractedData) -> ExtractedData:
-    """
-    Add OCR text from images to content.
-
-    Currently only 'local' method (EasyOCR) is implemented.
-    Batch and auto methods fall back to local.
-    """
-    data.current_stage = "ocr"
-
-    if not data.config.ocr_from_images:
-        return data
-
-    if not data.images:
-        return data
-
-    if data.config.ocr_method in ('batch', 'auto'):
-        logger.info(f"OCR method '{data.config.ocr_method}' requested but batch not implemented. Using local OCR.")
-
-    ocr_by_page, ocr_errors = OCREnhancer.extract_local(data.images)
-
-    # Store OCR errors in metadata
-    if ocr_errors:
-        data.metadata['ocr_errors'] = ocr_errors
-        data.metadata['ocr_failed_count'] = len(ocr_errors)
-        # Log warnings for each failed OCR attempt
-        for image_path, error_msg in ocr_errors.items():
-            data.log_warning(f"EasyOCR failed for {image_path}: {error_msg}")
-
-    if not ocr_by_page:
-        return data
-
-    # Use the configured OCR integration method
-    integration_method = data.config.ocr_integration_method
-    if integration_method == 'separate_chunks':
-        data.content_text = OCREnhancer.integrate_ocr_separate_chunks(data.content_text, ocr_by_page)
-    elif integration_method == 'combine_all':
-        data.content_text = OCREnhancer.integrate_ocr_combine_all(data.content_text, ocr_by_page)
-    else:  # Default to 'append_to_page'
-        data.content_text = OCREnhancer.integrate_ocr_append_to_page(data.content_text, ocr_by_page)
-
-    total_ocr_length = sum(len(t) for texts in ocr_by_page.values() for t in texts)
-    total_ocr_count = sum(len(texts) for texts in ocr_by_page.values())
-
-    data.metadata['ocr_applied'] = True
-    data.metadata['ocr_method'] = 'local'
-    data.metadata['ocr_integration_method'] = integration_method
-    data.metadata['ocr_text_length'] = total_ocr_length
-    data.metadata['ocr_image_count'] = total_ocr_count
-
-    return data
-
-
-def describe_images(data: ExtractedData) -> ExtractedData:
-    """
-    Generate image descriptions using vision models.
-
-    TODO: Implement Dataloop model integration.
-    Currently returns without modifications.
-    """
-    data.current_stage = "image_description"
-
-    if not data.images:
-        return data
-
-    if not data.config.vision_model_id:
-        data.log_warning("Vision model not configured. Skipping image descriptions.")
-        return data
-
-    # Placeholder - not yet implemented
-    data.log_warning("Image captioning with Dataloop models not yet implemented. Skipping.")
-    data.metadata['image_descriptions_generated'] = False
-
-    return data
diff --git a/transforms/text_normalization.py b/transforms/text_normalization.py
deleted file mode 100644
index f9031fb..0000000
--- a/transforms/text_normalization.py
+++ /dev/null
@@ -1,133 +0,0 @@
-"""
-Text normalization transforms.
-
-All functions follow signature: (data: ExtractedData) -> ExtractedData
-"""
-
-import logging
-import re
-from functools import partial
-
-from unstructured.cleaners.core import (
-    replace_unicode_quotes,
-    clean as unstructured_clean,
-    clean_non_ascii_chars,
-    clean_ordered_bullets,
-    group_broken_paragraphs,
-    remove_punctuation,
-)
-from unstructured.documents.elements import Text
-
-from utils.extracted_data import ExtractedData
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class TextNormalizer:
-    """Text normalization operations."""
-
-    @staticmethod
-    def normalize_whitespace(text: str) -> str:
-        """Normalize whitespace: collapse multiple spaces, limit consecutive newlines."""
-        text = re.sub(r' +', ' ', text)
-        text = re.sub(r'\n{3,}', '\n\n', text)
-        return text
-
-    @staticmethod
-    def remove_empty_lines(text: str) -> str:
-        """Remove empty lines from text (collapses paragraph breaks)."""
-        lines = [line for line in text.split('\n') if line.strip()]
-        return '\n'.join(lines)
-
-    @staticmethod
-    def deep_clean(text: str) -> str:
-        """
-        Aggressive text cleaning using unstructured.io library.
-
-        Applies: whitespace removal, dash/bullet normalization, trailing punctuation
-        removal, lowercase conversion, unicode quote replacement, non-ASCII cleaning,
-        broken paragraph grouping, and ordered bullet cleaning.
-        """
-        if not text:
-            return ""
-
-        try:
-            cleaner_partial = partial(
-                unstructured_clean,
-                extra_whitespace=True,
-                dashes=True,
-                bullets=True,
-                trailing_punctuation=True,
-                lowercase=True
-            )
-
-            cleaners = [
-                cleaner_partial,
-                replace_unicode_quotes,
-                clean_non_ascii_chars,
-                group_broken_paragraphs,
-                remove_punctuation,
-            ]
-
-            element = Text(text)
-            element.apply(*cleaners)
-
-            if element.text.split() != []:
-                element.text = clean_ordered_bullets(text=element.text)
-
-            return element.text
-
-        except Exception as e:
-            logger.warning(f"Deep cleaning failed: {str(e)}, returning original text")
-            return text
-
-
-# Transform wrappers
-
-def clean(data: ExtractedData) -> ExtractedData:
-    """
-    Clean and normalize text content based on config options.
-
-    Respects:
-    - config.normalize_whitespace: Collapse multiple spaces/newlines (default: True)
-    - config.remove_empty_lines: Remove blank lines (default: True)
-    """
-    data.current_stage = "cleaning"
-
-    text = data.content_text
-    if not text:
-        data.cleaned_text = ""
-        return data
-
-    # Always strip lines
-    text = text.strip()
-    text = '\n'.join(line.strip() for line in text.split('\n'))
-
-    # Optional: normalize whitespace
-    if data.config.normalize_whitespace:
-        text = TextNormalizer.normalize_whitespace(text)
-
-    # Optional: remove empty lines
-    if data.config.remove_empty_lines:
-        text = TextNormalizer.remove_empty_lines(text)
-
-    data.cleaned_text = text
-    data.metadata['cleaning_applied'] = True
-    data.metadata['normalize_whitespace'] = data.config.normalize_whitespace
-    data.metadata['remove_empty_lines'] = data.config.remove_empty_lines
-
-    return data
-
-
-def deep_clean(data: ExtractedData) -> ExtractedData:
-    """Apply aggressive text cleaning using unstructured.io library."""
-    data.current_stage = "deep_cleaning"
-
-    content = data.get_text()
-    if not content:
-        return data
-
-    data.cleaned_text = TextNormalizer.deep_clean(content)
-    data.metadata['deep_cleaning_applied'] = True
-
-    return data
diff --git a/transforms/upload.py b/transforms/upload.py
deleted file mode 100644
index ec820ed..0000000
--- a/transforms/upload.py
+++ /dev/null
@@ -1,188 +0,0 @@
-"""
-Upload transforms for storing chunks in Dataloop.
-
-All functions follow signature: (data: ExtractedData) -> ExtractedData
-"""
-
-import io
-import logging
-import os
-from pathlib import Path
-from typing import List, Dict, Any, Optional
-
-import dtlpy as dl
-import pandas as pd
-
-from utils.chunk_metadata import ChunkMetadata
-from utils.extracted_data import ExtractedData
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-class ChunkUploader:
-    """Chunk upload operations to Dataloop."""
-
-    @staticmethod
-    def upload(
-        chunks: List[str],
-        source_item: dl.Item,
-        target_dataset: dl.Dataset,
-        remote_path: str = '/chunks',
-        processor_metadata: Optional[Dict[str, Any]] = None,
-        chunk_metadata_list: Optional[List[Dict[str, Any]]] = None,
-        image_id_map: Optional[Dict[int, str]] = None
-    ) -> List[dl.Item]:
-        """Upload chunks to Dataloop dataset using pandas DataFrame bulk upload."""
-        # Resolve image IDs if provided
-        if chunk_metadata_list and image_id_map:
-            for chunk_meta in chunk_metadata_list:
-                image_indices = chunk_meta.get('image_indices', [])
-                actual_image_ids = [
-                    image_id_map[idx] for idx in image_indices if idx in image_id_map
-                ]
-                chunk_meta['image_ids'] = actual_image_ids
-
-        logger.info(
-            f"Uploading chunks | item_id={source_item.id} count={len(chunks)} "
-            f"remote_path={remote_path} target_dataset={target_dataset.name}"
-        )
-
-        # Prepare DataFrame with individual metadata for each chunk
-        upload_data = []
-        base_name = Path(source_item.name).stem
-        full_remote_path = os.path.join(remote_path, source_item.dir.lstrip('/')).replace('\\', '/')
-
-        for idx, chunk_text in enumerate(chunks):
-            chunk_filename = f"{base_name}_chunk_{idx:04d}.txt"
-
-            # Create BytesIO buffer for the chunk
-            buffer = io.BytesIO(chunk_text.encode('utf-8'))
-            buffer.name = chunk_filename
-            buffer.seek(0)
-
-            # Use provided metadata if available, otherwise create default
-            if chunk_metadata_list and idx < len(chunk_metadata_list):
-                chunk_meta = chunk_metadata_list[idx]
-                if isinstance(chunk_meta, dict):
-                    chunk_metadata = chunk_meta
-                else:
-                    chunk_metadata = chunk_meta.to_dict() if hasattr(chunk_meta, 'to_dict') else chunk_meta
-            else:
-                chunk_metadata = ChunkMetadata.create(
-                    source_item=source_item,
-                    total_chunks=len(chunks),
-                    chunk_index=idx,
-                    processor_specific_metadata=processor_metadata
-                ).to_dict()
-
-            upload_data.append({
-                'local_path': buffer,
-                'remote_path': full_remote_path,
-                'remote_name': chunk_filename,
-                'item_metadata': chunk_metadata
-            })
-
-        # Create DataFrame for bulk upload
-        df = pd.DataFrame(upload_data)
-
-        # Try bulk upload first
-        try:
-            logger.info("Attempting bulk upload with pandas DataFrame...")
-            uploaded_items = target_dataset.items.upload(local_path=df, overwrite=True)
-
-            if uploaded_items is None:
-                raise dl.PlatformException("Bulk upload returned None")
-            elif isinstance(uploaded_items, dl.Item):
-                uploaded_items = [uploaded_items]
-            else:
-                uploaded_items = list(uploaded_items)
-
-            logger.info(f"Bulk upload successful | uploaded_count={len(uploaded_items)}")
-
-        except Exception as e:
-            logger.warning(f"Bulk upload failed: {str(e)}. Falling back to individual uploads...")
-            uploaded_items = ChunkUploader._fallback_individual_upload(
-                upload_data, target_dataset, len(chunks)
-            )
-
-        try:
-            uploaded_names = [it.name for it in uploaded_items]
-        except Exception:
-            uploaded_names = ["<unknown>"]
-
-        logger.info(
-            f"Upload completed | item_id={source_item.id} uploaded_count={len(uploaded_items)} "
-            f"remote_path={full_remote_path} sample_names={uploaded_names[:3]}"
-        )
-
-        return uploaded_items
-
-    @staticmethod
-    def _fallback_individual_upload(
-        upload_data: List[Dict],
-        target_dataset: dl.Dataset,
-        total_chunks: int
-    ) -> List[dl.Item]:
-        """Fallback to individual uploads when bulk upload fails."""
-        uploaded_items = []
-        failed_uploads = []
-
-        for idx, row in enumerate(upload_data):
-            try:
-                logger.debug(f"Uploading chunk {idx + 1}/{len(upload_data)}: {row['remote_name']}")
-
-                uploaded_item = target_dataset.items.upload(
-                    local_path=row['local_path'],
-                    remote_path=row['remote_path'],
-                    remote_name=row['remote_name'],
-                    item_metadata=row['item_metadata'],
-                    overwrite=True
-                )
-
-                if uploaded_item:
-                    uploaded_items.append(uploaded_item)
-                else:
-                    failed_uploads.append(idx)
-
-            except Exception as item_e:
-                logger.error(f"Failed to upload chunk {idx}: {str(item_e)}")
-                failed_uploads.append(idx)
-
-        if failed_uploads:
-            logger.error(f"Failed to upload {len(failed_uploads)} chunks: indices {failed_uploads}")
-
-        if not uploaded_items:
-            raise dl.PlatformException(f"All individual uploads failed! Total chunks: {total_chunks}")
-
-        logger.info(f"Individual upload completed | successful={len(uploaded_items)}/{total_chunks}")
-        return uploaded_items
-
-# Transform wrappers
-
-def upload_to_dataloop(data: ExtractedData) -> ExtractedData:
-    """Upload chunks to Dataloop dataset with optional image associations."""
-    data.current_stage = "upload"
-
-    if not data.chunks:
-        data.log_warning("No chunks to upload")
-        data.uploaded_items = []
-        return data
-
-    if not data.item or not data.target_dataset:
-        data.log_error("Missing source item or target dataset.")
-        return data
-
-    uploaded_items = ChunkUploader.upload(
-        chunks=data.chunks,
-        source_item=data.item,
-        target_dataset=data.target_dataset,
-        remote_path=data.config.remote_path,
-        processor_metadata=data.metadata,
-        chunk_metadata_list=data.chunk_metadata if data.chunk_metadata else None,
-        image_id_map=data.metadata.get('image_id_map', {}),
-    )
-
-    data.uploaded_items = uploaded_items
-    data.metadata['uploaded_count'] = len(uploaded_items)
-
-    return data
diff --git a/utils/__init__.py b/utils/__init__.py
index 39d8b4a..b241bd9 100644
--- a/utils/__init__.py
+++ b/utils/__init__.py
@@ -1,25 +1,23 @@
 """
-Utils infrastructure package for shared functionality.
-
-Public API:
-- ExtractedData: Central pipeline data structure
-- Config: Configuration with validation
-- ErrorTracker: Error tracking utilities
-- ImageContent, TableContent: Data types for content extraction
-- ChunkMetadata: Metadata structure for chunks
+Utilities package for shared functionality.
+Common helpers used across processors, chunkers, and extractors.
 """
 
-from .config import Config
-from .errors import ErrorTracker
-from .extracted_data import ExtractedData
-from .data_types import ImageContent, TableContent
+from .text_cleaning import clean_text
+from .dataloop_helpers import (
+    get_or_create_target_dataset,
+    upload_chunks,
+    cleanup_temp_items_and_folder
+)
+from .dataloop_model_executor import DataloopModelExecutor
 from .chunk_metadata import ChunkMetadata
 
 __all__ = [
-    'ExtractedData',
-    'Config',
-    'ErrorTracker',
-    'ImageContent',
-    'TableContent',
-    'ChunkMetadata',
+    'clean_text',
+    'get_or_create_target_dataset', 
+    'upload_chunks',
+    'cleanup_temp_items_and_folder',
+    'DataloopModelExecutor',
+    'ChunkMetadata'
 ]
+
diff --git a/utils/chunk_metadata.py b/utils/chunk_metadata.py
index c8b4bb6..c50a2f3 100644
--- a/utils/chunk_metadata.py
+++ b/utils/chunk_metadata.py
@@ -3,128 +3,49 @@ Shared metadata management for chunk items across all processors.
 Provides a standardized way to create and manage metadata for document chunks.
 """
 
-import time
-from dataclasses import dataclass, field
-from typing import Optional, List, Dict, Any
 import dtlpy as dl
+from typing import Dict, Any
 
 
-@dataclass
 class ChunkMetadata:
     """
-    Standardized chunk metadata with validation.
-
-    This dataclass provides a standardized structure for chunk metadata
-    across all document processors, with validation at instantiation.
+    Shared metadata class for all document processors.
+    Creates standardized metadata structure for chunk items with minimal reference info.
     """
-
-    # Required fields
-    source_item_id: str
-    source_file: str
-    source_dataset_id: str
-    chunk_index: int
-    total_chunks: int
-
-    # Optional fields
-    page_numbers: Optional[List[int]] = None
-    image_ids: Optional[List[str]] = None
-    bbox: Optional[tuple] = None
-    processing_timestamp: float = field(default_factory=time.time)
-    processor: Optional[str] = None
-    extraction_method: Optional[str] = None
-    processor_specific_metadata: Optional[Dict[str, Any]] = None
-
-    def __post_init__(self):
-        """Validate required fields at instantiation."""
-        if not self.source_item_id:
-            raise ValueError("source_item_id is required")
-        if not self.source_file:
-            raise ValueError("source_file is required")
-        if not self.source_dataset_id:
-            raise ValueError("source_dataset_id is required")
-        if self.chunk_index < 0:
-            raise ValueError("chunk_index must be non-negative")
-        if self.total_chunks < 1:
-            raise ValueError("total_chunks must be at least 1")
-
-    def to_dict(self) -> Dict[str, Any]:
+    
+    @staticmethod
+    def create_for_chunk(
+        original_item: dl.Item,
+        chunk_index: int,
+        total_chunks: int) -> Dict[str, Any]:
         """
-        Convert to dictionary format compatible with Dataloop metadata structure.
-
+        Create standardized metadata for a single chunk item.
+        
+        Chunk items contain only reference information back to the original document.
+        Processing details are stored on the original document item.
+        
+        Args:
+            original_item (dl.Item): Original document item that was processed
+            chunk_index (int): Index of this chunk (1-based)
+            total_chunks (int): Total number of chunks created from the document
+        
         Returns:
-            Dict[str, Any]: Metadata in Dataloop format {'user': {...}}
+            Dict[str, Any]: Metadata structure for chunk item
+        
+        Example:
+            >>> metadata = ChunkMetadata.create_for_chunk(item, 1, 50)
         """
-        base_metadata = {
-            'source_item_id': self.source_item_id,
-            'source_file': self.source_file,
-            'source_dataset_id': self.source_dataset_id,
-            'chunk_index': self.chunk_index,
-            'total_chunks': self.total_chunks,
-            'extracted_chunk': True,
-            'processing_timestamp': self.processing_timestamp,
+        chunk_metadata = {
+            'document': original_item.name,
+            'document_type': original_item.mimetype,
+            'chunk_index': chunk_index,
+            'total_chunks': total_chunks,
+            'original_item_id': original_item.id,
+            'original_dataset_id': original_item.dataset.id,
         }
-
-        # Add optional fields if present
-        if self.page_numbers:
-            base_metadata['page_numbers'] = self.page_numbers
-        if self.image_ids:
-            base_metadata['image_ids'] = self.image_ids
-        if self.bbox:
-            base_metadata['bbox'] = self.bbox
-        if self.processor:
-            base_metadata['processor'] = self.processor
-        if self.extraction_method:
-            base_metadata['extraction_method'] = self.extraction_method
-
-        # Merge processor-specific metadata if provided
-        if self.processor_specific_metadata:
-            base_metadata.update(self.processor_specific_metadata)
-
+        
         # Return in Dataloop's metadata structure format
-        return {'user': base_metadata}
-
-    @classmethod
-    def create(
-        cls,
-        source_item: dl.Item,
-        total_chunks: int,
-        chunk_index: Optional[int] = None,
-        processor_specific_metadata: Optional[Dict[str, Any]] = None,
-        page_numbers: Optional[List[int]] = None,
-        image_ids: Optional[List[str]] = None,
-        processor: Optional[str] = None,
-        extraction_method: Optional[str] = None,
-    ) -> 'ChunkMetadata':
-        """
-        Create ChunkMetadata instance from Dataloop item.
-
-        Args:
-            source_item: Source document item that was processed
-            total_chunks: Total number of chunks created from the document
-            chunk_index: Index of this chunk (0-based)
-            processor_specific_metadata: Additional metadata specific to the processor
-            page_numbers: Page numbers this chunk spans
-            image_ids: IDs of associated image items
-            processor: Processor name (e.g., 'pdf', 'doc')
-            extraction_method: Extraction method used (e.g., 'pymupdf', 'pymupdf4llm')
-
-        Returns:
-            ChunkMetadata: Instance with all fields populated
-        """
-
-        # If chunk_index not provided, use 0 as default (for bulk uploads without per-chunk metadata)
-        if chunk_index is None:
-            chunk_index = 0
+        return {
+            'user': chunk_metadata
+        }
 
-        return cls(
-            source_item_id=source_item.id,
-            source_file=source_item.name,
-            source_dataset_id=source_item.dataset.id,
-            chunk_index=chunk_index,
-            total_chunks=total_chunks,
-            page_numbers=page_numbers,
-            image_ids=image_ids,
-            processor=processor,
-            extraction_method=extraction_method,
-            processor_specific_metadata=processor_specific_metadata,
-        )
diff --git a/utils/config.py b/utils/config.py
deleted file mode 100644
index 18f1e76..0000000
--- a/utils/config.py
+++ /dev/null
@@ -1,133 +0,0 @@
-"""
-Simple configuration with validation.
-
-This module provides a flat, easy-to-understand configuration class.
-All settings are in one place with straightforward validation.
-"""
-from dataclasses import dataclass, asdict
-from typing import Optional, Literal, List
-
-
-@dataclass
-class Config:
-    """
-    Single configuration class with all processing settings.
-
-    Flat structure for simplicity - no nested config objects.
-    Use `from_dict()` to create from dictionaries.
-    Use `validate()` to check configuration before processing.
-
-    Example:
-        config = Config(
-            use_ocr=True,
-            ocr_model_id='model-123',
-            max_chunk_size=500
-        )
-        config.validate()
-    """
-
-    # Error handling
-    error_mode: Literal['stop', 'continue'] = 'continue'
-    max_errors: int = 10
-
-    # Extraction settings
-    use_markdown_extraction: bool = False
-    extract_images: bool = True
-    extract_tables: bool = True
-
-    # OCR settings
-    ocr_from_images: bool = False
-    ocr_method: Literal['local', 'batch', 'auto'] = 'local'
-    ocr_model_id: Optional[str] = None
-    ocr_integration_method: Literal['append_to_page', 'separate_chunks', 'combine_all'] = 'append_to_page'
-
-    # Chunking settings
-    chunking_strategy: Literal['recursive', 'fixed-size', 'nltk-sentence', 'nltk-paragraphs', '1-chunk'] = 'recursive'
-    max_chunk_size: int = 300
-    chunk_overlap: int = 40
-
-    # Cleaning settings
-    normalize_whitespace: bool = True
-    remove_empty_lines: bool = True
-    to_correct_spelling: bool = False
-
-    # LLM settings
-    llm_model_id: Optional[str] = None
-    generate_summary: bool = False
-    extract_entities: bool = False
-    translate: bool = False
-    target_language: str = 'English'
-
-    # Vision settings
-    vision_model_id: Optional[str] = None
-
-    # Upload settings
-    remote_path: str = '/chunks'
-
-    def validate(self) -> None:
-        """
-        Validate configuration values.
-
-        Raises:
-            ValueError: If any configuration values are invalid.
-
-        Example:
-            config = Config(max_chunk_size=-1)
-            config.validate()  # Raises ValueError
-        """
-        errors: List[str] = []
-
-        # Validate chunk settings
-        if self.max_chunk_size <= 0:
-            errors.append(f"max_chunk_size must be positive, got {self.max_chunk_size}")
-
-        if self.chunk_overlap < 0:
-            errors.append(f"chunk_overlap cannot be negative, got {self.chunk_overlap}")
-
-        if self.chunk_overlap >= self.max_chunk_size:
-            errors.append(
-                f"chunk_overlap ({self.chunk_overlap}) must be less than "
-                f"max_chunk_size ({self.max_chunk_size})"
-            )
-
-        # Validate OCR settings
-        if self.ocr_from_images and self.ocr_method in ('batch', 'auto') and not self.ocr_model_id:
-            errors.append("ocr_model_id is required when ocr_method is 'batch' or 'auto'")
-
-        # Validate LLM settings
-        if (self.generate_summary or self.extract_entities or self.translate) and not self.llm_model_id:
-            errors.append("llm_model_id is required when generate_summary, extract_entities, or translate is enabled")
-
-        # Validate error settings
-        if self.max_errors <= 0:
-            errors.append(f"max_errors must be positive, got {self.max_errors}")
-
-        if errors:
-            raise ValueError("Configuration errors:\n  - " + "\n  - ".join(errors))
-
-    @classmethod
-    def from_dict(cls, config_dict: dict) -> 'Config':
-        """
-        Create Config from dictionary, ignoring unknown keys.
-
-        Args:
-            config_dict: Dictionary with configuration values.
-
-        Returns:
-            Config instance with values from dictionary.
-
-        Example:
-            config = Config.from_dict({
-                'ocr_from_images': True,
-                'to_correct_spelling': True,
-                'use_markdown_extraction': False,
-                'unknown_key': 'ignored'  # Will be ignored
-            })
-        """
-        valid_keys = set(cls.__dataclass_fields__.keys())
-        filtered = {k: v for k, v in config_dict.items() if k in valid_keys}
-        return cls(**filtered)
-
-    def to_dict(self) -> dict:
-        """Convert Config to dictionary."""
-        return asdict(self)
diff --git a/utils/data_types.py b/utils/data_types.py
deleted file mode 100644
index 57b6359..0000000
--- a/utils/data_types.py
+++ /dev/null
@@ -1,48 +0,0 @@
-"""
-Data structures for extracted multimodal content.
-"""
-
-from dataclasses import dataclass
-from typing import Any, Optional
-
-
-@dataclass
-class ImageContent:
-    """Represents an extracted image."""
-
-    path: str
-    caption: Optional[str] = None
-    page_number: Optional[int] = None
-    bbox: Optional[tuple] = None
-    format: Optional[str] = None
-    size: Optional[tuple] = None
-    data: Optional[bytes] = None
-
-    def to_dict(self):
-        return {
-            'path': self.path,
-            'caption': self.caption,
-            'page_number': self.page_number,
-            'bbox': self.bbox,
-            'format': self.format,
-            'size': self.size,
-        }
-
-
-@dataclass
-class TableContent:
-    """Represents an extracted table."""
-
-    data: Any
-    markdown: Optional[str] = None
-    html: Optional[str] = None
-    page_number: Optional[int] = None
-    location: Optional[dict] = None
-
-    def to_dict(self):
-        return {
-            'markdown': self.markdown,
-            'html': self.html,
-            'page_number': self.page_number,
-            'location': self.location,
-        }
diff --git a/utils/dataloop_helpers.py b/utils/dataloop_helpers.py
new file mode 100644
index 0000000..6feb68e
--- /dev/null
+++ b/utils/dataloop_helpers.py
@@ -0,0 +1,187 @@
+"""
+Dataloop helper utilities for dataset management and chunk uploading.
+Handles dataset creation, item upload, and metadata management.
+"""
+
+import dtlpy as dl
+import logging
+import io
+import os
+from pathlib import Path
+from typing import List, Dict, Any, Optional
+from .chunk_metadata import ChunkMetadata
+
+logger = logging.getLogger('item-processor-logger')
+
+
+def get_or_create_target_dataset(original_item: dl.Item, target_dataset: Optional[str]) -> dl.Dataset:
+    """
+    Get the target dataset for chunks or create one if needed.
+    
+    Args:
+        original_item (dl.Item): Original item to get dataset context
+        target_dataset (Optional[str]): Target dataset ID if specified
+        
+    Returns:
+        dl.Dataset: Target dataset for chunks
+    """
+    logger.info(
+        f"Determining target dataset | item_id={original_item.id} "
+        f"target_dataset={target_dataset or 'auto'}"
+    )
+    
+    if target_dataset:
+        try:
+            target_ds = dl.datasets.get(dataset_id=target_dataset)
+            logger.info(
+                f"Using specified target dataset | dataset_id={target_ds.id} "
+                f"dataset_name={target_ds.name}"
+            )
+            return target_ds
+        except dl.exceptions.NotFound:
+            logger.warning(f"Target dataset with ID {target_dataset} not found, a new dataset will be created")
+    
+    # Auto-create chunks dataset
+    original_dataset_name = original_item.dataset.name
+    new_dataset_name = f"{original_dataset_name}_chunks"
+    project_id = original_item.project.id
+    
+    logger.info(
+        f"Auto-creating chunks dataset | original_dataset={original_dataset_name} "
+        f"new_dataset={new_dataset_name} project_id={project_id}"
+    )
+    
+    try:
+        # Check if dataset already exists
+        target_ds = original_item.project.datasets.get(dataset_name=new_dataset_name)
+        logger.info(
+            f"Using existing chunks dataset | dataset_id={target_ds.id} "
+            f"dataset_name={target_ds.name}"
+        )
+    except dl.exceptions.NotFound:
+        # Create new dataset
+        target_ds = original_item.project.datasets.create(dataset_name=new_dataset_name)
+        logger.info(
+            f"Created new chunks dataset | dataset_id={target_ds.id} "
+            f"dataset_name={target_ds.name}"
+        )
+    
+    return target_ds
+
+
+def upload_chunks(chunks: List[str], 
+                 original_item: dl.Item, 
+                 target_dataset: dl.Dataset,
+                 remote_path: str) -> List[dl.Item]:
+    """
+    Upload text chunks as items to the target dataset using BytesIO buffers.
+    Each chunk item gets minimal metadata with reference to original item and chunk index.
+    
+    Args:
+        chunks (List[str]): Text chunks to upload
+        original_item (dl.Item): Original document item
+        target_dataset (dl.Dataset): Target dataset for chunks
+        remote_path (str): Remote path in dataset
+        
+    Returns:
+        List[dl.Item]: Uploaded chunk items
+    """
+    logger.info(
+        f"Uploading chunks | item_id={original_item.id} count={len(chunks)} "
+        f"remote_path={remote_path} target_dataset={target_dataset.name}"
+    )
+    
+    total_chunks = len(chunks)
+    base_name = Path(original_item.name).stem
+    full_remote_path = os.path.join(remote_path, original_item.dir.lstrip('/')).replace('\\', '/')
+    
+    # Upload chunks individually with their own metadata
+    uploaded_items = []
+    
+    for idx, chunk in enumerate(chunks, start=1):
+        chunk_filename = f"{base_name}_chunk_{idx:04d}.txt"
+        
+        # Create BytesIO buffer
+        buffer = io.BytesIO(chunk.encode('utf-8'))
+        buffer.name = chunk_filename
+        buffer.seek(0)
+        
+        # Create metadata for this specific chunk
+        chunk_metadata = ChunkMetadata.create_for_chunk(
+            original_item=original_item,
+            chunk_index=idx,
+            total_chunks=total_chunks
+        )
+        
+        # Upload single chunk
+        try:
+            uploaded_item = target_dataset.items.upload(
+                local_path=buffer,
+                remote_path=full_remote_path,
+                item_metadata=chunk_metadata,
+                overwrite=True,
+            )
+            uploaded_items.append(uploaded_item)
+            
+        except Exception as e:
+            logger.error(f"Failed to upload chunk {idx}: {e}")
+            # Continue with other chunks
+            continue
+    
+    logger.info(
+        f"Upload completed | item_id={original_item.id} uploaded_count={len(uploaded_items)}/{total_chunks} "
+        f"remote_path={full_remote_path}"
+    )
+    
+    return uploaded_items
+
+
+def cleanup_temp_items_and_folder(
+    items: List[dl.Item],
+    folder_path: str,
+    dataset: dl.Dataset,
+    local_temp_dir: Optional[str] = None) -> None:
+    """
+    Clean up temporary items, folder, and local directory.
+    
+    Args:
+        items: List of Dataloop items to delete
+        folder_path: Remote folder path in dataset to clean up
+        dataset: Dataset containing the items
+        local_temp_dir: Optional local temp directory to delete
+    """
+    import shutil
+    
+    # Delete uploaded items
+    if items:
+        logger.info(f"Cleaning up uploaded items | count={len(items)}")
+        for item in items:
+            try:
+                item.delete()
+                logger.debug(f"Deleted item | item_id={item.id}")
+            except Exception as e:
+                logger.warning(f"Failed to delete item {item.id}: {e}")
+    
+    # Delete temp folder in Dataloop
+    try:
+        logger.info(f"Deleting temp folder in Dataloop | folder={folder_path}")
+        filters = dl.Filters()
+        filters.add(field='dir', values=folder_path)
+        remaining_items = dataset.items.list(filters=filters)
+        for item in remaining_items:
+            try:
+                item.delete()
+            except:
+                pass
+        logger.info("Temp folder cleanup completed")
+    except Exception as e:
+        logger.warning(f"Failed to cleanup temp folder: {e}")
+    
+    # Delete local temp directory
+    if local_temp_dir:
+        try:
+            shutil.rmtree(local_temp_dir, ignore_errors=True)
+            logger.debug(f"Deleted local temp directory | path={local_temp_dir}")
+        except Exception as e:
+            logger.warning(f"Failed to delete local temp dir: {e}")
+
diff --git a/utils/dataloop_model_executor.py b/utils/dataloop_model_executor.py
new file mode 100644
index 0000000..0eb186f
--- /dev/null
+++ b/utils/dataloop_model_executor.py
@@ -0,0 +1,139 @@
+"""
+Base class for using Dataloop models.
+Handles model lookup, deployment verification, execution, and result management.
+"""
+
+import dtlpy as dl
+from typing import Any, Optional, Dict
+import logging
+
+logger = logging.getLogger('item-processor-logger')
+
+
+class DataloopModelExecutor:
+    """
+    Base class for components that use Dataloop models.
+    Handles model lookup, deployment, execution, and error handling.
+    """
+    
+    def __init__(self, model_id: Optional[str] = None):
+        """
+        Initialize with a model ID.
+        
+        Args:
+            model_id (Optional[str]): Dataloop model ID
+        """
+        self.model_id = model_id
+        self.model = None
+        
+        if not model_id:
+            logger.info("No model_id provided - will use fallback methods")
+    
+    def _get_model(self) -> Optional[dl.Model]:
+        """
+        Fetch and cache the Dataloop model.
+        
+        Returns:
+            Optional[dl.Model]: The Dataloop model, or None if not found
+        """
+        if self.model is None and self.model_id:
+            try:
+                self.model = dl.models.get(model_id=self.model_id)
+                logger.info(f"Loaded model | model_id={self.model_id} name={self.model.name}")
+            except Exception as e:
+                logger.error(f"Failed to load model | model_id={self.model_id} error={str(e)}")
+                return None
+        return self.model
+    
+    def _check_model_deployed(self, auto_deploy: bool = True) -> bool:
+        """
+        Check if the model is deployed and ready.
+        
+        Args:
+            auto_deploy (bool): If True, automatically deploys the model if not deployed (default: True)
+        
+        Returns:
+            bool: True if model is deployed (or deployment triggered)
+            
+        Raises:
+            ValueError: If model is not deployed and auto_deploy is False
+        """
+        model = self._get_model()
+        if model is None:
+            raise ValueError(f"Could not load model with id: {self.model_id}")
+        
+        if model.status != 'deployed':
+            if auto_deploy:
+                logger.info(f"Model not deployed, deploying now | model_id={self.model_id} status={model.status}")
+                try:
+                    model.deploy()
+                    logger.info(f"Model deployment triggered | model_id={self.model_id}")
+                except Exception as e:
+                    logger.error(f"Failed to deploy model | model_id={self.model_id} error={str(e)}")
+                    raise ValueError(f"Failed to deploy model {self.model_id}: {str(e)}")
+            else:
+                raise ValueError(
+                    f"Model {self.model_id} is not deployed (status: {model.status}). "
+                    f"Please deploy the model before using it for predictions."
+                )
+        else:
+            logger.info(f"Model is deployed | model_id={self.model_id} status={model.status}")
+        
+        return model, model.status
+    
+    def execute(self, item: dl.Item, **kwargs) -> dl.Item:
+        """
+        Execute the model on the item and wait for completion.
+        
+        Args:
+            item (dl.Item): Dataloop item to process
+            **kwargs: Additional parameters for the model
+            
+        Returns:
+            dl.Item: Updated item after model execution
+            
+        Raises:
+            ValueError: If model_id is not configured or model not deployed
+            Exception: If execution fails or times out
+        """
+        if not self.model_id:
+            raise ValueError("model_id must be provided to execute")
+        
+        # Check if model is deployed
+        self._check_model_deployed()
+        
+        model = self._get_model()
+        
+        logger.info(f"Executing model | model_id={self.model_id} item_id={item.id}")
+        
+        try:
+            # Execute prediction
+            execution = model.predict(item_ids=[item.id], **kwargs)
+            
+            # Wait for execution to complete
+            logger.info(f"Waiting for execution to complete | execution_id={execution.id}")
+            execution.wait()
+            if execution.latest_status['status'] == dl.ExecutionStatus.FAILED:
+                raise Exception(f"Model execution failed for item {item.id}: {execution.latest_status['message']}")
+            elif execution.latest_status['status'] == 'success':
+                logger.info(f"Model execution successful | execution_id={execution.id}")
+            
+            # Refresh item to get updated data
+            logger.info(f"Fetching updated item | item_id={item.id}")
+            updated_item = dl.items.get(item_id=item.id)
+            
+            return updated_item
+            
+        except Exception as e:
+            logger.error(f"Model execution failed: {str(e)}")
+            raise Exception(f"Model execution failed for item {item.id}: {str(e)}")
+    
+    def has_dataloop_backend(self) -> bool:
+        """
+        Check if this executor is configured to use Dataloop models.
+        
+        Returns:
+            bool: True if model_id is configured
+        """
+        return bool(self.model_id)
+
diff --git a/utils/errors.py b/utils/errors.py
deleted file mode 100644
index 4f3517d..0000000
--- a/utils/errors.py
+++ /dev/null
@@ -1,87 +0,0 @@
-"""
-Simple error tracking without over-abstraction.
-
-This module provides a straightforward way to track errors and warnings
-during document processing. No complex patterns - just a list of errors
-and simple decision logic.
-"""
-from dataclasses import dataclass, field
-from typing import List
-import logging
-
-logger = logging.getLogger("rag-preprocessor")
-
-
-@dataclass
-class ErrorTracker:
-    """
-    Simple error and warning tracker for pipeline processing.
-
-    Tracks errors and warnings as simple strings, and provides
-    basic decision logic for whether to continue processing.
-
-    Example:
-        tracker = ErrorTracker(error_mode='continue', max_errors=5)
-
-        # Log an error and check if we should continue
-        if not tracker.add_error("Extraction failed", "extraction"):
-            return []  # Stop processing
-
-        # Log a warning (never stops processing)
-        tracker.add_warning("OCR quality low")
-
-        # Get summary for logging
-        print(tracker.get_summary())  # "Errors: 1, Warnings: 1"
-    """
-
-    errors: List[str] = field(default_factory=list)
-    warnings: List[str] = field(default_factory=list)
-    max_errors: int = 10
-    error_mode: str = 'continue'
-
-    def add_error(self, message: str, stage: str = "") -> bool:
-        """
-        Add an error and return whether to continue processing.
-
-        Args:
-            message: Error description.
-            stage: Optional processing stage (e.g., "extraction", "chunking").
-
-        Returns:
-            True if processing should continue, False if it should stop.
-
-        Example:
-            if not tracker.add_error("Failed to parse", "extraction"):
-                return []  # Stop processing
-        """
-        error_msg = f"[{stage}] {message}" if stage else message
-        self.errors.append(error_msg)
-        logger.error(error_msg)
-
-        # Decision logic: stop or continue?
-        if self.error_mode == 'stop':
-            return False  # Stop on first error
-        else:
-            # Continue if we haven't hit max_errors yet
-            return len(self.errors) < self.max_errors
-
-    def add_warning(self, message: str, stage: str = "") -> None:
-        """
-        Add a warning (doesn't affect processing continuation).
-
-        Args:
-            message: Warning description.
-            stage: Optional processing stage.
-        """
-        warning_msg = f"[{stage}] {message}" if stage else message
-        self.warnings.append(warning_msg)
-        logger.warning(warning_msg)
-
-    def get_summary(self) -> str:
-        """
-        Get a simple summary string for logging.
-
-        Returns:
-            Summary like "Errors: 2, Warnings: 3"
-        """
-        return f"Errors: {len(self.errors)}, Warnings: {len(self.warnings)}"
diff --git a/utils/extracted_data.py b/utils/extracted_data.py
deleted file mode 100644
index 207d2e7..0000000
--- a/utils/extracted_data.py
+++ /dev/null
@@ -1,159 +0,0 @@
-"""
-Central data structure for pipeline processing.
-
-ExtractedData flows through the entire processing pipeline, carrying:
-- Input: source item, target dataset, configuration
-- Extraction outputs: text, images, tables
-- Processing results: cleaned text, chunks
-- Upload results: uploaded items
-- Error tracking: errors and warnings per stage
-"""
-from dataclasses import dataclass, field
-from typing import List, Dict, Any, Optional
-
-try:
-    import dtlpy as dl
-except ImportError:
-    dl = None  # Allow usage without dtlpy for testing
-
-from .data_types import ImageContent, TableContent
-from .config import Config
-from .errors import ErrorTracker
-
-
-@dataclass
-class ExtractedData:
-    """
-    Central data structure that flows through the processing pipeline.
-
-    This dataclass carries all data between pipeline stages:
-    extraction -> cleaning -> chunking -> upload
-
-    Example:
-        data = ExtractedData(
-            item=pdf_item,
-            target_dataset=chunks_dataset,
-            config=Config(max_chunk_size=500)
-        )
-
-        # After extraction
-        data.content_text = "extracted text..."
-        data.images = [ImageContent(...)]
-
-        # After cleaning
-        data.cleaned_text = "cleaned text..."
-
-        # After chunking
-        data.chunks = ["chunk 1", "chunk 2", ...]
-
-        # After upload
-        data.uploaded_items = [item1, item2, ...]
-    """
-
-    # === INPUT ===
-    item: Optional[Any] = None  # dl.Item - source document
-    target_dataset: Optional[Any] = None  # dl.Dataset - where to upload chunks
-    config: Config = field(default_factory=Config)
-
-    # === EXTRACTION OUTPUTS ===
-    content_text: str = ""  # Raw extracted text
-    images: List[ImageContent] = field(default_factory=list)
-    tables: List[TableContent] = field(default_factory=list)
-    metadata: Dict[str, Any] = field(default_factory=dict)  # Document metadata
-
-    # === PROCESSING OUTPUTS ===
-    cleaned_text: str = ""  # Text after cleaning
-    chunks: List[str] = field(default_factory=list)  # Text chunks
-    chunk_metadata: List[Dict[str, Any]] = field(default_factory=list)  # Per-chunk metadata
-
-    # === UPLOAD OUTPUTS ===
-    uploaded_items: List[Any] = field(default_factory=list)  # dl.Item list
-    uploaded_image_ids: Dict[str, str] = field(default_factory=dict)  # path -> item_id
-
-    # === ERROR TRACKING ===
-    errors: ErrorTracker = field(default_factory=ErrorTracker)
-    current_stage: str = "init"
-
-    def __post_init__(self):
-        """Initialize error tracker with config settings."""
-        # Handle config passed as dict
-        if isinstance(self.config, dict):
-            self.config = Config.from_dict(self.config)
-
-        # Sync error tracker with config
-        self.errors.error_mode = self.config.error_mode
-        self.errors.max_errors = self.config.max_errors
-
-    def log_error(self, message: str) -> bool:
-        """
-        Log an error and return whether to continue processing.
-
-        Args:
-            message: Error description.
-
-        Returns:
-            True if processing should continue, False if it should stop.
-        """
-        return self.errors.add_error(message, self.current_stage)
-
-    def log_warning(self, message: str) -> None:
-        """Log a warning (doesn't affect processing continuation)."""
-        self.errors.add_warning(message, self.current_stage)
-
-    def get_text(self) -> str:
-        """
-        Get the current text content (cleaned if available, else raw).
-
-        Returns:
-            Cleaned text if available, otherwise raw extracted text.
-        """
-        return self.cleaned_text if self.cleaned_text else self.content_text
-
-    def has_content(self) -> bool:
-        """Check if any content has been extracted."""
-        return bool(self.content_text or self.images or self.tables)
-
-    def has_images(self) -> bool:
-        """Check if images were extracted."""
-        return len(self.images) > 0
-
-    def has_tables(self) -> bool:
-        """Check if tables were extracted."""
-        return len(self.tables) > 0
-
-    def has_chunks(self) -> bool:
-        """Check if text has been chunked."""
-        return len(self.chunks) > 0
-
-    @property
-    def item_name(self) -> str:
-        """Get source item name (or 'unknown' if not available)."""
-        if self.item and hasattr(self.item, 'name'):
-            return self.item.name
-        return "unknown"
-
-    @property
-    def item_id(self) -> Optional[str]:
-        """Get source item ID (or None if not available)."""
-        if self.item and hasattr(self.item, 'id'):
-            return self.item.id
-        return None
-
-    def get_summary(self) -> Dict[str, Any]:
-        """
-        Get processing summary for logging/debugging.
-
-        Returns:
-            Dictionary with processing statistics.
-        """
-        return {
-            'item': self.item_name,
-            'stage': self.current_stage,
-            'text_length': len(self.content_text),
-            'cleaned_length': len(self.cleaned_text),
-            'images': len(self.images),
-            'tables': len(self.tables),
-            'chunks': len(self.chunks),
-            'uploaded': len(self.uploaded_items),
-            'errors': self.errors.get_summary(),
-        }
diff --git a/utils/text_cleaning.py b/utils/text_cleaning.py
new file mode 100644
index 0000000..a3216ec
--- /dev/null
+++ b/utils/text_cleaning.py
@@ -0,0 +1,73 @@
+"""
+Text cleaning utilities using unstructured.io library.
+Handles text normalization, cleaning, and standardization.
+"""
+
+import logging
+from functools import partial
+from unstructured.cleaners.core import (
+    replace_unicode_quotes, 
+    clean, 
+    clean_non_ascii_chars,
+    clean_ordered_bullets, 
+    group_broken_paragraphs, 
+    remove_punctuation
+)
+from unstructured.documents.elements import Text
+
+logger = logging.getLogger('item-processor-logger')
+
+
+def clean_text(text: str) -> str:
+    """
+    Clean a single chunk of text using unstructured.io cleaning functions.
+    
+    Applies the following cleaning steps:
+    - Removes extra whitespace
+    - Normalizes dashes and bullets
+    - Removes trailing punctuation
+    - Converts to lowercase
+    - Replaces unicode quotes
+    - Cleans non-ASCII characters
+    - Groups broken paragraphs
+    - Removes unnecessary punctuation
+    - Cleans ordered bullets
+    
+    Args:
+        text (str): Input text to clean
+        
+    Returns:
+        str: Cleaned text
+    """
+    try:
+        # Create a partial function for cleaner with specified parameters
+        cleaner1_partial = partial(
+            clean,
+            extra_whitespace=True,
+            dashes=True,
+            bullets=True,
+            trailing_punctuation=True,
+            lowercase=True
+        )
+
+        cleaners = [
+            cleaner1_partial,
+            replace_unicode_quotes,
+            clean_non_ascii_chars,
+            group_broken_paragraphs,
+            remove_punctuation
+        ]
+
+        # Create a Text element and apply cleaners
+        element = Text(text)
+        element.apply(*cleaners)
+        
+        if element.text.split() != []:
+            element.text = clean_ordered_bullets(text=element.text)
+        
+        return element.text
+        
+    except Exception as e:
+        logger.warning(f"Text cleaning failed: {str(e)}, returning original text")
+        return text
+
